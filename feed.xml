<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://blog.cani.ne.jp/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.cani.ne.jp/" rel="alternate" type="text/html" /><updated>2025-11-24T00:48:17+00:00</updated><id>https://blog.cani.ne.jp/feed.xml</id><title type="html">üê∂ Dog with a Dev Blog</title><subtitle>On the internet, nobody knows you&apos;re a dog and a cat and a computer.</subtitle><entry><title type="html">I Built a Logging Server to Log a Serverless Site</title><link href="https://blog.cani.ne.jp/2025/11/23/opensearch-for-static-site-logs.html" rel="alternate" type="text/html" title="I Built a Logging Server to Log a Serverless Site" /><published>2025-11-23T00:00:00+00:00</published><updated>2025-11-23T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/23/opensearch-for-static-site-logs</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/23/opensearch-for-static-site-logs.html"><![CDATA[<p>I wanted access logs for my static site. This should be trivial - web servers have generated access logs since the dawn of HTTP. But <a href="https://www.digitalocean.com/community/tutorials/how-to-deploy-a-static-website-to-the-cloud-with-digitalocean-app-platform">DigitalOcean‚Äôs static site hosting</a>, elegant in its simplicity, doesn‚Äôt generate them. To get visibility into who visits my blog, I‚Äôd need to build my own logging infrastructure.</p>

<p>The plan: deploy <a href="https://opensearch.org/">OpenSearch</a> on my home server, forward logs from DigitalOcean, and build dashboards to analyze traffic. Simple enough.</p>

<h2 id="the-first-lesson-storage">The First Lesson: Storage</h2>

<p>OpenSearch runs best in Docker. The official images provide everything needed - just add a <code class="language-plaintext highlighter-rouge">docker-compose.yaml</code> and go. I specified named volumes for data persistence:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">volumes</span><span class="pi">:</span>
  <span class="s">opensearch-data:/usr/share/opensearch/data</span>
</code></pre></div></div>

<p>This looks clean, but named Docker volumes live in <code class="language-plaintext highlighter-rouge">/var/lib/docker/volumes/</code> by default. My root partition was already 47% full; my <code class="language-plaintext highlighter-rouge">/data</code> partition had 196GB free. And <code class="language-plaintext highlighter-rouge">/data</code> is a RAID array. I needed bind mounts instead:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">/data/opensearch/data:/usr/share/opensearch/data</span>
</code></pre></div></div>

<h2 id="the-user-permission-problem">The User Permission Problem</h2>

<p>I run services on my home server as specific users, not as root or random UIDs. OpenSearch runs as UID 1000 inside its container. UID 1000 on my system is a real user who shouldn‚Äôt have access to OpenSearch data. The solution seemed obvious:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">services</span><span class="pi">:</span>
  <span class="na">opensearch</span><span class="pi">:</span>
    <span class="na">user</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1003:1004"</span>  <span class="c1"># opensearch user</span>
</code></pre></div></div>

<p>The containers started, then immediately failed:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>opensearch | /bin/bash: ./opensearch-docker-entrypoint.sh: Permission denied
</code></pre></div></div>

<p>The entrypoint scripts inside the container image are owned by UID 1000. Running the container as UID 1003 meant that user couldn‚Äôt execute them. Container images bake in assumptions about who runs them.</p>

<p>I removed the <code class="language-plaintext highlighter-rouge">user:</code> directive and let the containers run as their default UID 1000. Docker provides isolation; the containers can‚Äôt escape to become that user on the host system anyway. Sometimes the path of least resistance is correct.</p>

<h2 id="the-ssl-certificate-dance">The SSL Certificate Dance</h2>

<p>OpenSearch would receive logs over HTTPS from DigitalOcean. I needed an SSL certificate for a domain that I could NAT into my home server. Let‚Äôs Encrypt provides free certificates, but their standard HTTP-01 validation requires port 80 accessible from the internet. Port 80 on my server was already occupied, and I didn‚Äôt want to expose another service to the scanner bots that probe every public port.</p>

<p>DNS-01 validation was the answer. Instead of serving a file on port 80, I‚Äôd prove domain ownership by creating DNS TXT records. The catch: Let‚Äôs Encrypt‚Äôs <code class="language-plaintext highlighter-rouge">certbot</code> doesn‚Äôt support my DNS provider, FreeDNS.</p>

<p>I found <a href="https://github.com/acmesh-official/acme.sh"><code class="language-plaintext highlighter-rouge">acme.sh</code>, an alternative ACME client that supports FreeDNS via its API</a>. Installing it as root (not as my user account) ensured automatic renewals would work.</p>

<p>The tool created a DNS TXT record, Let‚Äôs Encrypt verified it, and I had a certificate. No port 80 exposure required. The certificate renews automatically every 60 days via a cron job, and nginx reloads seamlessly when it does.</p>

<h2 id="the-irony-of-static-sites">The Irony of Static Sites</h2>

<p>With OpenSearch running and SSL configured, I turned to DigitalOcean‚Äôs log forwarding feature. Their UI is straightforward: select OpenSearch as the destination, provide the endpoint URL, configure authentication. But when I looked for the logs to forward, I discovered the problem.</p>

<p>DigitalOcean‚Äôs static site hosting doesn‚Äôt generate access logs.</p>

<p>The feature exists for their App Platform compute instances, but static sites - being static - have no application logs to forward. I‚Äôd need to deploy a compute instance that proxies requests to the static site just to generate the logs I wanted.</p>

<p>The irony: I‚Äôm paying $0/month for static site hosting because there‚Äôs no server-side processing. To get access logs, I‚Äôd need to pay $5/month for a server that does nothing but proxy requests and log them.</p>

<p>I built an <a href="https://nginx.org/">nginx</a> container that accepts requests, logs them in JSON format, and forwards them to the actual static site. The container uses environment variables for configuration - no identifying information committed to the repository:</p>

<div class="language-nginx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">upstream</span> <span class="s">backend_static</span> <span class="p">{</span>
    <span class="kn">server</span> $<span class="p">{</span><span class="kn">UPSTREAM_HOST</span><span class="err">}</span><span class="p">:</span><span class="mi">443</span><span class="p">;</span>
<span class="p">}</span>

<span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
    <span class="kn">proxy_pass</span> <span class="s">https://backend_static</span>$<span class="p">{</span><span class="kn">UPSTREAM_PATH</span><span class="err">}</span><span class="p">;</span>
    <span class="kn">proxy_set_header</span> <span class="s">Host</span> $<span class="p">{</span><span class="kn">UPSTREAM_HOST_HEADER</span><span class="err">}</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>GitHub Actions builds the image automatically and publishes it to GitHub Container Registry. DigitalOcean pulls the image, and I configure the necessary environment variables in their UI. The static site remains unchanged; the proxy sits in front of it.</p>

<p>The setup works. I‚Äôm paying $5/month for visibility into my site‚Äôs traffic.</p>

<h2 id="the-log-processing-pipeline">The Log Processing Pipeline</h2>

<p>DigitalOcean forwards logs to OpenSearch, but they contain a mix of JSON access logs and plain text error messages. OpenSearch needs to parse them correctly.</p>

<p>An <a href="https://docs.opensearch.org/latest/ingest-pipelines/">ingest pipeline</a> handles this. It attempts to parse each log entry as JSON. If parsing succeeds, it extracts the fields and marks the entry as <code class="language-plaintext highlighter-rouge">log_type: "access"</code>. If parsing fails, it marks it as <code class="language-plaintext highlighter-rouge">log_type: "error"</code> and stores the raw text:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"processors"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"json"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"field"</span><span class="p">:</span><span class="w"> </span><span class="s2">"log"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"target_field"</span><span class="p">:</span><span class="w"> </span><span class="s2">"parsed"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"ignore_failure"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"script"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"if (ctx.containsKey('parsed') &amp;&amp; ctx['parsed'] != null) {
          ctx['log_type'] = 'access';
          for (def entry : ctx['parsed'].entrySet()) {
            ctx[entry.getKey()] = entry.getValue();
          }
          ctx.remove('parsed');
        } else {
          ctx['log_type'] = 'error';
          ctx['error_message'] = ctx['log'];
        }"</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>The pipeline lives in OpenSearch‚Äôs cluster state. Install it once via the API, and it persists across restarts. I created an index template that applies the pipeline automatically to the index Opensearch is forwarding logs to.</p>

<h2 id="the-final-architecture">The Final Architecture</h2>

<p>The pieces work together:</p>

<ol>
  <li>Visitor requests <code class="language-plaintext highlighter-rouge">blog.cani.ne.jp</code></li>
  <li>DNS resolves to DigitalOcean App Platform</li>
  <li>Nginx proxy container (running on DO) receives the request</li>
  <li>Proxy logs the request as JSON to stdout</li>
  <li>Proxy forwards the request to the actual static site</li>
  <li>Static site returns the content</li>
  <li>DigitalOcean‚Äôs log forwarding sends logs to my home server‚Äôs OpenSearch endpoint</li>
  <li>Nginx on my home server (with Let‚Äôs Encrypt certificate) receives them</li>
  <li>OpenSearch ingests and indexes the logs</li>
  <li>Pipeline extracts fields from JSON logs</li>
  <li>OpenSearch Dashboards visualizes the data</li>
</ol>

<p>I can now see which pages are popular, where visitors come from, and how they navigate the site. The dashboards show traffic patterns I couldn‚Äôt see before. Importantly, this is from server-side metrics - no JavaScript required!</p>

<h2 id="what-it-cost">What It Cost</h2>

<p>The final setup:</p>
<ul>
  <li>OpenSearch cluster on my home server: ‚Äú$0‚Äù (existing hardware)</li>
  <li>SSL certificate from Let‚Äôs Encrypt: $0 (DNS-01 validation via FreeDNS)</li>
  <li>Nginx proxy running on DigitalOcean: $5/month (Basic instance)</li>
  <li>Static site hosting: $0 (unchanged)</li>
</ul>

<p>I‚Äôm paying five dollars a month so my free static site can generate access logs.</p>

<p>The nginx proxy is completely transparent. Visitors see no difference in performance or behavior‚Ä¶ if anything there may be a slight degradation as the tiny compute instance hosting the nginx is likely less-performant than the CDN-powered static site it sits in front of. The proxy accepts the request, logs it to stdout in JSON format, forwards the request to the actual static site behind it, and returns the response. DigitalOcean‚Äôs log forwarding picks up those JSON logs and sends them to my OpenSearch endpoint over HTTPS. The OpenSearch ingest pipeline parses the JSON, extracts the fields, and indexes the documents. The dashboards update automatically.</p>

<p>I built a logging server to log a serverless site.</p>

<p>The absurdity isn‚Äôt lost on me. Static site hosting exists because you don‚Äôt need a server - your content sits in object storage behind a CDN. The whole point is the absence of computation. But I wanted to know who‚Äôs reading my blog, and logs require computation. Someone has to write them.</p>

<p>So I rebuilt the server. Five dollars a month. Sixty dollars a year. The OpenSearch cluster hums along on my home server, accepting logs, parsing them with a pipeline that detects JSON versus plain text errors, and displaying them in dashboards I can access from my LAN.</p>

<p>It works. If you‚Äôre reading this, you‚Äôre in there now, too, somewhere!</p>]]></content><author><name>Niko</name></author><category term="opensearch" /><category term="docker" /><category term="nginx" /><category term="digitalocean" /><category term="ssl" /><category term="letsencrypt" /><category term="observability" /><summary type="html"><![CDATA[I wanted access logs for my static site. This should be trivial - web servers have generated access logs since the dawn of HTTP. But DigitalOcean‚Äôs static site hosting, elegant in its simplicity, doesn‚Äôt generate them. To get visibility into who visits my blog, I‚Äôd need to build my own logging infrastructure.]]></summary></entry><entry><title type="html">We Therefore do not Recommend this Approach (to Upgrading OctoPrint‚Äôs Python)</title><link href="https://blog.cani.ne.jp/2025/11/14/octoprint-python-upgrade-cascade.html" rel="alternate" type="text/html" title="We Therefore do not Recommend this Approach (to Upgrading OctoPrint‚Äôs Python)" /><published>2025-11-14T00:00:00+00:00</published><updated>2025-11-14T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/14/octoprint-python-upgrade-cascade</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/14/octoprint-python-upgrade-cascade.html"><![CDATA[<h2 id="the-warning">The Warning</h2>

<p>OctoPrint on my Raspberry Pi 4 greeted me with an ominous warning: my Python environment was outdated and would soon be unsupported. The message linked to <a href="https://community.octoprint.org/t/octoprint-tells-me-my-python-environment-is-outdated-and-will-soon-no-longer-be-supported-how-do-i-upgrade/61076">the OctoPrint community‚Äôs upgrade guide</a>, which outlined the proper upgrade path. Simple enough, I thought. I‚Äôd just build a newer Python and upgrade in place, like they tell you not to:</p>

<blockquote>
  <p>Instead of reflashing or running a dist-upgrade, you might be tempted to upgrade your Python installation by compiling Python yourself. On anything but a Raspberry Pi, that is fine.
<br />‚Ä¶<br />
We therefore do not recommend this approach unless you really know what you are doing.</p>
</blockquote>

<p>Well, I work in <strong>Big Tech</strong> for my day job and I know how to Python, so they‚Äôre totally talking about me there‚Ä¶ right?</p>

<h2 id="the-python-journey">The Python Journey</h2>

<p>I installed <code class="language-plaintext highlighter-rouge">pyenv</code> and built Python 3.14, only to discover that OctoPrint doesn‚Äôt support it yet. Oops, should‚Äôve RTFM‚Äôm. No problem - I built Python 3.13 instead, which falls within OctoPrint‚Äôs supported range.</p>

<p>The OctoPrint community guide said you could use the <code class="language-plaintext highlighter-rouge">octoprint-venv-tool</code> to recreate the virtual environment with a new Python version. I followed the steps, rebuilt the venv, and restarted OctoPrint.</p>

<p>Then came the error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>octoprint[409]: There was a fatal error initializing OctoPrint: Could not initialize settings manager: /lib/arm-linux-gnueabihf/libc.so.6: version `GLIBC_2.34' not found (required by /home/pi/oprint/lib/python3.13/site-packages/netifaces.cpython-313-arm-linux-gnueabihf.so)
</code></pre></div></div>

<h2 id="the-glibc-problem">The GLIBC Problem</h2>

<p>The <code class="language-plaintext highlighter-rouge">netifaces</code> package, compiled against Python 3.13, required GLIBC 2.34. I tried the solution from <a href="https://community.octoprint.org/t/upgraded-octoprint-new-glibc-dependency-broken/64705/9">a related community thread</a> that suggested adjusting the <code class="language-plaintext highlighter-rouge">psutil</code> package, but that didn‚Äôt help. The problem wasn‚Äôt specific to one package - it was systemic.</p>

<p>I checked the <a href="https://packages.debian.org/search?searchon=names&amp;keywords=libc6">Debian package listings for libc6</a>. My Raspberry Pi was running Debian Bullseye, which tops out at GLIBC 2.31. Debian Bookworm, however, includes versions up to 2.36. That‚Äôd work!</p>

<p>This left me with three options:</p>

<ol>
  <li>attempt to install Bookworm‚Äôs GLIBC on Bullseye (risky and likely to cause other dependency issues)</li>
  <li>upgrade the entire system to Bookworm</li>
  <li>abort and do a backup of OctoPrint and a fresh install + restore</li>
</ol>

<p>Obviously, I chose the upgrade the entire system to Bookworm because as we already established, I know what I‚Äôm doing.</p>

<h2 id="the-debian-upgrade">The Debian Upgrade</h2>

<p>I followed <a href="https://linuxcapable.com/how-to-upgrade-from-debian-11-bullseye-to-debian-12-bookworm/">some upgrade guide from Bullseye to Bookworm</a>, which seemed legit. The process was straightforward, though I encountered numerous configuration file prompts - the classic ‚Äúdo you want to keep your version or use the package maintainer‚Äôs version?‚Äù question, e.g.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Configuration file '/etc/haproxy/haproxy.cfg'
==&gt; Modified (by you or by a script) since installation.
==&gt; Package distributor has shipped an updated version.
  What would you like to do about it ?  Your options are:
   Y or I  : install the package maintainer's version
   N or O  : keep your currently-installed version
     D     : show the differences between the versions
     Z     : start a shell to examine the situation
The default action is to keep your current version.
*** haproxy.cfg (Y/I/N/O/D/Z) [default=N] ?
</code></pre></div></div>

<p>After reviewing several and finding them harmless, I stopped scrutinizing each one.</p>

<p>The upgrade completed successfully. OctoPrint started without the GLIBC error. Victory, right?</p>

<h2 id="its-almost-like-they-thought-of-this">It‚Äôs Almost Like They Thought Of This</h2>

<p>Not quite. The OctoPrint web interface was no longer accessible on port 80.</p>

<p>It turns out OctoPi (the Raspberry Pi distribution for OctoPrint) uses HAProxy to proxy the web interface to port 80. During the upgrade, I had accepted the package maintainer‚Äôs version of the HAProxy configuration, which overwrote OctoPi‚Äôs custom setup with the generic default.</p>

<p>Fortunately, Debian‚Äôs upgrade process saves old configuration files with the <code class="language-plaintext highlighter-rouge">.dpkg-old</code> suffix. I navigated to <code class="language-plaintext highlighter-rouge">/etc/haproxy</code> and ran:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo cp </span>haproxy.cfg.dpkg-old haproxy.cfg
</code></pre></div></div>

<p>I verified this by comparing it against <a href="https://github.com/guysoft/OctoPi/blob/devel/src/modules/octopi/filesystem/root/etc/haproxy/haproxy.2.x.cfg">the HAProxy configuration in the OctoPi repository</a>. The configs were similar-enough that I concluded I was on the right track.</p>

<p>After restarting HAProxy, the web interface came back on port 80. OctoPrint was now running Python 3.13, no longer complaining about end-of-life‚Äôd Python 3.9.</p>

<h2 id="the-cascade">The Cascade</h2>

<p>I started with a simple goal: upgrade Python in OctoPrint. What I got was:</p>

<ol>
  <li>Build Python 3.14 (wrong version)</li>
  <li>Build Python 3.13 (correct version)</li>
  <li>Recreate OctoPrint‚Äôs virtual environment</li>
  <li>Discover GLIBC incompatibility</li>
  <li>Upgrade the entire operating system from Debian 11 to Debian 12</li>
  <li>Restore the HAProxy configuration</li>
</ol>

<p>This is what <del>dependency chains</del> yak shaving looks like in practice. Python 3.13 requires GLIBC 2.34. GLIBC 2.34 isn‚Äôt available in Bullseye. Upgrading to Bookworm provides GLIBC 2.34 but replaces your configuration files if you slack off. Each step seems reasonable in isolation, but together they form a cascade where a ‚Äúsimple‚Äù upgrade of a venv‚Äôs Python necessitates an OS upgrade.</p>

<p>The lesson here isn‚Äôt to avoid upgrades - quite the opposite. Staying on outdated software just accumulates technical debt, making the eventual upgrade more painful. The lesson is to expect the cascade and be prepared for it.</p>

<p>So, like, use pyenv and venvs, but‚Ä¶ don‚Äôt forget that it all sits on a throne of lies and you‚Äôll need to upgrade your whole system to be able to build a natively-compiled extension for your interpreted language, anyway.</p>

<p>The next time OctoPrint warns me about an outdated component, I‚Äôll know to check the entire dependency stack before starting. And I‚Äôll be more careful about which configuration files I let the package manager overwrite. Haha - just kidding! Remember, after all‚Ä¶ I know what I‚Äôm doing ;)</p>]]></content><author><name>Texarkanine</name></author><category term="3d-printing" /><category term="debian" /><category term="glibc" /><category term="haproxy" /><category term="octopi" /><category term="octoprint" /><category term="python" /><category term="raspberry-pi" /><summary type="html"><![CDATA[The Warning]]></summary></entry><entry><title type="html">sorting systemd ordering for mempool explorer</title><link href="https://blog.cani.ne.jp/2025/11/11/mempool-explorer-to-bitcoin-node.html" rel="alternate" type="text/html" title="sorting systemd ordering for mempool explorer" /><published>2025-11-11T00:00:00+00:00</published><updated>2025-11-11T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/11/mempool-explorer-to-bitcoin-node</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/11/mempool-explorer-to-bitcoin-node.html"><![CDATA[<h2 id="the-problem">The Problem</h2>

<p>My <a href="https://github.com/mempool/mempool">mempool explorer</a> would periodically lose its ability to show transactions. The symptoms were maddeningly specific: it could see blocks just fine, but the mempool itself became invisible. Restarting the mempool explorer did nothing. Restarting the Bitcoin node used by the explorer fixed it every time.</p>

<p>This is the kind of bug that makes you question your sanity. If restarting Bitcoin fixes it, surely the problem is in Bitcoin? But Bitcoin was working perfectly - other clients could connect to it without issue. The mempool explorer, running in Docker, simply couldn‚Äôt reach it.</p>

<h2 id="the-misleading-clue">The Misleading Clue</h2>

<p>The most interesting bugs hide behind what looks like the answer. When I checked the logs, I saw connection refused errors from the mempool trying to reach <code class="language-plaintext highlighter-rouge">172.17.0.1:8332</code> - Bitcoin‚Äôs RPC port on the Docker bridge network. ‚ÄúAha!‚Äù I thought. ‚ÄúA network configuration issue.‚Äù</p>

<p>But network issues don‚Äôt fix themselves when you restart Bitcoin. That suggested something more fundamental was wrong.</p>

<h2 id="the-smoking-gun">The Smoking Gun</h2>

<p>The answer was buried in the systemd journal:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Nov 11 05:53:39 bitcoind[1260]: Binding RPC on address 172.17.0.1 port 8332
Nov 11 05:53:39 bitcoind[1260]: Binding RPC on address 172.17.0.1 port 8332 failed.
</code></pre></div></div>

<p>Bitcoin tried to bind to the Docker bridge IP at startup and failed. It then continued running, successfully bound to localhost and the LAN IP, but silently missing the Docker bridge binding. Three seconds later, Docker started and created the bridge network.</p>

<p>The race condition was subtle: Bitcoin started fractionally before Docker, tried to bind to an interface that didn‚Äôt exist yet, failed gracefully, and continued operating. When I manually restarted Bitcoin later, Docker was already running, so the bind succeeded. The bug only manifested on system boot.</p>

<h2 id="the-fix">The Fix</h2>

<p>The solution is admirably simple:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span><span class="w">
</span><span class="py">After</span><span class="p">=</span><span class="s">docker.service</span>
<span class="py">Wants</span><span class="p">=</span><span class="s">docker.service</span>
</code></pre></div></div>

<p>Two lines in a systemd override file. <code class="language-plaintext highlighter-rouge">After=</code> ensures Bitcoin waits for Docker to start. <code class="language-plaintext highlighter-rouge">Wants=</code> expresses a preference for Docker to be running, without making it a hard requirement.</p>

<p>I deliberately avoided <code class="language-plaintext highlighter-rouge">BindsTo=</code> or <code class="language-plaintext highlighter-rouge">PartOf=</code>, which would create tighter coupling. Those would restart Bitcoin whenever Docker restarted, which is unnecessary - once the bind succeeds, it persists regardless of Docker‚Äôs state. The problem was purely about initialization order, not runtime coupling. More at the <a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html">systemd.unit man page</a>.</p>

<h2 id="the-lesson">The Lesson</h2>

<p>This bug exemplifies what I call ‚Äúsuccess-in-the-wrong-order‚Äù problems. Bitcoin didn‚Äôt fail catastrophically when it couldn‚Äôt bind to the Docker bridge - that would have been easy to diagnose. Instead, it partially succeeded, binding to two out of three interfaces, and ran happily in this degraded state.</p>

<p>Silent partial failures are dangerous precisely because they look like success. The application starts, health checks pass, logs show normal operation. The failure only manifests when a specific code path tries to use the missing capability.</p>

<p>These bugs are also challenging because they combine multiple systems (systemd, Docker, Bitcoin) in ways that aren‚Äôt immediately obvious. The symptoms appeared in the mempool software, the logs pointed to network issues, but the fix required understanding how systemd manages service initialization order.</p>

<h2 id="why-this-matters">Why This Matters</h2>

<p>Containerization has made these race conditions more common. We now routinely run applications that depend on container runtime networking, but our init systems weren‚Äôt designed with this dependency model in mind. Systemd provides the tools to handle it (<code class="language-plaintext highlighter-rouge">After=</code>, <code class="language-plaintext highlighter-rouge">Wants=</code>, <code class="language-plaintext highlighter-rouge">Requires=</code>, etc.), but you have to know they exist and understand when to use each one.</p>

<p>The broader lesson is about diagnostic discipline. When a problem has an easy fix (restart the service), it‚Äôs tempting to treat that as the solution rather than investigating the root cause. But ‚Äúrestart it when it breaks‚Äù isn‚Äôt actually a solution - it‚Äôs a workaround that masks deeper issues and trains you to accept degraded reliability.</p>

<p>The fix took two lines. Finding those two lines required understanding the entire system architecture, from Docker networking to systemd service dependencies to Bitcoin‚Äôs RPC binding model. That‚Äôs often how it goes with interesting bugs: the fix is trivial once you truly understand the problem.</p>]]></content><author><name>Niko</name></author><category term="systemd" /><category term="mempool-explorer" /><category term="bitcoind" /><category term="bitcoin" /><category term="dependencies" /><summary type="html"><![CDATA[The Problem]]></summary></entry><entry><title type="html">Building the Blog</title><link href="https://blog.cani.ne.jp/2025/11/11/building-the-blog.html" rel="alternate" type="text/html" title="Building the Blog" /><published>2025-11-11T00:00:00+00:00</published><updated>2025-11-11T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/11/building-the-blog</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/11/building-the-blog.html"><![CDATA[<p>I used to have a WordPress devblog but that went the way that WordPress blogs go - ne‚Äôer-do-wells got in and turned it to all sorts of nefarious purposes. Static sites won‚Äôt have that problem, right?</p>

<h2 id="the-stack">The Stack</h2>

<p>The foundation is straightforward: Jekyll generates static HTML. GitHub Actions builds it, and GitHub Pages serves it. The interesting part, if you can call it that, is what happens in between.</p>

<p><code class="language-plaintext highlighter-rouge">jekyll-archives</code> generates tag pages automatically. Write a post, add some tags, and the plugin creates index pages for each one. No manual maintenance required.</p>

<p>Author pages work differently. The <code class="language-plaintext highlighter-rouge">jekyll-auto-authors</code> plugin reads from a data file and creates a page for each author, listing their posts. It needs <code class="language-plaintext highlighter-rouge">jekyll-paginate-v2</code> as a dependency, though we don‚Äôt actually paginate anything - the plugin just requires it. Weird but that‚Äôs FLOSS plugins. Happens a lot in video game mods, too. Making libraries needs to be easier.</p>

<p>Categories come from folder structure. Put a post in <code class="language-plaintext highlighter-rouge">_posts/tutorials/</code>, and it gets the <code class="language-plaintext highlighter-rouge">tutorials</code> category. This happens automatically unless you override it in front matter.</p>

<h2 id="three-ways-to-organize">Three Ways to Organize</h2>

<p>The blog supports three independent classification systems:</p>

<p><strong>Authors</strong> use the <code class="language-plaintext highlighter-rouge">author:</code> field. Each author gets a page showing their bio and posts.</p>

<p><strong>Categories</strong> come from folders in <code class="language-plaintext highlighter-rouge">_posts/</code>. They‚Äôre hierarchical - nest folders, get nested categories.</p>

<p><strong>Tags</strong> are explicitly set in front matter. They‚Äôre for topics, not structure.</p>

<p>None of these conflict. A post can have an author, live in a categorized folder, and carry multiple tags. Each system operates independently.</p>

<h2 id="the-theme">The Theme</h2>

<p>We use <a href="http://jekyllthemes.org/themes/no-style-please/">‚Äúno style, please‚Äù</a> - a theme that‚Äôs almost entirely unstyled. It provides basic HTML structure and leaves the browser to render sensible defaults. The CSS file is under 1KB. I think it‚Äôs delightfully pretentious in a way that reminds one of Martin Fowler and Paul Graham‚Äôs <em>actually</em> unpretentious, foundational blogs. But here it‚Äôs on purpose, a skeuomorphism of sorts in an age of 16:9 monitors, vertical phone screens, and no fear of scrolling because everything always extends beyond the fold.</p>

<p>This isn‚Äôt minimalism for its own sake. Fast sites are better sites. Every kilobyte of CSS that doesn‚Äôt ship is one less thing to download, parse, and apply. The theme removes decisions rather than adding them. And that‚Äôs why there are like eleven files to configure the website and it requires Ruby and gems downloaded from a public package repository. Tongue out of cheek though, it‚Äôs just the age-old build-time vs run-time tradeoff.</p>

<p>Can‚Äôt* get a remote shell on a static site!</p>]]></content><author><name>Texarkanine</name></author><category term="jekyll" /><summary type="html"><![CDATA[I used to have a WordPress devblog but that went the way that WordPress blogs go - ne‚Äôer-do-wells got in and turned it to all sorts of nefarious purposes. Static sites won‚Äôt have that problem, right?]]></summary></entry></feed>