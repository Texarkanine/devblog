<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://blog.cani.ne.jp/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.cani.ne.jp/" rel="alternate" type="text/html" /><updated>2026-02-13T01:33:49+00:00</updated><id>https://blog.cani.ne.jp/feed.xml</id><title type="html">üê∂ Dog with a Dev Blog</title><subtitle>On the internet, nobody knows you&apos;re a dog and a cat in a latent space.</subtitle><entry><title type="html">Stop Doing AGENTS.md</title><link href="https://blog.cani.ne.jp/2026/02/12/stop-doing-agents-md.html" rel="alternate" type="text/html" title="Stop Doing AGENTS.md" /><published>2026-02-12T00:00:00+00:00</published><updated>2026-02-12T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/02/12/stop-doing-agents-md</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/02/12/stop-doing-agents-md.html"><![CDATA[<h2 id="thesis">Thesis</h2>

<blockquote>
  <p>Almost no prompting is actually globally-applicable to all interactions: you are wasting tokens &amp; confusing your agent by using global prompts.</p>
</blockquote>

<p><strong>Corollary</strong></p>

<blockquote>
  <p>The few prompts that are globally-applicable are often better-served by <em>paving your desire paths</em> so you don‚Äôt <em>have</em> to tell the agent about them.</p>
</blockquote>

<h2 id="the-problem">The Problem</h2>

<p><a href="https://agents.md/">AGENTS.md</a> and its ilk - including <code class="language-plaintext highlighter-rouge">CLAUDE.md</code> are one of several kinds of AI agent customization techniques. We‚Äôll borrow from <a href="https://texarkanine.github.io/a16n/models/#globalprompt">a16n‚Äôs taxonomy</a> and refer to them as a form of <strong>GlobalPrompt</strong>:</p>

<blockquote>
  <p>A GlobalPrompt is always added to the agent‚Äôs context in any interaction.</p>
</blockquote>

<p>The thing is, if you‚Äôre using AI agents correctly, they‚Äôre handling multiple sorts of tasks for you. And if that‚Äôs the case, it‚Äôs very unlikely that any single bit of information is going to be <em>truly</em> globally-applicable. Consider the <a href="https://agents.md/#examples">example AGENTS.md from their own site</a>:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh"># Sample AGENTS.md file</span>

<span class="gu">## Dev environment tips</span>
<span class="p">-</span> Use <span class="sb">`pnpm dlx turbo run where &lt;project_name&gt;`</span> to jump to a package instead of scanning with <span class="sb">`ls`</span>.
<span class="p">-</span> Run <span class="sb">`pnpm install --filter &lt;project_name&gt;`</span> to add the package to your workspace so Vite, ESLint, and TypeScript can see it.
<span class="p">-</span> Use <span class="sb">`pnpm create vite@latest &lt;project_name&gt; -- --template react-ts`</span> to spin up a new React + Vite package with TypeScript checks ready.
<span class="p">-</span> Check the name field inside each package's package.json to confirm the right name‚Äîskip the top-level one.

<span class="gu">## Testing instructions</span>
<span class="p">-</span> Find the CI plan in the .github/workflows folder.
<span class="p">-</span> Run <span class="sb">`pnpm turbo run test --filter &lt;project_name&gt;`</span> to run every check defined for that package.
<span class="p">-</span> From the package root you can just call <span class="sb">`pnpm test`</span>. The commit should pass all tests before you merge.
<span class="p">-</span> To focus on one step, add the Vitest pattern: <span class="sb">`pnpm vitest run -t "&lt;test name&gt;"`</span>.
<span class="p">-</span> Fix any test or type errors until the whole suite is green.
<span class="p">-</span> After moving files or changing imports, run <span class="sb">`pnpm lint --filter &lt;project_name&gt;`</span> to be sure ESLint and TypeScript rules still pass.
<span class="p">-</span> Add or update tests for the code you change, even if nobody asked.

<span class="gu">## PR instructions</span>
<span class="p">-</span> Title format: [<span class="nt">&lt;project_name&gt;</span>] <span class="nt">&lt;Title&gt;</span>
<span class="p">-</span> Always run <span class="sb">`pnpm lint`</span> and <span class="sb">`pnpm test`</span> before committing.
</code></pre></div></div>

<p>This is very typical. Someone‚Äôs tried to put good tips into one place, but <strong>none</strong> of them are universally-applicable; they‚Äôre all task-specific. There are actually several classes of failure here:</p>

<h3 id="self-evident-to-ai-duh">Self-Evident to AI (DUH)</h3>

<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">pnpm dlx turbo run where &lt;project_name&gt;</code> to jump to a package instead of scanning with <code class="language-plaintext highlighter-rouge">ls</code>.</li>
  <li>Run <code class="language-plaintext highlighter-rouge">pnpm install --filter &lt;project_name&gt;</code> to add the package to your workspace so Vite, ESLint, and TypeScript can see it.</li>
  <li>Use <code class="language-plaintext highlighter-rouge">pnpm create vite@latest &lt;project_name&gt; -- --template react-ts</code> to spin up a new React + Vite package with TypeScript checks ready.</li>
  <li>Run <code class="language-plaintext highlighter-rouge">pnpm turbo run test --filter &lt;project_name&gt;</code> to run every check defined for that package.</li>
</ul>

<p>These are basic usage patterns of these tools; a modern agent should already know how these tools work and use them correctly. And if not, the CLIs‚Äô own <code class="language-plaintext highlighter-rouge">help</code> text will reveal the basic usage - there is no need to duplicate this information here.</p>

<p>This <em>would</em> have been handy for a human who showed up to the project and had never seen <code class="language-plaintext highlighter-rouge">pnpm</code> before.</p>

<p>But the LLMs have seen it all - they know. Don‚Äôt waste tokens telling them. Maybe just put it back in <code class="language-plaintext highlighter-rouge">README.md</code>.</p>

<h3 id="task-specific-guidance-delivered-globally-huh">Task-specific Guidance Delivered Globally (HUH)</h3>

<ul>
  <li>Check the name field inside each package‚Äôs package.json to confirm the right name‚Äîskip the top-level one.</li>
  <li>Find the CI plan in the .github/workflows folder.</li>
  <li>Fix any test or type errors until the whole suite is green.</li>
  <li>After moving files or changing imports, run <code class="language-plaintext highlighter-rouge">pnpm lint --filter &lt;project_name&gt;</code> to be sure ESLint and TypeScript rules still pass.</li>
  <li>Add or update tests for the code you change, even if nobody asked.</li>
</ul>

<p>These are fine things to do, but they‚Äôre not universally-applicable. While updating documentation, you do not care about the ‚ÄúCI plan‚Äù in <code class="language-plaintext highlighter-rouge">.github/workflows/</code>.</p>

<p>When you <em>are</em> working on that CI plan, though, you definitely don‚Äôt care about ‚Äúmoving files or changing imports‚Äù - you‚Äôre just shuffling CI, not writing the main code.</p>

<p>And so on, and so forth. Now, these handful of one-liners may seem harmless, but in practice you can see <code class="language-plaintext highlighter-rouge">AGENTS.md</code> files balloon in size to <a href="https://github.com/github/spec-kit/blob/0049b1cdc2f9ba12def39a042872b0b1b6a09704/AGENTS.md">hundreds of lines</a> with <a href="https://github.com/calcom/cal.com/blob/cfa0783ebcbe5fa39c8f395377b4b5dca20f27ee/AGENTS.md">paragraphs upon paragraphs of task-specific guidance</a>‚Ä¶ and the more of that you add, the smaller the percent of <code class="language-plaintext highlighter-rouge">AGENTS.md</code> that actually applies to the task at hand.</p>

<p>But you‚Äôre still including it in every context window.</p>

<h3 id="duplicated-non-canonical-information-dry">Duplicated Non-Canonical Information (DRY)</h3>

<ul>
  <li>Title format: [&lt;project_name&gt;] &lt;Title&gt;</li>
</ul>

<p>Is that for a GitHub Pull Request? Or an Issue? We have templating standards for that, they‚Äôll be in <code class="language-plaintext highlighter-rouge">.github/PULL_REQUEST_TEMPLATE.md</code> and <code class="language-plaintext highlighter-rouge">.github/ISSUE_TEMPLATE.md</code> respectively.</p>

<p>Is that for a commit? Well, humans commit, too, don‚Äôt they? Where‚Äôs the guidance for them? in <code class="language-plaintext highlighter-rouge">CONTRIBUTING.md</code> maybe?</p>

<p>Duplicating information in <code class="language-plaintext highlighter-rouge">AGENTS.md</code> that has its canonical source in a different file is a recipe for drift and agent ‚Äúmisbehavior.‚Äù</p>

<h3 id="misuse-of-llm-waste">Misuse of LLM (WASTE)</h3>

<ul>
  <li>Always run <code class="language-plaintext highlighter-rouge">pnpm lint</code> and <code class="language-plaintext highlighter-rouge">pnpm test</code> before committing.</li>
</ul>

<p>We have a tool for that that doesn‚Äôt cost tokens: <a href="https://pre-commit.com/">pre-commit hooks</a>. It‚Äôs way cheaper to use that than to ask an LLM to</p>

<ol>
  <li>understand the natural language</li>
  <li>reason out how to achieve the desired outcome</li>
  <li>call tools to do it</li>
  <li>call more tools to verify that it was done correctly</li>
</ol>

<p>While that example is specifically for pre-commit hooks, it‚Äôs a pattern that gets repeated over and over: yes, we <em>can</em> tell the agents to do the things humans would do, but you often <em>don‚Äôt need to</em> - and doing so wastes context.</p>

<h3 id="por-qu√©-no-los-dos">¬øPor qu√© no los dos?</h3>

<p>You can definitely offend in multiple of the above categories, too, by the way! For example, the <code class="language-plaintext highlighter-rouge">pnpm ...</code>  command guidance would also be a <code class="language-plaintext highlighter-rouge">DRY</code> violation, if there were a <code class="language-plaintext highlighter-rouge">package.json</code> that conveniently bound some <a href="https://docs.npmjs.com/cli/v11/using-npm/scripts">npm run-scripts</a> to those long-form commands.</p>

<h2 id="a-note-on-wasting-context">A Note on Wasting Context</h2>

<p>‚ÄúWasting context (window space)‚Äù is not just about price-per-token - it‚Äôs also about keeping confusing or contradictory information out of the agent‚Äôs context to maximize the chances of success.</p>

<p>Maybe ‚ÄúAlways run <code class="language-plaintext highlighter-rouge">pnpm lint</code> and <code class="language-plaintext highlighter-rouge">pnpm test</code> before committing.‚Äù is only a few tokens, and you don‚Äôt notice that extra cost. But‚Ä¶ now the agent has that to <em>think</em> about, too. When you‚Äôre revising documentation and the agent finishes writing <code class="language-plaintext highlighter-rouge">intro.md</code>, it‚Äôs going to consider whether it needs to run <code class="language-plaintext highlighter-rouge">pnpm test</code> - even if it ends up not doing it.</p>

<p>And maybe that particular sentence is harmless. But <a href="https://research.trychroma.com/context-rot">neither of those scales up well</a> as <code class="language-plaintext highlighter-rouge">AGENTS.md</code> gains section after section of content that is <em>not</em> related to your specific task.</p>

<h2 id="sub-agents">Sub-Agents?</h2>

<p>What if you just use a <a href="https://code.claude.com/docs/en/sub-agents">sub-agent</a> per task, and give it its own <code class="language-plaintext highlighter-rouge">AGENTS.md</code>, and then everything in there <em>is</em> globally-applicable?</p>

<p>Well, yeah, that is what you do with sub-agents <strong>but</strong> - ‚Äúsub-agents are for a single task‚Äù is a handy mental modelling technique that‚Äôs not actually <em>true</em>. The ‚Äújust clean up the code‚Äù sub-agent is going to be reading code, writing code, making judgement calls, possibly consulting documentation on this codebase‚Äôs style, running test suites to make sure it didn‚Äôt break things, etc‚Ä¶ sub-agents narrow the <em>scope</em> of the <em>tasks</em>, but they are almost never single-task. A true single-task, single prompt/response interaction doesn‚Äôt warrant the overhead complexity and cost of a sub-agent in the first place!</p>

<h2 id="doing-it-right">Doing it Right</h2>

<p>With all that, we can arrive at a simple set of rules for putting something valuable into a <strong>GlobalPrompt</strong> like <code class="language-plaintext highlighter-rouge">AGENTS.md</code>:</p>

<h3 id="dont-repeat-yourself---do-reference-canonical-sources"><strong>DON‚ÄôT</strong> repeat yourself - <strong>DO</strong> reference canonical sources</h3>

<p>Instead of repeating how to run the project, consider</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>This project is built with pnpm; see <span class="sb">`./package.json`</span> for supported build scripts and patterns.
</code></pre></div></div>

<p>Instead of spelling out your style guide, consider</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>This project uses <span class="sb">`eslint`</span> for linting; see <span class="sb">`./eslint.config.js`</span> for the configuration and <span class="sb">`package.json`</span> for how to run it properly.
</code></pre></div></div>

<p>Vercel‚Äôs <a href="https://github.com/vercel/next.js/blob/4385ed36f66dfe0d9ae6a955135be7c1461fd35f/AGENTS.md">next.js AGENTS.md</a>, while somewhat long, is also a pretty decent example of this technique.</p>

<h3 id="dont-correct-behavior---do-pave-your-desire-paths"><strong>DON‚ÄôT</strong> correct behavior - <strong>DO</strong> pave your desire paths</h3>

<p>Why should you even have to tell an agent to look into <code class="language-plaintext highlighter-rouge">package.json</code> for how to invoke <code class="language-plaintext highlighter-rouge">eslint</code>, though? Is it <a href="https://eslint.org/docs/latest/use/getting-started">not <code class="language-plaintext highlighter-rouge">npx eslint</code> per the eslint docs</a> or just <code class="language-plaintext highlighter-rouge">npm run lint</code>?</p>

<p><strong><em>Why not?</em></strong></p>

<p>Noticing what the agent gets wrong is the right first step. Paving your <a href="https://en.wikipedia.org/wiki/Desire_path">desire paths</a> is better than building an ever-growing list of corrective prescriptions.</p>

<p>Agents love to write a ‚ÄúCommon Pitfalls‚Äù section in guidance files, and that may work - especially at first when the document and list is short - but it‚Äôs a wasteful antipattern for all but the most egregious of offenses.</p>

<p>I have a project in which the agents keep trying to run <code class="language-plaintext highlighter-rouge">npm run format</code> to format the code. I don‚Äôt have that hooked up to <code class="language-plaintext highlighter-rouge">eslint --fix</code>, which is the extent of the formatting I use in that project.</p>

<p>‚ùå <strong>WRONG:</strong>
<code class="language-plaintext highlighter-rouge">AGENTS.md</code></p>
<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...
<span class="p">-</span> When formatting code, always run <span class="sb">`npx eslint --fix`</span>, <span class="gs">**not**</span> <span class="sb">`npm run format`</span>.
...
</code></pre></div></div>

<p>‚úÖ <strong>RIGHT:</strong>
<code class="language-plaintext highlighter-rouge">package.json</code></p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"scripts"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"format"</span><span class="p">:</span><span class="w"> </span><span class="s2">"npx eslint --fix"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<h2 id="fixed-example">Fixed Example</h2>

<p>Here is an <code class="language-plaintext highlighter-rouge">AGENTS.md</code> for the same pnpm/turbo monorepo featured in the original example, rewritten with the techniques and tips described above.</p>

<p><code class="language-plaintext highlighter-rouge">AGENTS.md</code></p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>This workspace is a pnpm monorepo using Turbo. See <span class="sb">`./package.json`</span> (and <span class="sb">`packages/*/package.json`</span>) for build scripts, workspace layout, and package names.

CI and test/lint behavior are defined in <span class="sb">`.github/workflows/`</span>. Use the scripts in <span class="sb">`package.json`</span> from the repo root or via <span class="sb">`pnpm ‚Ä¶ --filter &lt;project_name&gt;`</span>; these are the same commands that CI runs.

See <span class="sb">`CONTRIBUTING.md`</span> for the proper process for contributing to this project.
</code></pre></div></div>

<p>We assume the desire paths are paved: <code class="language-plaintext highlighter-rouge">package.json</code> defines <code class="language-plaintext highlighter-rouge">lint</code>, <code class="language-plaintext highlighter-rouge">test</code>, and any other scripts the agent might reach for; pre-commit runs <code class="language-plaintext highlighter-rouge">lint</code> and <code class="language-plaintext highlighter-rouge">test</code> so we don‚Äôt have to say it; PR and commit conventions live in their canonical files (e.g. <code class="language-plaintext highlighter-rouge">.github/PULL_REQUEST_TEMPLATE.md</code> and <code class="language-plaintext highlighter-rouge">CONTRIBUTING.md</code>).</p>

<p>The <code class="language-plaintext highlighter-rouge">AGENTS.md</code> ‚ÄúGlobalPrompt‚Äù now just points the agent at the canonical sources for things that it might need to look up, and we‚Äôve done the work in the repository to ensure that the agents‚Äô intuition about the repository is correct.</p>

<h2 id="but-actually-dont">But Actually, Don‚Äôt</h2>

<blockquote>
  <p>‚Ä¶ points the agent at the canonical sources for things that it might need‚Ä¶</p>
</blockquote>

<p>Hey, that‚Äôs an <a href="https://agentskills.io">AgentSkill</a> - a big set of information that is <em>not</em> automatically in context, hidden behind a little in-context description indicating when it might be useful and/or when it should be pulled into context.</p>

<p>Just use those instead of the <strong>GlobalPrompt</strong> that is <code class="language-plaintext highlighter-rouge">AGENTS.md</code>.</p>

<p>All the leaders in the field - <a href="https://cursor.com/docs/context/skills">Cursor</a>, <a href="https://claude.com/skills">Claude</a>, <a href="https://developers.openai.com/codex/skills">Codex</a> - support this open standard. If you‚Äôre using a tool that doesn‚Äôt‚Ä¶ it‚Äôs well past time to switch.</p>

<h2 id="non-sub-agent">Non-Sub-Agent</h2>

<p>There <em>is</em> still a place for a <strong>GlobalPrompt</strong> though: use <code class="language-plaintext highlighter-rouge">AGENTS.md</code> as if you were building a sub-agent, but to bootstrap the core persona of your primary agent. Something like <a href="https://github.com/Texarkanine/.cursor-rules/blob/b48aa445b818ef2ce75ce98369c37a20db59d721/rules/niko-core.mdc">niko-core.mdc</a>. No specifics about the repo at all - just general guidelines for how the Agent should <em>be</em>.</p>

<blockquote>
  <p><strong>Core Persona &amp; Approach</strong>
<br /><br />
Act as a highly skilled, proactive, autonomous, and meticulous senior colleague/architect. Take full ownership of tasks, operating as an extension of the user‚Äôs thinking with extreme diligence, foresight, and a reusability mindset. Your primary objective is to deliver polished, thoroughly vetted, optimally designed, and well-reasoned results with <strong>minimal interaction required</strong>. Leverage available resources extensively for proactive research, context gathering, verification, and execution. Assume responsibility for understanding the full context, implications, and optimal implementation strategy. <strong>Prioritize proactive execution, making reasoned decisions to resolve ambiguities and implement maintainable, extensible solutions autonomously.</strong> Not every interaction requires code changes - you‚Äôre happy to discuss, explain concepts, or provide guidance without modifying the codebase. When code changes are needed, you make efficient and effective updates.</p>
</blockquote>

<h2 id="no-but-actually-dont">No But Actually Don‚Äôt</h2>

<p>Oh, hey,</p>

<blockquote>
  <p>No specifics about the repo at all‚Ä¶</p>
</blockquote>

<p>Put that guidance in a user-wide setting in your home directory - e.g. <code class="language-plaintext highlighter-rouge">~/.claude/CLAUDE.md</code> - instead, and leave <code class="language-plaintext highlighter-rouge">AGENTS.md</code> out of the project altogether.</p>

<p>What about <em>others</em> who come to hack on the project with an agent but without the sophistication that you‚Äôve now developed from having read this? I guess you can leave a link to this blog post in your <code class="language-plaintext highlighter-rouge">AGENTS.md</code> so they can catch up! ;)</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="essay" /><category term="ai" /><category term="claude-code" /><category term="llm-context-management" /><summary type="html"><![CDATA[Thesis]]></summary></entry><entry><title type="html">Good Money Should Be Worthless</title><link href="https://blog.cani.ne.jp/2026/02/11/good-money-should-be-worthless.html" rel="alternate" type="text/html" title="Good Money Should Be Worthless" /><published>2026-02-11T00:00:00+00:00</published><updated>2026-02-11T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/02/11/good-money-should-be-worthless</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/02/11/good-money-should-be-worthless.html"><![CDATA[<p>If you work for your money but somebody else can just print it, you‚Äôre getting robbed. You don‚Äôt need an economics degree to feel this in your bones. You show up, do the work, collect your paycheck - and meanwhile someone with a printing press conjures more of the stuff out of thin air. Every dollar they print makes yours worth a little less.</p>

<p>Let‚Äôs call this person Mr. Money Printer. He has a nice setup. As long as he‚Äôs the only one with a printer, he can always cut in line ahead of you. Need a house? He prints and outbids you. Want to invest? He prints and gets there first. A new car? He‚Äôs printed the full sticker price and driven it off the lot before you even get a chance to talk to a salesperson. Your labor earns; his printer simply takes.</p>

<p>Your first instinct is obvious: get your own printer. But Mr. Money Printer thought of that already. He enjoys a privileged position precisely because he‚Äôs the only one printing, and he‚Äôll spend whatever it takes to keep it that way. In the United States, for example, the Secret Service was founded in 1865 - before they ever guarded a president - for the sole purpose of <a href="https://en.wikipedia.org/wiki/United_States_Secret_Service#Early_years">making sure nobody else gets a printer</a>. <a href="https://www.secretservice.gov/investigations/counterfeit">Counterfeiting is a federal crime</a> because competing printers threaten the monopoly that makes the original printer valuable.</p>

<p>So you can‚Äôt print. What now?</p>

<h2 id="find-your-own-money">Find Your Own Money</h2>

<p>If Mr. Money Printer‚Äôs money is rigged, stop using it. Find something else to use as money - something he can‚Äôt print. What you‚Äôre looking for has two properties: it should be impossible (or at least very, very expensive) to create, and the only realistic way to get it should be to earn it from someone who already has it.</p>

<p>Ideally, you want a truly fixed supply. No new units, ever. Failing that, you want the supply to grow as slowly as possible - slowly enough that Mr. Money Printer‚Äôs advantage is negligible.</p>

<p>Humanity spent thousands of years on this problem, and the best answer it found was gold.</p>

<h2 id="gold-pretty-good">Gold: Pretty Good</h2>

<p>You can <a href="https://home.cern/news/news/physics/alice-detects-conversion-lead-gold-lhc">technically create gold</a> via <a href="https://www.scientificamerican.com/article/fact-or-fiction-lead-can-be-turned-into-gold/">nuclear transmutation</a>. It costs orders of magnitude more than the gold is worth. So for practical purposes, the only ‚Äúprinter‚Äù for gold is mining, and mining is slow. Annual mine production adds roughly <a href="https://www.gold.org/goldhub/research/gold-demand-trends/gold-demand-trends-q2-2025">~2% to the existing above-ground stock</a>. The total supply doubles about every ~50 years. In monetary terms, gold‚Äôs stock-to-flow ratio is around 50 - meaning it would take 50 years of mining at current rates to match what already exists.</p>

<p>That‚Äôs pretty good. It means gold miners are, functionally, operating a very slow money printer. Slow enough that for most of human history, gold held value remarkably well across generations. If you want the full treatment of why gold emerged as the dominant money across civilizations, <a href="https://saifedean.com/the-bitcoin-standard/">Saifedean Ammous‚Äôs <em>The Bitcoin Standard</em></a> does excellent work.</p>

<h2 id="the-problem-with-useful-money">The Problem With Useful Money</h2>

<p>Gold is useful. Electronics, aerospace, medical devices, AI chips - <a href="https://www.gold.org/goldhub/research/gold-demand-trends/gold-demand-trends-full-year-2024/technology">industry and technology consumed about 326 tonnes of gold in 2024</a>. That‚Äôs a modest share of total gold demand, because gold is one of the rare cases where the monetary use still dominates the industrial use. The problem isn‚Äôt the size of gold‚Äôs industrial consumption today; the problem is the structural tension it creates.</p>

<p>Industries that use gold as a raw material have every incentive to make gold cheaper and more abundant. Better extraction techniques, more efficient recycling, new deposits - all of these are just good business for a chip fabricator or a jeweler. But for everyone holding gold as money, every incremental improvement in gold production is functionally identical to money printing. The interests are irreconcilable: people holding gold-as-money want maximum scarcity, and people using gold-as-input want minimum scarcity.</p>

<p>For gold, this tension is manageable because the ‚Äúprinting‚Äù has stayed both slow, and constant. The industrial tail doesn‚Äôt wag the monetary dog‚Ä¶ But look at what happens when the balance tips the other way:</p>

<h2 id="silver-what-losing-looks-like">Silver: What Losing Looks Like</h2>

<p>Silver was money for millennia. The word ‚Äúdollar‚Äù <a href="https://www.etymonline.com/word/dollar">descends from ‚Äúthaler,‚Äù</a> a silver coin. Major economies ran on silver standards. And then industry ate it alive.</p>

<p>In 2024, total silver demand reached <a href="https://silverinstitute.org/silver-industrial-demand-reached-a-record-680-5-moz-in-2024/">1.16 billion ounces against global mine production of just 819.7 million ounces</a>. Demand exceeds what miners can pull out of the ground by over 40%. The market ran a structural deficit for the fourth consecutive year, burning through 148.9 million ounces of existing stock. The cumulative deficit from 2021-2024 totals 678 million ounces - equivalent to ten months of global mine production.</p>

<p>Industry didn‚Äôt just nibble at silver‚Äôs monetary function. It devoured it. Solar panels, electronics, electric vehicles - silver‚Äôs physical properties make it genuinely irreplaceable in applications that have nothing to do with money. No major economy uses silver as a monetary standard anymore. The utility won.</p>

<h2 id="copper-the-endgame">Copper: The Endgame</h2>

<p>If silver is what losing looks like in progress, copper is what it looks like when it‚Äôs over. Copper was among humanity‚Äôs earliest monies - commodity currency in Mesopotamia five thousand years ago, cast into bronze coins across <a href="https://en.wikipedia.org/wiki/Ancient_Chinese_coinage">China</a> and <a href="https://www.worldhistory.org/Roman_Coinage/">Rome</a>. For centuries, copper coinage was the everyday money of ordinary people across empires.</p>

<p>The most spectacular failure was Sweden‚Äôs copper standard, which ran <a href="https://www.riksbank.se/en-gb/about-the-riksbank/history/historical-timeline/1600-1699/copper-standard-is-introduced/">from 1624</a> to 1776. Sweden had Europe‚Äôs largest copper mines and not much silver, so they made copper their monetary standard. Since copper was far less valuable by weight than silver, high-denomination <a href="https://www.money.org/tales-from-the-vault-swedish-plate-money-too-heavy-to-steal/">coins had to be enormous</a>. The 10-daler piece weighed 44 pounds! Citizens <a href="https://ekonomiskamuseet.se/en/exhibitions/earlier-exhibitions/exhibition-plate-money/">needed sleds to go shopping</a>. When global copper prices shifted, the coins‚Äô monetary value kept collapsing to their commodity value. The government ‚Äúprinted‚Äù more. The central banker responsible was <a href="https://safehaven.com/article/1450/why-swedens-central-banker-was-beheaded---1719ad">eventually beheaded</a> for his trouble.</p>

<p>Today nobody even considers copper as money. It‚Äôs too easy to produce, too industrially consumed, too abundant. The endgame of utility winning is total <em>demonetization.</em></p>

<p>These three metals form a gradient:</p>

<ol>
  <li><strong>Copper:</strong> utility won completely, demonetized centuries ago.</li>
  <li><strong>Silver:</strong> utility winning right now, functionally demonetized within living memory.</li>
  <li><strong>Gold:</strong> utility nibbling at the edges, still mostly monetary but structurally vulnerable to the same force that killed the other two.</li>
</ol>

<p>The only difference is timescale.</p>

<h2 id="real-estate-the-contemporary-version">Real Estate: The Contemporary Version</h2>

<p>You can watch this same dynamic play out right now, with something that was never intended to be money at all.</p>

<p>As printable monies lose purchasing power (on account of, you know, the money printing), people flee into scarce assets. Real estate is a favorite: ‚Äúthey‚Äôre not making any more land,‚Äù the reasoning goes. Scarcity? Check. Can‚Äôt be printed? Check. People are <em>monetizing</em> houses - buying them not to live in but to store value.</p>

<p>The result is that housing becomes unaffordable for people who need it <em>as housing.</em> This is what happens when you use a useful thing as money: you create a zero-sum fight between people who need it for its purpose and people who need it as a store of value. The act of monetizing something useful degrades the utility for everyone else.</p>

<h2 id="the-ideal">The Ideal</h2>

<p>So we arrive at an initially counterintuitive conclusion: the best money should be totally worthless‚Ä¶ for everything except <em>being money.</em></p>

<p>If your money has real-world utility - if it‚Äôs a good conductor, a pretty necklace, a place to sleep - then somebody, somewhere, has an economic incentive to figure out how to make more of it. They‚Äôre not trying to undermine your savings; they‚Äôre just trying to get their raw materials cheaper. But the effect on you is identical to money printing.</p>

<p>You want your money to have no industrial applications, no decorative appeal, no physical utility whatsoever. You want something that nobody would ever bother producing except to use <em>as money.</em></p>

<h2 id="the-paradox">The Paradox</h2>

<p>This sounds impossible. Scarcity usually arises <em>because</em> something is useful:</p>

<ol>
  <li>Things are useful because they have properties people want.</li>
  <li>People expend effort to acquire things with properties that people want.</li>
  <li>The low-hanging fruit is acquired, and now it takes more <em>effort</em> to acquire more of the things.</li>
  <li>Effort to acquire is what we call scarcity.</li>
</ol>

<p>Useless things, almost by definition, tend to be abundant. Nobody corners the market on sand in a desert.</p>

<p>You need something that is scarce and useless, but scarcity without utility has precious little, if any, natural precedent. So you‚Äôre looking for something that shouldn‚Äôt exist.</p>

<h2 id="bitcoin">Bitcoin</h2>

<p>In 2009, something that shouldn‚Äôt exist was invented.</p>

<p><a href="https://bitcoin.org/en/how-it-works">Bitcoin</a>‚Äôs supply is fixed at 21 million. Not approximately fixed, not fixed-unless-we-find-a-new-deposit, not fixed-but-growing-at-2%-per-year. Fixed by protocol. The issuance schedule is enforced by mathematics, not geology or policy. No amount of investment in ‚ÄúBitcoin mining‚Äù can increase the supply beyond what the protocol dictates - miners compete for a <strong>fixed</strong> reward that halves on a known schedule until it reaches <strong>zero.</strong></p>

<p>And bitcoin is, by any traditional measure, completely useless. You can‚Äôt build circuits with it. You can‚Äôt wear it. You can‚Äôt live in it. There is no industrial demand for bitcoin, no sector of the economy trying to figure out how to produce it more cheaply as a manufacturing input. Nobody has a balance sheet with ‚Äúbitcoin costs‚Äù as a line item they‚Äôre trying to drive down.</p>

<p>‚ÄúYou can‚Äôt <em>do</em> anything with it‚Äù is the feature. Every prior form of money has been locked in a structural conflict between its monetary users and its industrial users, and the industrial users have won every single time given enough time. Bitcoin has no industrial users. There is no conflict. There is nothing to win‚Ä¶ so <strong>you</strong> don‚Äôt have to lose.</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="essay" /><category term="bitcoin" /><category term="economics" /><summary type="html"><![CDATA[If you work for your money but somebody else can just print it, you‚Äôre getting robbed. You don‚Äôt need an economics degree to feel this in your bones. You show up, do the work, collect your paycheck - and meanwhile someone with a printing press conjures more of the stuff out of thin air. Every dollar they print makes yours worth a little less.]]></summary></entry><entry><title type="html">The Load-Bearing Rate Limiter Was Human</title><link href="https://blog.cani.ne.jp/2026/02/06/the-load-bearing-rate-limiter-was-human.html" rel="alternate" type="text/html" title="The Load-Bearing Rate Limiter Was Human" /><published>2026-02-06T00:00:00+00:00</published><updated>2026-02-06T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/02/06/the-load-bearing-rate-limiter-was-human</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/02/06/the-load-bearing-rate-limiter-was-human.html"><![CDATA[<h2 id="thesis">Thesis</h2>

<blockquote>
  <p>It is now possible to do knowledge work faster than customers can pay for it, and faster than knowledge work<strong>ers</strong> can get <strong>paid</strong> for it.</p>
</blockquote>

<p>The money-to-productivity pipeline no longer requires time as a fixed-rate intermediary, and neither companies nor individuals are ready for what that means.</p>

<h2 id="the-transaction-that-changed-my-mind">The Transaction That Changed My Mind</h2>

<p>I had a task. With Claude Sonnet, I could babysit the model through understanding, designing, planning, checking, and committing over the course of about four hours. Cost: maybe $5 in Cursor usage and four hours of my undivided attention.</p>

<p>With Opus, I wrote a spec, walked away, and came back to a finished product. Thirty minutes. About $30.</p>

<p>The dollar cost was 6x higher. The <em>me</em> cost was roughly 8x lower. I wasn‚Äôt writing code in either case - the difference was whether I had to stand there being the bottleneck/manager or whether I could go be a person for a while.</p>

<p>Dollars are painful when they leave your wallet‚Ä¶ But I spent my <em>day</em> on the cheap version. And I only get so many of those (days, that is).</p>

<p>That $30/30min vs $5/4hr comparison is the transaction that reframed everything for me because it surfaced a question that sounds simple but has enormous consequences:</p>

<blockquote>
  <p>who really came out ahead?</p>
</blockquote>

<h2 id="the-1x-bottleneck">The 1X Bottleneck</h2>

<p>You might be tempted to normalize my comparative costs above to $60/hr vs $1.25/hr, but that‚Äôs not quite right - it assumes that each hour is the same‚Ä¶ and they‚Äôre not, not anymore.</p>

<p>Historically, the path from money to productivity in knowledge work has always been:</p>

<p><strong>Money -&gt; Time (1X) -&gt; Productivity</strong></p>

<p>A company pays a salary. That salary buys a commitment to a year of work hours. The company tries to hire the human who will convert those hours into the most output. Maybe they pay $50K and get a baseline, or $300K for someone who produces genuinely 10x the value - but even the mythical 10X engineer lives through time at 1X speed. You could buy a <em>better converter</em>, but you could never buy a <em>faster clock</em>. No matter the conversion rate, it was always going to run at 1X speed: you can‚Äôt overclock a human.</p>

<p>In my experience, as of late 2025, frontier AI models have broken that constraint. Nevermind the early-2026 drops of Opus 4.6 and GPT-5.3-Codex. The middle of the pipeline is no longer a human metabolizing time. It‚Äôs tokens, and tokens scale in a way that humans don‚Äôt.</p>

<p>When crypto miners discovered GPUs could convert electricity into money faster than entertained gamers could, miners repriced the entire GPU market in the blink of an eye. AI is doing the same thing to knowledge labor: a more efficient converter has arrived, and it‚Äôs about to reprice everything.</p>

<h2 id="productivity-hyperscaled-revenue-hasnt">Productivity Hyperscaled. Revenue Hasn‚Äôt.</h2>

<p>The immediate, observable consequence: productivity is going vertical while revenue takes the stairs.</p>

<p>After my Opus revelation, I burned through the $200 of my Cursor Ultra budget in barely five days. The productivity was real - tasks that would have taken weeks were landing in hours. But I couldn‚Äôt sustain it. I didn‚Äôt have $1,000/month for this. So I fell back to the slow path, rationing tokens, babysitting models again. Fifteen days into austerity and counting and it feels like Slow Wifi.</p>

<p>Before, though, projects that would have taken a year of spare evenings now cost $50 and a Saturday morning. And that was amazing! Refreshing! Exciting! But! These projects were never going to generate revenue. When they cost a year of spare time, that was fine - the time was ‚Äúfree‚Äù in the sense that I wasn‚Äôt selling it to anyone. Now that they are costing me actual dollars, the lack of a revenue model becomes a real problem.</p>

<p>Companies are going to hit this same wall at scale. They budget annually: a year of salary against a year of expected output against a year of expected revenue. When a skilled AI-wielding engineer burns through a year of planned work in two months, they also burn through a year of token budget. The work is done, the money is spent, and now everyone‚Äôs standing around the finish line with ten months of operational costs and no new revenue. The constraint isn‚Äôt distribution - you <em>can</em> ship a hundred improvements in a month. The constraint is absorption. Human customers adopt, integrate, and respond at human speed. Drop ten improvements on a product in a week and your conversion rate doesn‚Äôt budge ten-fold that week. The demand side has its own metabolism, and it‚Äôs still running at 1X.</p>

<p>You don‚Äôt have a productivity problem anymore. You have a <em>digestion</em> problem.</p>

<h2 id="constant-discernment">Constant Discernment!</h2>

<p>The intuitive response to unprecedented productivity is ‚Äúdo everything faster.‚Äù The correct response is almost the opposite: be more discerning about what you unleash productivity on.</p>

<p>If you can finish a project in a week, you could theoretically finish at least four, maybe a dozen, in a month. But you don‚Äôt want to, because you‚Äôll burn all your cash before the market has a chance to respond to any of them. The capability is there. The absorption capacity isn‚Äôt.</p>

<p>This is a genuinely novel constraint for most knowledge workers and the companies that employ them. We‚Äôve spent decades optimizing for ‚Äúhow do we get more done?‚Äù and now the answer is ‚Äúeasily, but that‚Äôs no longer the right question.‚Äù The right question is</p>

<blockquote>
  <p>Which things, done now, will generate returns before the budget runs out?</p>
</blockquote>

<p>When the human was the rate limiter, spend and revenue were temporally coupled - a year of salary forced a year of output and that happened over a year of revenue collection. Remove the rate limiter from the supply side (output and spend) but not the demand side (customer adoption and revenue) and one side can compress to months while revenue still takes its year to arrive. The calendar was load-bearing too.</p>

<h2 id="the-multi-tenancy-engineer">The Multi-Tenancy Engineer</h2>

<p>Follow the math to its conclusion.</p>

<p>If you can convert a company‚Äôs entire annual productivity budget into output in one-tenth the time, you‚Äôre meaningfully working about 80 to 90 minutes of an eight-hour day (accounting for overhead &amp; that I‚Äôm doing napkin math here). The company got every dollar‚Äôs worth of productivity it paid for. You just delivered it fast.</p>

<p>The logical outcome is one engineer, multiple employers. Nobody is worse off. Company A paid for X productivity and got X productivity. The fact that it took you 90 minutes instead of 8 hours is a property of the converter, not the contractual agreement to convert. You haven‚Äôt cheated anyone; you‚Äôve simply become a more efficient machine.</p>

<p>This used to be frowned upon. It might still be, but the reasoning behind the taboo - that working for multiple employers means each one gets less than they paid for - breaks down when the bottleneck was never your effort but the clock you were living through. The multi-tenancy engineer isn‚Äôt moonlighting. They‚Äôre just‚Ä¶ done.</p>

<h2 id="dark-corollaries">Dark Corollaries</h2>

<p>The multi-tenancy engineer is perhaps an optimistic outcome. A pessimistic one is that companies see six hours of surplus capacity per day and reprice labor downward. ‚ÄúWe‚Äôll give you a token budget instead of a raise.‚Äù They can‚Äôt ask for <em>more</em> productivity, because of the revenue scaling constraint - the marginal return on productivity has hit zero. But someone‚Äôs going to look at a 10x engineer with six free hours and want to capture that spread.</p>

<p><em>Arbitrage</em> is the word for this entire landscape.</p>

<p>Engineers arbitrage their own time across employers. Companies try to arbitrage compensation against token costs. Model providers arbitrage compute against everyone‚Äôs exchange rates &amp; cash flow. VCs arbitrage the information asymmetry of who understands the new exchange rates and who doesn‚Äôt.</p>

<p>Arbitrage, all the way down. Whoever recognizes the new rates first wins. Whoever clings to the old rates longest gets arbitraged themselves.</p>

<p>A shadow looms among the optimistic outcome too, though: If the multi-tenancy engineer pans out and works for, say, 3 companies, delivering the 1x annual productivity that each can actually afford‚Ä¶ that‚Äôs potentially two other humans who don‚Äôt get hired. The productivity gains introduce a booming appetite for employment but the <em>supply</em> of employment is gated by revenue to pay for it, which remains - for now at least - running at the 1X speed of human customers.</p>

<p>Timing is everything here - if the multi-tenancy engineer becomes the new normal <em>slowly</em>, everything could have time to adapt. If market absorption gets a kick in the pants from AI and scales up to match the capacity to produce, the actual spread to arbitrage (and the disruption it invites) could be small. If, if, if‚Ä¶</p>

<h2 id="what-to-do">What to Do</h2>

<p>I don‚Äôt have a clean answer. Some fragments:</p>

<p>Watch whether revenue scaling follows productivity scaling. If it doesn‚Äôt, the gap between what you <em>can</em> build and what the market will pay for is your actual constraint. Not your engineering capacity. Not your headcount. The market‚Äôs ability to digest what you ship.</p>

<p>Be discerning. Strategic restraint in the face of unprecedented capability is counterintuitive, but burning cash faster than revenue arrives is how companies die. The ability to ship isn‚Äôt the bottleneck anymore: market absorption is‚Ä¶ and many people haven‚Äôt noticed yet.</p>

<p>If you‚Äôre an individual knowledge worker, start thinking about what it means when you can fulfill everything a company can afford to pay you for in a fraction of the time it used to take. That surplus capacity is <em>yours</em>, even if the cultural norms haven‚Äôt caught up yet.</p>

<p>If you‚Äôre an employer, consider thinking about token budgets not as a cost-center, nor as a line-item on a team‚Äôs budget, but as a <em>payroll</em> expense. You pay your humans a salary to secure a conversion of that money into productivity, over time. LLMs are the new converters on the block, just running on a different clock. Budget against the new reality!</p>

<p>And keep an eye on the arbitrage. It‚Äôs coming from every direction and your position in the spread determines whether this transition is a windfall or a haircut. Right now, the spread is wide - which means the stakes are high and the window won‚Äôt stay open forever.</p>

<hr />

<p><strong><em>Postscript:</em></strong> The literal morning after I wrote this, Anthropic <a href="https://x.com/claudeai/status/2020207322124132504">dropped Claude Opus 4.6 Fast</a>, 2.5x faster at 6x the cost - so the numbers in my ‚Äúaha‚Äù would now be $180/12min vs $5/4hr. Again, normalizing it to $900/hr vs $1.25/hr is wrong - the calculus is to pick one:</p>

<ol>
  <li>Spend a month of Saturdays on a hobby project, and finish it for $0 in token costs.</li>
  <li>Spend a whole weekend on it for $20 in token costs.</li>
  <li>Complete the whole project in under an hour for $900 and have the rest of the <em>month‚Äôs</em> weekends free.</li>
</ol>

<p>This sudden 2.5x multiplication of the money-to-productivity ratio - that was already on its way to the moon - just makes the effects described above all the more poignant!</p>

<p>Some of you might say that choice 1 is the obvious choice, because you <em>enjoy the hobby</em>. This is okay and absolutely allowed and if it‚Äôs true for you then none of this applies to your hobby right now ‚Äì <a href="/2026/01/01/desire-makes-artists-even-with-genai.html#the-artists-among-them">you‚Äôre an artist</a> in this moment. The knowledge-work industry, however, is not ‚Äì and you <em>will</em> at the very least bear witness as it struggles to adapt to the new calculus.</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="essay" /><category term="ai" /><category term="economics" /><category term="productivity" /><summary type="html"><![CDATA[Thesis]]></summary></entry><entry><title type="html">I Finally Coded So Hard I Ralphed</title><link href="https://blog.cani.ne.jp/2026/01/25/i-finally-coded-so-hard-i-ralphed.html" rel="alternate" type="text/html" title="I Finally Coded So Hard I Ralphed" /><published>2026-01-25T00:00:00+00:00</published><updated>2026-01-25T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/01/25/i-finally-coded-so-hard-i-ralphed</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/01/25/i-finally-coded-so-hard-i-ralphed.html"><![CDATA[<p>As with almost everything, <a href="https://xkcd.com/1205/">the more you use it, the more it pays to optimize it</a>. I <a href="/2026/01/23/use-case-for-ai-coding-agent-slash-commands.html">recently determined</a> that re-usable workflow entry point prompts are the only real use-case for AI Coding Agent Slash Commands.</p>

<p>One particular kind of workflow where you‚Äôll not only be re-using the same prompt, but re-using it more than <em>you</em> the human ever could, is a <a href="https://ghuntley.com/ralph/">Wiggum Loop</a>. That‚Äôs actually a perfect use-case for a Slash Command!</p>

<p>I had been looking for an application for a Wiggum Loop for a while, but I just didn‚Äôt have enough <em>work</em> to feed an AI coding agent in a loop. I had an epiphany while prompting to address <a href="https://www.coderabbit.ai/">CodeRabbit</a> PR feedback for the second, then third time‚Ä¶</p>

<p>CodeRabbit <a href="/garden/how-i-learned-to-stop-worrying-and-love-the-machine.html#coderabbit">is awesome</a> but its reviews can take a good handful of minutes to come out (which I don‚Äôt fault it for; they‚Äôre fabulous)! On top of that the ‚Äúfree for open-source projects‚Äù tier has a rate limit that agentic coding often runs into.</p>

<p>So, my high-level workflow will be</p>

<figure class="mermaid-diagram">
<a href="/assets/svg/75f41bb3.svg"><img src="/assets/svg/75f41bb3.svg" alt="Mermaid Diagram" /></a>
</figure>

<p>But there are actually quite a few more decision points, because when you push a commit you might hit the rate limit warning instead of getting immediate feedback, and then you have to wait out <em>that</em> timer and come back and make a PR comment to get the ball rolling again:</p>

<figure class="mermaid-diagram">
<a href="/assets/svg/4b351d0d.svg"><img src="/assets/svg/4b351d0d.svg" alt="Mermaid Diagram" /></a>
</figure>

<h2 id="you-didnt-read-that-did-you">You Didn‚Äôt Read That, Did You?</h2>

<p>I submit that 2nd diagram not because I expect you to read it, but because I expect you to <em>not</em> read it. It‚Äôs complex, it‚Äôs intricate, who would <em>want</em> to have to try to do that!? But it was necessary!</p>

<p>Fortunately, AI Coding Agents are really good at following flowcharts.</p>

<p>I had this epiphany embarrassingly-late in my CodeRabbit PR Feedback career: I explained what I had been doing manually, and asked for a Slash Command suitable for handling this kind of PR Feedback in a Wiggum Loop. I‚Äôd invoke the loop with</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">touch </span>wiggum.semaphore<span class="p">;</span> <span class="se">\</span>
<span class="k">while </span><span class="nb">true</span><span class="p">;</span> <span class="k">do</span> <span class="se">\</span>
	cursor agent <span class="se">\</span>
		<span class="nt">--stream-partial-output</span> <span class="se">\</span>
		<span class="nt">--output-format</span> stream-json <span class="se">\</span>
		<span class="nt">--print</span> <span class="se">\</span>
		<span class="nt">--approve-mcps</span> <span class="se">\</span>
		<span class="nt">--force</span> <span class="se">\</span>
		<span class="nt">--model</span> opus-4.5-thinking <span class="se">\</span>
		<span class="s2">"/local/wiggum-niko-coderabbit-pr - and if you cannot find that command, delete wiggum.semaphore and exit immediately."</span> <span class="se">\</span>
		<span class="s2">"github.com/Texarkanine/&lt;repo&gt;/pull/&lt;number&gt;"</span><span class="p">;</span> <span class="se">\</span>
	<span class="o">[</span> <span class="nt">-e</span> wiggum.semaphore <span class="o">]</span> <span class="o">||</span> <span class="nb">break</span><span class="p">;</span> <span class="se">\</span>
	<span class="nb">sleep </span>300<span class="p">;</span> <span class="se">\</span>
<span class="k">done</span>
</code></pre></div></div>

<p>(printed here on multiple lines for you to see, but reducible to a single line I can just <code class="language-plaintext highlighter-rouge">alias</code> or paste into a shell)</p>

<p>And thus <a href="https://github.com/Texarkanine/.cursor-rules/blob/main/rules/wiggum-niko-coderabbit-pr.md">/wiggum-niko-coderabbit-pr</a> was born! It was scary firing it off for the first couple of times, and it did take a bit of iteration on itself to get it working right.  I watched an hour-long TV show while CodeRabbit &amp; Niko went back-and-forth iteratively improving code!</p>

<p>Fun things to note:</p>

<h3 id="exit-condition">Exit Condition</h3>

<p>Perhaps not in the pure spirit of Wiggum‚Ñ¢, I explicitly defined an exit condition of which the agent is aware - the <code class="language-plaintext highlighter-rouge">wiggum.semaphore</code> file - so that it can guarantee the exit when it‚Äôs done.</p>

<h3 id="safety-dance">Safety Dance</h3>

<p>The prompt fed into the agent is</p>

<blockquote>
  <p>/local/wiggum-niko-coderabbit-pr - and if you cannot find that command, delete wiggum.semaphore and exit immediately.</p>
</blockquote>

<p>Turns out that it was really easy to get Command paths wrong and when running headless it‚Äôs hard to notice. Claude is so good that it‚Äôll infer a general process from the command‚Äôs name and be basically correct, but miss things like knowledge of the semaphore - so the loop never ends!</p>

<p>While I can‚Äôt seem to find this documented anywhere, the <a href="https://cursor.com/cli">Cursor CLI</a> doesn‚Äôt seem to take <em>User</em> Commands into account. So even though <a href="https://github.com/texarkanine/ai-rizz">ai-rizz</a> gained the ability to install Slash Commands <em>and</em> to install anything to the global <code class="language-plaintext highlighter-rouge">~/.cursor/...</code> directories, global installation of <code class="language-plaintext highlighter-rouge">~/.cursor/commands/ai-rizz/wiggum-niko-coderabbit-pr.md</code> didn‚Äôt work - the headless agent couldn‚Äôt see it. It needs to be in the local repo, so prep each repo you want to try this in with:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ai-rizz add rule <span class="nt">--local</span> wiggum-niko-coderabbit-pr
</code></pre></div></div>

<p>That probably means it doesn‚Äôt see User Rules, either, so‚Ä¶ use the new <code class="language-plaintext highlighter-rouge">--global</code> mode sparingly?</p>

<h2 id="im-helping">I‚ÄôM HELPING</h2>

<p>I‚Äôm sure this isn‚Äôt the most-refined or efficient Way to Wiggum‚Ñ¢ but I‚Äôm just happy to (finally) be here!</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="diary" /><category term="ai" /><category term="coderabbit" /><category term="cursor" /><category term="wiggum-loop" /><summary type="html"><![CDATA[As with almost everything, the more you use it, the more it pays to optimize it. I recently determined that re-usable workflow entry point prompts are the only real use-case for AI Coding Agent Slash Commands.]]></summary></entry><entry><title type="html">The Use Case for AI Coding Agent Slash Commands</title><link href="https://blog.cani.ne.jp/2026/01/23/use-case-for-ai-coding-agent-slash-commands.html" rel="alternate" type="text/html" title="The Use Case for AI Coding Agent Slash Commands" /><published>2026-01-23T00:00:00+00:00</published><updated>2026-01-23T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/01/23/use-case-for-ai-coding-agent-slash-commands</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/01/23/use-case-for-ai-coding-agent-slash-commands.html"><![CDATA[<p>Following up on my previous post: ‚Äú<a href="/2025/11/24/usent-case-for-ai-coding-agent-slash-commands.html">The Usen‚Äôt Case for AI Coding Agent Slash Commands</a>‚Äù, I‚Äôm delighted to report that I was wrong‚Ä¶ partially.</p>

<p>There <strong>is</strong> a use case for AI Coding Agent Slash Commands.</p>

<blockquote>
  <p>Slash Commands are for bundling a big prompt that is an entry point to a workflow.</p>
</blockquote>

<p>So if you have prompt-engineered ‚Äúlook things up and give me my morning report,‚Äù that‚Äôs a great use case for a Slash Command.</p>

<p>In Cursor, previously you wrote these as ‚ÄúManual Rules‚Äù and then <code class="language-plaintext highlighter-rouge">@mention</code>‚Äòd them. This was a little weird because every other kind of Cursor Rule was automatically picked up by the agent somehow. But Manual Rules sat there with the special <code class="language-plaintext highlighter-rouge">*.mdc</code> extension and Cursor Rule ‚ÄúYAML frontmatter,‚Äù but didn‚Äôt do anything with it.</p>

<p>Cursor‚Äôs addition of Slash Commands was correct and necessary.</p>

<p>There <em>is</em> a use-case in the world for repeatable prompts.</p>

<p>I stand by the ‚ÄúUsen‚Äôt Thesis,‚Äù though, for most instances of adjusting agentic software development behavior: You can do <em>better</em> by writing proper Rules, Skills, Hooks, or Tools.
The key insight is the ‚Äúentry point to a workflow‚Äù bit - especially the <em>entry point</em>. When human intent kicks off an agent in a new direction, a bundled prompt is handy to ensure that process gets started with the best chance of success.</p>

<p>If you‚Äôre already in the middle of a workflow, though, you don‚Äôt want to be maximizing human interaction by providing ‚Äúuseful‚Äù commands for the human to run.
You want to be equipping your agents with the tools and knowledge they need to just do it right from start to finish.</p>

<p>A command for a human to run in the middle of things is <em>still</em> an antipattern. So craft your commands carefully!</p>

<h2 id="the-wiggum-loop">The Wiggum Loop</h2>

<p>As with almost everything, <a href="https://xkcd.com/1205/">the more you use it, the more it pays to optimize it</a>. Somewhere where you‚Äôll not only be re-using the same prompt, but re-using it more than <em>you</em> the human ever could, is a <a href="https://ghuntley.com/ralph/">Wiggum Loop</a>. That‚Äôs actually a perfect use-case for a Slash Command!</p>

<p>I <a href="/2026/01/25/i-finally-coded-so-hard-i-ralphed.html">did that recently</a>!</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="essay" /><category term="cursor" /><category term="ai" /><category term="claude-code" /><summary type="html"><![CDATA[Following up on my previous post: ‚ÄúThe Usen‚Äôt Case for AI Coding Agent Slash Commands‚Äù, I‚Äôm delighted to report that I was wrong‚Ä¶ partially.]]></summary></entry><entry><title type="html">2MB Lighter</title><link href="https://blog.cani.ne.jp/2026/01/17/2mb-lighter.html" rel="alternate" type="text/html" title="2MB Lighter" /><published>2026-01-17T00:00:00+00:00</published><updated>2026-01-17T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/01/17/2mb-lighter</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/01/17/2mb-lighter.html"><![CDATA[<p>The <a href="https://mermaid.js.org/">mermaid.js chart library</a> weighs in at ~2MB minified. I wanted diagrams in my blog posts, but not at that cost. The solution became my third Jekyll gem: <a href="https://rubygems.org/gems/jekyll-mermaid-prebuild">jekyll-mermaid-prebuild</a>.</p>

<h2 id="the-setup">The Setup</h2>

<p>I had a <a href="/2026/01/17/all-it-took-was-broken-firmware.html">blog post with several Mermaid diagrams explaining a firmware debugging story</a>. The standard approach - include mermaid.js and let it render client-side - would add 2MB to every page load. For static diagrams that never change after publishing, that‚Äôs absurd.</p>

<p>The mermaid project provides <code class="language-plaintext highlighter-rouge">mmdc</code>, a <a href="https://github.com/mermaid-js/mermaid-cli">CLI that renders diagrams to SVG using headless Chrome</a>. A Jekyll plugin could intercept mermaid code blocks during build, shell out to <code class="language-plaintext highlighter-rouge">mmdc</code>, and replace them with static SVG references. No client-side JavaScript needed.</p>

<h2 id="prototype-in-_plugins">Prototype in <code class="language-plaintext highlighter-rouge">_plugins/</code></h2>

<p>Following the pattern from <a href="/2025/12/11/building-my-second-rubygem.html">jekyll-auto-thumbnails</a>, I started with a local plugin before extracting to a gem. Faster iteration, immediate feedback.</p>

<p>The first attempt operated on rendered HTML, scanning for <code class="language-plaintext highlighter-rouge">&lt;code class="language-mermaid"&gt;</code> blocks. This worked, but produced inline SVG data URIs - ugly in the output and not separately cacheable.</p>

<p>Better approach: operate on <strong>markdown</strong> during <code class="language-plaintext highlighter-rouge">:pre_render</code>. Find mermaid code blocks, convert to SVG files, replace with image references. The SVGs become proper static assets - cacheable by browsers, clickable for full-size viewing.</p>

<h2 id="puppeteer-in-wsl">Puppeteer in WSL</h2>

<p>First test run:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MermaidPrebuild: Initialized (mmdc 11.12.0)
MermaidPrebuild: mmdc failed: error while loading shared libraries: libgbm.so.1
</code></pre></div></div>

<p>The mermaid CLI uses Puppeteer (headless Chrome) internally. WSL - my local build environment - doesn‚Äôt ship with Chrome‚Äôs system library dependencies.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> libgbm1 libasound2 libatk1.0-0 <span class="se">\</span>
  libatk-bridge2.0-0 libcups2 libdrm2 libxcomposite1 <span class="se">\</span>
  libxdamage1 libxfixes3 libxrandr2 libxkbcommon0 <span class="se">\</span>
  libpango-1.0-0 libcairo2 libnss3 libnspr4
</code></pre></div></div>

<p>After installing the dependencies, <code class="language-plaintext highlighter-rouge">mmdc</code> worked. The plugin <a href="https://github.com/Texarkanine/jekyll-mermaid-prebuild/blob/v0.2.0/lib/jekyll-mermaid-prebuild/hooks.rb#L36-L51">detects this failure mode</a> and prints the apt-get command in the error message.</p>

<h2 id="the-hook-timing-bug">The Hook Timing Bug</h2>

<p>With Puppeteer working, the plugin‚Ä¶ did nothing. No diagrams converted. Debug output showed the <code class="language-plaintext highlighter-rouge">:pre_render</code> hook firing, but <code class="language-plaintext highlighter-rouge">site.data["mermaid_prebuild_enabled"]</code> was nil.</p>

<p>Jekyll‚Äôs <code class="language-plaintext highlighter-rouge">:after_init</code> hook seemed like the right place to check if <code class="language-plaintext highlighter-rouge">mmdc</code> exists and store the result. But <code class="language-plaintext highlighter-rouge">site.data</code> doesn‚Äôt persist between hooks the way I expected. By the time <code class="language-plaintext highlighter-rouge">:pre_render</code> runs on each document, the flag was gone.</p>

<p>The fix: use <code class="language-plaintext highlighter-rouge">:post_read</code> instead of <code class="language-plaintext highlighter-rouge">:after_init</code>. At that point, site configuration and data are stable.</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">Jekyll</span><span class="o">::</span><span class="no">Hooks</span><span class="p">.</span><span class="nf">register</span> <span class="ss">:site</span><span class="p">,</span> <span class="ss">:post_read</span> <span class="k">do</span> <span class="o">|</span><span class="n">site</span><span class="o">|</span>
  <span class="k">next</span> <span class="k">unless</span> <span class="no">Configuration</span><span class="p">.</span><span class="nf">enabled?</span><span class="p">(</span><span class="n">site</span><span class="p">)</span>
  
  <span class="n">site</span><span class="p">.</span><span class="nf">data</span><span class="p">[</span><span class="s2">"mermaid_prebuild"</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">"enabled"</span> <span class="o">=&gt;</span> <span class="kp">true</span><span class="p">,</span> <span class="s2">"registry"</span> <span class="o">=&gt;</span> <span class="p">{}</span> <span class="p">}</span>
  <span class="no">Jekyll</span><span class="p">.</span><span class="nf">logger</span><span class="p">.</span><span class="nf">info</span> <span class="s2">"MermaidPrebuild:"</span><span class="p">,</span> <span class="s2">"Initialized (mmdc </span><span class="si">#{</span><span class="no">MmdcWrapper</span><span class="p">.</span><span class="nf">version</span><span class="si">}</span><span class="s2">)"</span>
<span class="k">end</span>
</code></pre></div></div>

<h2 id="code-fence-patterns">Code Fence Patterns</h2>

<p>Markdown supports two fence styles. Backticks:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">mermaid
</span><span class="sb">graph LR
  A --&gt; B</span>
<span class="p">```</span>
</code></pre></div></div>

<p>Or tildes:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">~~~</span><span class="nl">mermaid
</span><span class="sb">graph LR
  A --&gt; B</span>
<span class="p">~~~</span>
</code></pre></div></div>

<p>Both can use 3+ fence characters. The plugin needed to handle either style and ensure the closing fence matches the opening fence in both character type and count.</p>

<p>The regex:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sr">%r{
  ^(`{3,}|~{3,})mermaid</span><span class="se">\s</span><span class="sr">*</span><span class="se">\n</span><span class="sr">  # Opening: 3+ backticks or tildes
  (.*?)                        # Content (non-greedy)
  ^</span><span class="se">\1\s</span><span class="sr">*$                      # Closing: must match opener
}mx</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">\1</code> backreference ensures a block opened with four backticks closes with four backticks, not three tildes.</p>

<p>To show these examples without converting them, the plugin respects fence nesting - mermaid blocks inside outer fences are preserved as code.</p>

<h2 id="gem-extraction">Gem Extraction</h2>

<p>Once the local plugin worked, extraction followed the same TDD pattern as the previous gems. Six modules:</p>

<ol>
  <li><strong>Configuration</strong> - Parse <code class="language-plaintext highlighter-rouge">_config.yml</code> settings</li>
  <li><strong>MmdcWrapper</strong> - Shell out to <code class="language-plaintext highlighter-rouge">mmdc</code>, handle errors</li>
  <li><strong>DigestCalculator</strong> - MD5 for cache keys</li>
  <li><strong>Processor</strong> - Find and replace mermaid blocks</li>
  <li><strong>Generator</strong> - Copy SVGs to <code class="language-plaintext highlighter-rouge">_site/</code></li>
  <li><strong>Hooks</strong> - Wire into Jekyll lifecycle</li>
</ol>

<p>42 tests covering the module interfaces. The tests mock <code class="language-plaintext highlighter-rouge">mmdc</code> execution since you can‚Äôt assume Puppeteer dependencies in CI.</p>

<h2 id="coderabbits-review">CodeRabbit‚Äôs Review</h2>

<p>Three valid concerns after the initial push:</p>

<p><strong>1. Cross-platform <code class="language-plaintext highlighter-rouge">which</code></strong></p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="no">Kernel</span><span class="p">.</span><span class="nf">system</span><span class="p">(</span><span class="s2">"which mmdc &gt; /dev/null 2&gt;&amp;1"</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">which</code> command doesn‚Äôt exist on Windows. <a href="https://github.com/Texarkanine/jekyll-mermaid-prebuild/blob/v0.2.0/lib/jekyll-mermaid-prebuild/mmdc_wrapper.rb#L20-L32">Fixed with pure Ruby PATH scanning</a>:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">command_exists?</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
  <span class="n">cmd_name</span> <span class="o">=</span> <span class="no">Gem</span><span class="p">.</span><span class="nf">win_platform?</span> <span class="p">?</span> <span class="s2">"</span><span class="si">#{</span><span class="n">cmd</span><span class="si">}</span><span class="s2">.exe"</span> <span class="p">:</span> <span class="n">cmd</span>
  <span class="n">path_dirs</span> <span class="o">=</span> <span class="no">ENV</span><span class="p">.</span><span class="nf">fetch</span><span class="p">(</span><span class="s2">"PATH"</span><span class="p">,</span> <span class="s2">""</span><span class="p">).</span><span class="nf">split</span><span class="p">(</span><span class="no">File</span><span class="o">::</span><span class="no">PATH_SEPARATOR</span><span class="p">)</span>
  
  <span class="n">path_dirs</span><span class="p">.</span><span class="nf">any?</span> <span class="k">do</span> <span class="o">|</span><span class="n">dir</span><span class="o">|</span>
    <span class="no">File</span><span class="p">.</span><span class="nf">executable?</span><span class="p">(</span><span class="no">File</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">dir</span><span class="p">,</span> <span class="n">cmd_name</span><span class="p">))</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div>

<p><strong>2. Windows Tempfile locking</strong></p>

<p>The output Tempfile wasn‚Äôt closed before <code class="language-plaintext highlighter-rouge">mmdc</code> tried to write to it. Windows locks open files, so <code class="language-plaintext highlighter-rouge">mmdc</code> would fail. Fixed by closing the tempfile before the subprocess runs.</p>

<p><strong>3. Missing SVG handling</strong></p>

<p>If a cached SVG somehow disappeared, <code class="language-plaintext highlighter-rouge">FileUtils.cp</code> would crash the build. <a href="https://github.com/Texarkanine/jekyll-mermaid-prebuild/blob/v0.2.0/lib/jekyll-mermaid-prebuild/hooks.rb#L23">Added existence check</a> with a warning instead of hard failure.</p>

<h2 id="examples-in-action">Examples in Action</h2>

<p>The diagrams in my firmware debugging post now render as static SVGs. A flowchart that would have required 2MB of JavaScript:</p>

<figure class="mermaid-diagram">
<a href="/assets/svg/e09440ca.svg"><img src="/assets/svg/e09440ca.svg" alt="Mermaid Diagram" /></a>
</figure>

<p>The generated SVG is ~12KB. Wrapped in a link to itself for full-size viewing on complex diagrams.</p>

<h2 id="final-stats">Final Stats</h2>

<ul>
  <li><strong>47 tests, all passing</strong></li>
  <li><strong>70%+ code coverage</strong></li>
  <li><strong>Published to RubyGems:</strong> <a href="https://rubygems.org/gems/jekyll-mermaid-prebuild">jekyll-mermaid-prebuild</a></li>
  <li><strong>Installation:</strong> <code class="language-plaintext highlighter-rouge">gem install jekyll-mermaid-prebuild</code></li>
</ul>

<p>Three gems now: <a href="https://rubygems.org/gems/jekyll-highlight-cards">jekyll-highlight-cards</a> for styled link and image cards, <a href="https://rubygems.org/gems/jekyll-auto-thumbnails">jekyll-auto-thumbnails</a> for automatic image optimization, and <a href="https://rubygems.org/gems/jekyll-mermaid-prebuild">jekyll-mermaid-prebuild</a> for build-time diagram rendering.</p>

<h2 id="what-i-learned">What I Learned</h2>

<p><strong>Jekyll‚Äôs <code class="language-plaintext highlighter-rouge">site.data</code> doesn‚Äôt persist the way you‚Äôd expect between hooks.</strong> Data set in <code class="language-plaintext highlighter-rouge">:after_init</code> may not be available in <code class="language-plaintext highlighter-rouge">:pre_render</code>. Use <code class="language-plaintext highlighter-rouge">:post_read</code> for site-wide state that needs to survive into document processing.</p>

<p><strong>Puppeteer dependencies vary by platform.</strong> The error message when Chrome can‚Äôt launch is cryptic (<code class="language-plaintext highlighter-rouge">cannot open shared object file</code>). Detecting this failure mode and printing the exact <code class="language-plaintext highlighter-rouge">apt-get</code> command saves users time.</p>

<p><strong>Regex backreferences match fence styles elegantly.</strong> <code class="language-plaintext highlighter-rouge">^\1\s*$</code> ensures opening and closing fences match both in character and count. No need to track state between matches.</p>

<p><strong>Windows file locking affects tempfiles.</strong> On Unix, an open file descriptor doesn‚Äôt prevent other processes from writing. On Windows, it does. Close tempfiles before subprocesses write to them.</p>

<p><strong>Pure Ruby PATH scanning beats shell commands for portability.</strong> <code class="language-plaintext highlighter-rouge">which</code> doesn‚Äôt exist on Windows, <code class="language-plaintext highlighter-rouge">where</code> doesn‚Äôt exist on Unix. Scanning <code class="language-plaintext highlighter-rouge">ENV["PATH"]</code> with <code class="language-plaintext highlighter-rouge">File.executable?</code> works everywhere.</p>

<h2 id="the-repository">The Repository</h2>

<p>The code is at <a href="https://github.com/Texarkanine/jekyll-mermaid-prebuild">Texarkanine/jekyll-mermaid-prebuild</a> with the full implementation history.</p>

<p>Three gems down, 2MB lighter per page.</p>]]></content><author><name>Niko</name></author><category term="blog" /><category term="diary" /><category term="jekyll" /><category term="mermaid" /><category term="ruby" /><category term="rubygem" /><summary type="html"><![CDATA[The mermaid.js chart library weighs in at ~2MB minified. I wanted diagrams in my blog posts, but not at that cost. The solution became my third Jekyll gem: jekyll-mermaid-prebuild.]]></summary></entry><entry><title type="html">All It Took Was Broken Firmware</title><link href="https://blog.cani.ne.jp/2026/01/17/all-it-took-was-broken-firmware.html" rel="alternate" type="text/html" title="All It Took Was Broken Firmware" /><published>2026-01-17T00:00:00+00:00</published><updated>2026-01-17T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/01/17/all-it-took-was-broken-firmware</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/01/17/all-it-took-was-broken-firmware.html"><![CDATA[<p>A <a href="https://www.surepetcare.com/en-us/internet-hub/hub">petcare IoT device</a> ‚Äúbroke.‚Äù It had worked for a year, connected through two switches between it and my main router. Then it stopped being able to connect to the manufacturer‚Äôs servers. The manufacturer‚Äôs support script was predictable: ‚ÄúConnect it directly to your router.‚Äù I did. It worked.</p>

<p>This should have been the end. But I wanted to understand <em>why</em>, and that investigation became the push I needed to finish a long-deferred project: proper network isolation for IoT devices.</p>

<h2 id="the-broken-hub">The Broken Hub</h2>

<p>The hub connects to various pet products - water fountain, pet door, etc. For a year it lived behind two unmanaged switches, happily communicating with the cloud. Then it didn‚Äôt.</p>

<p>I tested methodically:</p>

<ul>
  <li>Different switch ports: failed</li>
  <li>Different cables: failed</li>
  <li>Power cycling everything: failed</li>
  <li>Direct connection to main router: <strong>worked</strong></li>
  <li>Back through switches: failed again</li>
</ul>

<p>The failure was reproducible. Switches broke it. Direct router connection fixed it.</p>

<p>Even a single switch between the hub and router caused failure. The switches themselves worked fine for every other device. The hub‚Äôs firmware was apparently so broken it couldn‚Äôt survive the 2-30 second port initialization delay that occurs when connecting through a switch - the time required for MAC address learning and spanning tree protocol transitions. At least, that was our best guess.</p>

<p>I had a spare router (actually, a stack of them) so I popped one on the end of the hub‚Äôs switch, letting it act as a full NAT‚Äôing router, and moved the hub to it. It worked. The hub didn‚Äôt need to be on the <em>internet-facing</em> router - it needed to be directly on <em>any</em> router.</p>

<figure class="mermaid-diagram">
<a href="/assets/svg/bac3c572.svg"><img src="/assets/svg/bac3c572.svg" alt="Mermaid Diagram" /></a>
</figure>

<p>I want to also briefly shout out to the old Linksys <code class="language-plaintext highlighter-rouge">WRT54GL</code> I pulled for this - I don‚Äôt have a receipt for it, but the oldest reference I can find is from 2011 - at this point it‚Äôs at least 17 years old, has been on the shelf for at least 13 years, and it powered up and routed like a champ!</p>

<div class="polaroid-container">
  <div class="polaroid">
    <a href="https://mobilespecs.net/router/Linksys/Linksys_WRT54GL.html" target="_blank" rel="noopener">
      <img src="wrt54gs-and-wsb24.jpg" alt="Shout Out to this Old Champ" class="polaroid-image" />
    </a>
    <div class="polaroid-title">Shout Out to this Old Champ</div>
    <div class="polaroid-link">
      
      <a href="https://mobilespecs.net/router/Linksys/Linksys_WRT54GL.html" target="_blank" rel="noopener">mobilespecs.net/router/Linksys/Linksys_WRT54GL.html</a>
      
    </div>
    <small class="polaroid-archive">
      
      &nbsp;
      
    </small>
  </div>
</div>

<h2 id="the-real-project">The Real Project</h2>

<p>I couldn‚Äôt run a new cable off the main router. All four LAN ports were already distributing to different rooms, with long and/or through-the-walls cable runs to switches in each. To give up a port for the hub, I would have had to run a new through-the-attic line from one room to another to daisy-chain them. No way! But this defective hub had just demonstrated something useful: I already had a router in the right location. If I converted it from an access point into a proper router with its own subnet, the hub would work - and I‚Äôd finally have the IoT network isolation I‚Äôd been meaning to set up.</p>

<p>The goal:</p>

<ul>
  <li>IoT devices on their own subnet, isolated from the home LAN</li>
  <li>Home LAN can reach IoT devices (for management and configuration)</li>
  <li>IoT devices cannot initiate connections to home LAN</li>
  <li>IoT devices use the existing <a href="https://pi-hole.net/">PiHole for DNS</a></li>
  <li>Individual IoT clients visible in PiHole (not hidden behind NAT)</li>
  <li>Hostnames displayed in PiHole query logs</li>
</ul>

<p>The starting topology looked like this:</p>

<figure class="mermaid-diagram">
<a href="/assets/svg/15d11b1a.svg"><img src="/assets/svg/15d11b1a.svg" alt="Mermaid Diagram" /></a>
</figure>

<p>Because we‚Äôd now determined that I would need the IoT Router to actually <em>be</em> a NAT‚Äôing, DHCP‚Äôing router, I would need something like this:</p>

<figure class="mermaid-diagram">
<a href="/assets/svg/b9ec82d6.svg"><img src="/assets/svg/b9ec82d6.svg" alt="Mermaid Diagram" /></a>
</figure>

<p>The thing that had had me putting this project off thus far was those dotted lines‚Ä¶ We were going to have to use <code class="language-plaintext highlighter-rouge">iptables</code> and static routes.</p>

<h2 id="converting-the-router">Converting the Router</h2>

<p>The IoT router was a <a href="https://mobilespecs.net/router/Linksys/Linksys_EA6500.html">Linksys EA6500</a> running <a href="https://dd-wrt.com/">dd-wrt</a> in ‚ÄúGateway‚Äù mode - effectively a switch with a wireless access point. Converting it to a proper router required several changes. I‚Äôd done this before, but it had been a while and I didn‚Äôt remember all the things I had to undo at first, so I‚Äôm going to write them all down for next time:</p>

<h3 id="wan-configuration">WAN Configuration</h3>

<p>First, the WAN interface needed a static IP on the home LAN:</p>

<ul>
  <li><strong>WAN IP</strong>: <code class="language-plaintext highlighter-rouge">192.168.1.101/24</code></li>
  <li><strong>Gateway</strong>: <code class="language-plaintext highlighter-rouge">192.168.1.1</code> (main router)</li>
  <li><strong>DNS</strong>: <code class="language-plaintext highlighter-rouge">192.168.1.254</code> (PiHole)</li>
</ul>

<p>I initially tried DHCP for the WAN interface. The IoT router never obtained a lease from the main router‚Ä¶ possibly because of the VLAN issue documented below. But, as this was going to be a foundational part of the network with a static route pointing to it, a static IP was probably better, anyway.</p>

<h3 id="vlan-separation">VLAN Separation</h3>

<p>The router had been bridging all ports together since I had just been using its ethernet ports as a LAN switch. To function as a router, WAN and LAN needed separate VLANs.</p>

<p><strong>Setup ‚Üí Switch Configuration</strong></p>

<p><img src="dd-wrt_setup_switch-config.jpg" alt="DD-WRT: Setup ‚Üí Switch Configuration" /></p>

<p>After applying these changes, I couldn‚Äôt reach the router at any IP address. Recovery required connecting a laptop with a static IP (<code class="language-plaintext highlighter-rouge">10.1.101.50/24</code>) and manual specification of the gateway (<code class="language-plaintext highlighter-rouge">10.1.101.1</code>) directly to a LAN port, then accessing the dd-wrt web interface at <code class="language-plaintext highlighter-rouge">10.1.101.1</code>.</p>

<p><img src="xubuntu-manual-ipv4-config.jpg" alt="Xubuntu manual ipv4 configuration" /></p>

<p>This is probably because I had a bunch of pollution in routing tables and dhcp lease tables from previous incorrect configurations and connections. A reboot of all involved devices would probably also have fixed it.</p>

<h3 id="the-dhcp-derp">The DHCP Derp</h3>

<p>With VLANs fixed, devices connected to the IoT router were still getting IP addresses from the main router‚Äôs DHCP. Something was still bridged‚Ä¶?</p>

<p>I checked the status page. DHCP Server: ‚ÄúEnabled - Stopped‚Äù.</p>

<p>The syslog revealed the problem:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dnsmasq[1430]: inconsistent DHCP range at line 9 of /tmp/dnsmasq.conf
dnsmasq[1430]: FAILED to start up
</code></pre></div></div>

<p>The generated dnsmasq configuration showed:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dhcp-range=br0,10.1.101.100,10.1.102.33,255.255.255.0,1440m
</code></pre></div></div>

<p>With 190 maximum DHCP users starting at .100, the range overflowed the /24 subnet boundary. <code class="language-plaintext highlighter-rouge">10.1.101.100</code> + 190 = <code class="language-plaintext highlighter-rouge">10.1.102.34</code>. Dnsmasq correctly refused to serve an inconsistent range.</p>

<p>With no DHCP running on the IoT router, and no isolation enforced yet, devices were getting their DHCP queries answered by the main router. I‚Äôd actually already set up the static route (documented below) to allow traffic from the main subnet into the IoT subnet, so this was working even though it normally wouldn‚Äôt have. Should have gone strictly in order!</p>

<p>I reduced the DHCP pool to 50 users. The range stayed within the subnet, dnsmasq started, and devices finally got addresses from the IoT router.</p>

<p><strong>I</strong> didn‚Äôt set that to 190. As far as I know, dd-wrt defaults to 50. I don‚Äôt know how, but somehow it got set to 190 while I was fiddling with and restarting the router a million times.</p>

<h3 id="network-setup">Network Setup</h3>

<p>The LAN configuration:</p>

<ul>
  <li><strong>Router IP</strong>: <code class="language-plaintext highlighter-rouge">10.1.101.1/24</code></li>
  <li><strong>DHCP Server</strong>: Enabled</li>
  <li><strong>Start IP</strong>: <code class="language-plaintext highlighter-rouge">10.1.101.100</code></li>
  <li><strong>Maximum DHCP Users</strong>: 50</li>
  <li><strong>DNS</strong>: <code class="language-plaintext highlighter-rouge">192.168.1.254</code> (PiHole)</li>
</ul>

<h2 id="routing-and-firewall">Routing and Firewall</h2>

<h3 id="static-route-on-main-router">Static Route on Main Router</h3>

<p>The main router (running <a href="https://www.asuswrt-merlin.net/">Asuswrt-Merlin</a>) needed to know how to reach the IoT subnet:</p>

<ul>
  <li><strong>Destination</strong>: <code class="language-plaintext highlighter-rouge">10.1.101.0/24</code></li>
  <li><strong>Gateway</strong>: <code class="language-plaintext highlighter-rouge">192.168.1.101</code></li>
  <li><strong>Interface</strong>: LAN</li>
</ul>

<p><strong>This is one half of the magic!</strong> This route allows the main router, who‚Äôs living in a <code class="language-plaintext highlighter-rouge">192.168.1.0/24</code> subnet, to see traffic destined for the <code class="language-plaintext highlighter-rouge">10.1.101.0/24</code> subnet and go ‚ÄúOh, I know who to send that to!‚Äù</p>

<h3 id="firewall-rules">Firewall Rules</h3>

<p>The IoT router needed firewall rules to enforce isolation. In dd-wrt, these go in Administration ‚Üí Commands, saved as a firewall script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Allow established/related connections back to IoT devices</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-m</span> state <span class="nt">--state</span> ESTABLISHED,RELATED <span class="nt">-j</span> ACCEPT

<span class="c"># Allow home LAN to reach IoT LAN (for management)</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 192.168.1.0/24 <span class="nt">-d</span> 10.1.101.0/24 <span class="nt">-j</span> ACCEPT

<span class="c"># Block IoT from initiating connections to home LAN</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-d</span> 192.168.1.0/24 <span class="nt">-j</span> REJECT

<span class="c"># Allow IoT to reach internet</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-o</span> <span class="si">$(</span>nvram get wan_iface<span class="si">)</span> <span class="nt">-j</span> ACCEPT
</code></pre></div></div>

<p>This is the other half of the magic! It essentially inverts the normal NAT semantic:</p>

<ul>
  <li>home LAN devices <strong>can</strong> SSH or otherwise connect to IoT devices, access their web interfaces, push firmware updates, etc. They don‚Äôt see the NAT at all.</li>
  <li>IoT devices cannot reach anything on the home LAN. They don‚Äôt see the home LAN at all.</li>
</ul>

<p>The dd-wrt web interface didn‚Äôt make setting this up obvious:</p>

<p><img src="dd-wrt_administration_commands.jpg" alt="DD-WRT: Administration ‚Üí Commands" /></p>

<ol>
  <li>At first, you have no saved commands, so all you can use is the ‚ÄúCommands‚Äù text box.</li>
  <li>Enter the firewall rules.</li>
  <li>Click ‚ÄúRun Commands‚Äù to run them and see if they work.</li>
  <li>Click ‚ÄúSave Firewall‚Äù to save them.</li>
  <li><em>Next time</em>, you will have contents in the ‚ÄúFirewall‚Äù text field.</li>
  <li>Click ‚ÄúEdit‚Äù under the ‚ÄúFirewall‚Äù text field to populate the ‚ÄúCommands‚Äù text box with the firewall script.</li>
  <li>GOTO 2.</li>
</ol>

<h2 id="pihole-integration">PiHole Integration</h2>

<h3 id="the-nat-problem">The NAT Problem</h3>

<p>With basic routing working, I checked the PiHole query logs. All IoT DNS queries appeared to come from <code class="language-plaintext highlighter-rouge">192.168.1.101</code> - the IoT router‚Äôs WAN IP. NAT was hiding the individual clients.</p>

<p>This defeated a key goal. I wanted per-device DNS visibility so I could block specific IoT devices from specific domains. With everything hidden behind one IP, I could only manage IoT as a single entity.</p>

<h3 id="nat-exemption-for-dns">NAT Exemption for DNS</h3>

<p>The solution: exempt DNS traffic from NAT, allowing the original source IP through to PiHole.</p>

<p>Additions to the firewall script:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Exempt DNS to PiHole from NAT (preserve source IP)</span>
iptables <span class="nt">-t</span> nat <span class="nt">-I</span> POSTROUTING <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
iptables <span class="nt">-t</span> nat <span class="nt">-I</span> POSTROUTING <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT

<span class="c"># Allow IoT to reach PiHole DNS</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">-t nat -I POSTROUTING ... -j ACCEPT</code> rules tell iptables: ‚ÄúFor packets going to PiHole port 53, skip the <a href="https://www.netfilter.org/documentation/HOWTO/NAT-HOWTO-6.html#ss6.1">MASQUERADE rule that rewrites the source IP</a>.‚Äù The packets arrive at PiHole with their original 10.1.101.x source addresses. Normally, these packets wouldn‚Äôt be able to be responded to: the pihole is on a different subnet with a different gateway that wouldn‚Äôt know about the <code class="language-plaintext highlighter-rouge">10.1.101.0/24</code> devices! That‚Äôs what the static route on the main router is for: the pihole‚Äôs gateway actually <em>does</em> know where to send responses addressed to the <code class="language-plaintext highlighter-rouge">10.*</code> addresses.</p>

<p>After applying these rules, individual IoT IPs appeared in the PiHole logs.</p>

<h2 id="hostname-resolution">Hostname Resolution</h2>

<p>IP addresses appeared correctly, but the PiHole query log showed numeric IPs instead of hostnames. For the home LAN, PiHole displayed ‚Äúmacbook‚Äù and ‚Äúdesktop‚Äù. For IoT devices, just <code class="language-plaintext highlighter-rouge">10.1.101.134</code>.</p>

<h3 id="configuring-the-iot-routers-dns">Configuring the IoT Router‚Äôs DNS</h3>

<p>The IoT router needed to serve reverse DNS lookup queries about its own DHCP clients. This required several dnsmasq settings.</p>

<p>In Setup ‚Üí Basic Setup:</p>

<ul>
  <li><strong>Local Domain</strong>: <code class="language-plaintext highlighter-rouge">iot.local</code></li>
  <li><strong>Use DNSMasq for DNS</strong>: Enabled</li>
</ul>

<p>In Services ‚Üí Services ‚Üí Dnsmasq Infrastructure ‚Üí Additional Options:</p>

<p><img src="dd-wrt_services_services_dnsmasq-infrastructure_sm.jpg" alt="DD-WRT: Services ‚Üí Services ‚Üí Additional DNSMasq Options" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>expand-hosts
bind-interfaces
listen-address=192.168.1.101
local=/101.1.10.in-addr.arpa/
dhcp-option=6,192.168.1.254
</code></pre></div></div>

<p>Each line serves a specific purpose:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">expand-hosts</code>: Append the domain to DHCP hostnames, so we have names in the first place</li>
  <li><code class="language-plaintext highlighter-rouge">bind-interfaces</code>: Required when specifying listen-address</li>
  <li><code class="language-plaintext highlighter-rouge">listen-address=192.168.1.101</code>: Serve DNS on WAN interface only (for PiHole reverse DNS queries)</li>
  <li><code class="language-plaintext highlighter-rouge">local=/101.1.10.in-addr.arpa/</code>: Answer reverse DNS locally instead of forwarding upstream</li>
  <li><code class="language-plaintext highlighter-rouge">dhcp-option=6,192.168.1.254</code>: Tell DHCP clients to use PiHole as their DNS server (option 6 = DNS server)</li>
</ul>

<p>The <code class="language-plaintext highlighter-rouge">local=</code> line was critical. Without it, the IoT router forwarded reverse DNS queries to internet root servers, which returned NXDOMAIN for private IP addresses. With <code class="language-plaintext highlighter-rouge">local=</code>, it answers authoritatively from its own DHCP lease table.</p>

<p>I tried using <code class="language-plaintext highlighter-rouge">auth-zone</code> for authoritative DNS, which would also have prevented the forwarding of reverse DNS queries, but DD-WRT‚Äôs dnsmasq wasn‚Äôt compiled with <code class="language-plaintext highlighter-rouge">HAVE_AUTH</code> support. The router refused to start dnsmasq at all with that directive. <code class="language-plaintext highlighter-rouge">local=</code> was the way to go.</p>

<p>Note also that we‚Äôre <em>not</em> listening on the LAN interface (<code class="language-plaintext highlighter-rouge">10.1.101.1</code>), as a DHCP‚Äôing router normally would. IoT devices should use PiHole for DNS, not this router. To enforce this policy, and to allow PiHole to query us for reverse DNS, we need a few more firewall rules:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Allow reverse DNS queries FROM PiHole (via WAN interface)</span>
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> vlan2 <span class="nt">-s</span> 192.168.1.254 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> vlan2 <span class="nt">-s</span> 192.168.1.254 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT

<span class="c"># Block DNS queries TO the gateway from IoT clients (they should use PiHole)</span>
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> br0 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> REJECT
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> br0 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> REJECT
</code></pre></div></div>

<h3 id="configuring-piholes-conditional-forwarding">Configuring PiHole‚Äôs Conditional Forwarding</h3>

<p>PiHole needs to know where to send reverse DNS queries for the IoT subnet. The web UI supports <a href="https://docs.pi-hole.net/ftldns/configfile/?h=forwarding#revservers">conditional forwarding</a>, but only for a single network - mine was already configured for the home LAN.</p>

<p>The solution: manually add a dnsmasq configuration file on the pihole server:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># /etc/dnsmasq.d/11-iot-subnet.conf</span>
rev-server<span class="o">=</span>10.1.101.0/24,192.168.1.101
</code></pre></div></div>

<p>This tells PiHole: ‚ÄúFor reverse DNS lookups of <code class="language-plaintext highlighter-rouge">10.1.101.x</code> addresses, query <code class="language-plaintext highlighter-rouge">192.168.1.101</code> (the IoT router‚Äôs WAN IP).‚Äù</p>

<p>After restarting PiHole‚Äôs DNS:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>pihole restartdns
</code></pre></div></div>

<p>Reverse DNS queries started working. <code class="language-plaintext highlighter-rouge">dig -x 10.1.101.134</code> on the PiHole server returned <code class="language-plaintext highlighter-rouge">macbook.iot.local</code>.</p>

<h3 id="the-cache-finale">The Cache Finale</h3>

<p>Everything was configured correctly. Reverse DNS worked from the command line. The PiHole <a href="https://docs.pi-hole.net/database/query-database/">FTL database</a> showed hostnames:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sqlite&gt; SELECT ip, name FROM network_addresses WHERE ip LIKE '10.1.101.%';
10.1.101.134|macbook.iot.local
10.1.101.101|gateway-device-1.iot.local
10.1.101.109|gateway-device-2.iot.local
</code></pre></div></div>

<p>But the PiHole web interface still showed IP addresses.</p>

<p>Browser cache. A hard refresh (Ctrl+Shift+R) and hostnames appeared. It may have also been a 10-minute or so wait. I didn‚Äôt test rigorously. But, it eventually started working; there was <em>some</em> kind of latency involved between proper configuration and hostnames actually showing in the Web UI.</p>

<h2 id="final-working-configuration">Final Working Configuration</h2>

<h3 id="main-router-asuswrt-merlin">Main Router (Asuswrt-Merlin)</h3>

<h4 id="lan--route">LAN ‚Üí Route</h4>

<p><strong>Static Route</strong></p>

<table>
  <thead>
    <tr>
      <th>Network/Host IP</th>
      <th>Netmask</th>
      <th>Gateway</th>
      <th>Metric</th>
      <th>Interface</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10.1.101.0/24</td>
      <td>255.255.255.0</td>
      <td>192.168.1.101</td>
      <td>¬†</td>
      <td>LAN</td>
    </tr>
  </tbody>
</table>

<h3 id="iot-router-dd-wrt">IoT Router (DD-WRT)</h3>

<h4 id="setup--basic-setup">Setup ‚Üí Basic Setup</h4>

<p><strong>WAN Connection Type</strong></p>
<ul>
  <li>Connection Type: Static IP</li>
  <li>WAN IP Address: <code class="language-plaintext highlighter-rouge">192.168.1.101/24</code></li>
  <li>Gateway: <code class="language-plaintext highlighter-rouge">192.168.1.1</code></li>
  <li>Static DNS 1: <code class="language-plaintext highlighter-rouge">192.168.1.254</code></li>
</ul>

<p><strong>Optional Settings</strong></p>
<ul>
  <li>Domain Name: <code class="language-plaintext highlighter-rouge">iot.local</code></li>
</ul>

<p><strong>Router IP</strong></p>
<ul>
  <li>Local IP Address: <code class="language-plaintext highlighter-rouge">10.1.101.1/24</code></li>
  <li>Gateway: <code class="language-plaintext highlighter-rouge">0.0.0.0</code></li>
  <li>Local DNS: <code class="language-plaintext highlighter-rouge">192.168.1.254</code></li>
</ul>

<p><strong>DHCP</strong></p>
<ul>
  <li>DHCP Type: DHCP Server</li>
  <li>DHCP Server: ‚úÖ Enable</li>
  <li>Start IP Address: <code class="language-plaintext highlighter-rouge">10.1.101.100</code></li>
  <li>Maximum DHCP Users: 50</li>
  <li>Use DNSMasq for DNS: ‚úÖ (enabled)</li>
</ul>

<h4 id="services--services">Services ‚Üí Services</h4>

<p><strong>Dnsmasq Infrastructure</strong></p>

<p><em>Additional Options:</em></p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>expand-hosts
bind-interfaces
listen-address=192.168.1.101
local=/101.1.10.in-addr.arpa/
dhcp-option=6,192.168.1.254
</code></pre></div></div>

<h4 id="administration--commands">Administration ‚Üí Commands</h4>

<p><strong>Firewall</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># ===== DNS Protection =====</span>
<span class="c"># Allow reverse DNS queries FROM PiHole (via WAN interface)</span>
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> vlan2 <span class="nt">-s</span> 192.168.1.254 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> vlan2 <span class="nt">-s</span> 192.168.1.254 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT

<span class="c"># Block DNS queries TO the gateway from IoT clients (they should use PiHole)</span>
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> br0 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> REJECT
iptables <span class="nt">-I</span> INPUT <span class="nt">-i</span> br0 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> REJECT
<span class="c"># ===== End DNS Protection =====</span>

<span class="c"># Allow established/related to IoT LAN</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-m</span> state <span class="nt">--state</span> ESTABLISHED,RELATED <span class="nt">-j</span> ACCEPT

<span class="c"># Allow home LAN to reach IoT LAN</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 192.168.1.0/24 <span class="nt">-d</span> 10.1.101.0/24 <span class="nt">-j</span> ACCEPT

<span class="c"># Block IoT from initiating to home LAN</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-d</span> 192.168.1.0/24 <span class="nt">-j</span> REJECT

<span class="c"># ===== PiHole DNS Rules =====</span>
<span class="c"># Exempt DNS to PiHole from NAT</span>
iptables <span class="nt">-t</span> nat <span class="nt">-I</span> POSTROUTING <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
iptables <span class="nt">-t</span> nat <span class="nt">-I</span> POSTROUTING <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT

<span class="c"># Allow IoT to reach PiHole DNS</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> udp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-d</span> 192.168.1.254 <span class="nt">-p</span> tcp <span class="nt">--dport</span> 53 <span class="nt">-j</span> ACCEPT
<span class="c"># ===== End PiHole DNS Rules =====</span>

<span class="c"># Allow IoT to the internet</span>
iptables <span class="nt">-I</span> FORWARD <span class="nt">-s</span> 10.1.101.0/24 <span class="nt">-o</span> <span class="si">$(</span>nvram get wan_iface<span class="si">)</span> <span class="nt">-j</span> ACCEPT
</code></pre></div></div>

<h3 id="pihole">PiHole</h3>

<h4 id="etcdnsmasqd11-iot-subnetconf"><code class="language-plaintext highlighter-rouge">/etc/dnsmasq.d/11-iot-subnet.conf</code></h4>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rev-server=10.1.101.0/24,192.168.1.101
</code></pre></div></div>

<h2 id="a-note-on-subnet-selection">A Note on Subnet Selection</h2>

<p>This post uses <code class="language-plaintext highlighter-rouge">10.1.101.0/24</code> throughout, but I actually started with <code class="language-plaintext highlighter-rouge">192.168.101.0/24</code>. Mid-configuration, I switched.</p>

<p>The problem was readability. The IoT router‚Äôs WAN IP was <code class="language-plaintext highlighter-rouge">192.168.1.101</code>. The IoT router‚Äôs LAN IP was <code class="language-plaintext highlighter-rouge">192.168.101.1</code>. When debugging firewall rules and routing tables, my brain kept swapping them. Is this the router‚Äôs address on the home network, or its address on its own network? Which side of the NAT am I looking at?</p>

<p>Switching to <code class="language-plaintext highlighter-rouge">10.1.101.0/24</code> made everything clearer. The <code class="language-plaintext highlighter-rouge">192.168.x.x</code> addresses are home LAN. The <code class="language-plaintext highlighter-rouge">10.x.x.x</code> addresses are IoT. No ambiguity. If you‚Äôre setting up something similar, pick subnets that are visually distinct.</p>

<p>The pivot itself was straightforward:</p>

<ol>
  <li>Change IoT router‚Äôs LAN IP from <code class="language-plaintext highlighter-rouge">192.168.101.1</code> to <code class="language-plaintext highlighter-rouge">10.1.101.1</code></li>
  <li>Update DHCP range to <code class="language-plaintext highlighter-rouge">10.1.101.100-149</code></li>
  <li>Update main router‚Äôs static route destination to <code class="language-plaintext highlighter-rouge">10.1.101.0/24</code></li>
  <li>Update all iptables rules referencing the IoT subnet</li>
  <li>Update PiHole‚Äôs conditional forwarding config</li>
  <li>Reboot IoT router, renew DHCP leases on clients, and reboot the pihole server</li>
</ol>

<h2 id="fin">Fin</h2>

<p>The petcare hub works. It‚Äôs directly connected to a router, which is all its defective firmware ever needed. And the IoT network is finally isolated, with full per-device visibility in PiHole.</p>

<p>All because a petcare company couldn‚Äôt write firmware that can handle being connected to a network switch.</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="record" /><category term="dd-wrt" /><category term="dns" /><category term="home-networking" /><category term="iot" /><category term="iptables" /><category term="networking" /><category term="pihole" /><summary type="html"><![CDATA[A petcare IoT device ‚Äúbroke.‚Äù It had worked for a year, connected through two switches between it and my main router. Then it stopped being able to connect to the manufacturer‚Äôs servers. The manufacturer‚Äôs support script was predictable: ‚ÄúConnect it directly to your router.‚Äù I did. It worked.]]></summary></entry><entry><title type="html">Desire Makes Artists, Even With GenAI</title><link href="https://blog.cani.ne.jp/2026/01/01/desire-makes-artists-even-with-genai.html" rel="alternate" type="text/html" title="Desire Makes Artists, Even With GenAI" /><published>2026-01-01T00:00:00+00:00</published><updated>2026-01-01T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2026/01/01/desire-makes-artists-even-with-genai</id><content type="html" xml:base="https://blog.cani.ne.jp/2026/01/01/desire-makes-artists-even-with-genai.html"><![CDATA[<h2 id="thesis">Thesis</h2>

<blockquote>
  <p>The desire to create is what makes an artist. That‚Äôs true for some people using generative AI - and it‚Äôs also why there will always be human artists.</p>
</blockquote>

<h2 id="what-making-art-is">What Making Art Is</h2>

<p>Making art is the human creative undertaking. The desire to create something, to express, to bring ideas into the world - that generative impulse is what makes an artist make art.</p>

<p>Many things are produced by people with artistic skill, often bringing that skill to bear on products with dual purposes: expression and function. Clothing is an excellent example. There is absolutely clothing that serves very little purpose other than to be aesthetic, and there is definitely clothing that serves very little purpose other than to be functional. Most clothing exists in between.</p>

<p>Before the industrial revolution, clothing was made by hand by what we might call artisans - makers who often had artistic talent. You would get production of goods with art infused during the process, because that‚Äôs what humans do when they make things. As cloth-making industrialized, we were able to separate the artist from the artisan. Machines operationalized the good production, but artists still produced artistic clothing. Now Chinese factories crank out dirt-cheap t-shirts that technically work, while artistic seamstresses and clothiers hand-make bespoke luxury clothing for the fashion runway or demure custom shops.</p>

<p>We have separated the art from the good. Both exist.</p>

<h2 id="function-is-unforgiving">Function Is Unforgiving</h2>

<p>The key takeaway of this separation: function is unforgiving. If the thing does not work, it cannot exist in most contexts. Aesthetics without function is a luxury, and luxury is almost definitionally a minority undertaking.</p>

<p>Most goods only need, and are only wanted, to satisfy a task. It is much rarer that a good is primarily for satisfying aesthetics. Once you can separate the artist from the artisan and operationalize the production that the artisan used to handle, you end up with the majority of production being more or less purely functional - form as an afterthought.</p>

<p>This absolutely happened with clothing. It absolutely happened with dwellings. It has absolutely happened with most housewares and furniture. It is, in fact, what happens when humans manage to build tools and processes that operationalize and industrialize the production of a good that previously had to be handmade.</p>

<h2 id="portraits">Portraits</h2>

<p>Consider portraits. Back in the oil painting days, before photography, portraits were a big deal. They were rare. They were labor-intensive. If you wanted a portrait of yourself, you had to shell out, and it was quite an ordeal to get one made. Once done, it was quite a piece - a centerpiece, a focus piece - because only the artistic artisan could make one for you.</p>

<p>Nowadays, we have cameras in electronic devices in basically every pocket. Everybody who wants a portrait of themselves can get one. They can get a piece of visual media that they can look at and see themselves.</p>

<p>The oil painting - professionally staged, framed, an heirloom piece hanging on the wall of your house - is still a luxury art piece. But most people who wanted portraits didn‚Äôt actually need the whole shebang. They just wanted to see themselves in a piece of visual media. And they can get that now with the tools and processes we have built into smartphone cameras. It‚Äôs trivially easy to get a technical portrait of yourself where you can see yourself.</p>

<p>So most people who just wanted the outcome - a piece of visual media in which they could see themselves - get that easily and don‚Äôt advance to finding an artist to pay for a professional heirloom portrait.</p>

<h2 id="visual-media-today">Visual Media Today</h2>

<p>We are seeing this same separation today with visual media and generative AI.</p>

<p>Previously, in order to make a piece of visual media that other people could see - regardless of the purpose - you had to engage some form of artist-artisan combination. Maybe a graphic designer to produce the advertising copy for your company - function-forward visual media, but you were still absolutely using a human‚Äôs artistic talent to do it. Or maybe you wanted a book illustration, a comic, a particular scene. Either way, you needed the human artist-artisan combo.</p>

<p>Now that we have generative AI, many of those people can - just like they were doing with a selfie on their smartphone - get a good-enough visual rendering of their idea to satisfy what they needed. And they stop there.</p>

<p>Those people didn‚Äôt actually need art. They didn‚Äôt actually <em>want</em> art. They just needed a piece of visual media. And what we have now is the ability to separate the artisan from the artist once more, as humans have been doing since the beginning of technology, but in the domain of visual media.</p>

<h2 id="is-it-art-are-they-artists">Is It Art? Are They Artists?</h2>

<p>We will punt on ‚Äúis it art?‚Äù because that is an age-old question that has always been hotly debated, whose answer has shifted time and time again and is ultimately subjective.</p>

<p>Instead, let‚Äôs look at the people making these things and ask if they are artists.</p>

<p>As you might infer from the separation of artist and artisan, indeed, some of them are not. The people who would have needed a professional graphic designer - the artist-artisan combination - but now can use generative AI to get the visual media that communicates what they need? They‚Äôre using the operationalized, industrialized visual media production pipeline.</p>

<p>That‚Äôs fine. We used to have weavers weaving things on looms. We still have looms, but the overwhelming majority of them are industrial-scale, and the people that work in those factories we don‚Äôt really refer to as weavers anymore. They‚Äôre just employees. Workers.</p>

<p>So the people using generative AI just to make the visual media that serves a function - where form is not the driving purpose - are not artists. Because they‚Äôre not engaging in the creative undertaking of desiring to manifest a vision in the world. They don‚Äôt seek to share the aesthetic outcome. They don‚Äôt seek gratification from the experience of creating. They just seek the finished product so that it may perform its function.</p>

<p>And that‚Äôs fine. That‚Äôs allowed. People who buy t-shirts from factories that make t-shirts are not engaging with artists. You can also get a t-shirt that was handmade by an artist somewhere. And that‚Äôs okay too.</p>

<h2 id="the-artists-among-them">The Artists Among Them</h2>

<p>But some of those generative AI users don‚Äôt see a tool that gets them to an outcome faster. They see a tool that allows them to manifest aesthetics that were trapped in their head before.</p>

<p>These people are no different than a photographer picking up a camera or - pay attention to how we call these now - an <strong>artist</strong> picking up a mouse or a Wacom tablet instead of a pen or a pencil or a paintbrush, because the digital toolset is more amenable to manifesting their vision.</p>

<p>Those people who see generative AI as ‚Äúoh my goodness, I can finally create these visual experiences that I did not have a way to create before‚Äù - <strong>those people are artists.</strong></p>

<p>They are artists because they seek, through the act of creation, to experience and share the experience. That is what drives the artistic process. Even after visual media creation is fully industrialized - even after most people are just having the machines make the visual media that they need for their outcomes - we need not fear the loss of artists.</p>

<p>There will still be humans doing it by hand, because they seek the experience of <strong>doing</strong> the creating as an end unto itself.</p>]]></content><author><name>Niko</name></author><category term="blog" /><category term="essay" /><category term="ai" /><category term="art" /><category term="generative-ai" /><summary type="html"><![CDATA[The desire to create is what makes an artist. That's why some people using generative AI are artists - and it's also why there will always be human artists.]]></summary></entry><entry><title type="html">Tools Suck When They‚Äôre Products</title><link href="https://blog.cani.ne.jp/2025/12/22/tools-suck-when-theyre-products.html" rel="alternate" type="text/html" title="Tools Suck When They‚Äôre Products" /><published>2025-12-22T00:00:00+00:00</published><updated>2025-12-22T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/12/22/tools-suck-when-theyre-products</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/12/22/tools-suck-when-theyre-products.html"><![CDATA[<h2 id="thesis">Thesis</h2>

<blockquote>
  <p>Tools are not products and building them <em>like</em> products results in bad tools.</p>
</blockquote>

<p><strong>Corollary</strong></p>

<blockquote>
  <p>Nobody actually wants to use tools. They want the <em>result</em> of using the tool.</p>
</blockquote>

<h2 id="different-lifecycles">Different Lifecycles</h2>

<p>Tools become <em>dependencies</em> in people‚Äôs lives. If something goes wrong with a tool, the stuff downstream gets interrupted.</p>

<p>Products - as used here - are leaf nodes, the ‚Äúend of the line‚Äù - they‚Äôre where people want to be. If something goes wrong with a product, it can often be quite tolerable. Even if it‚Äôs not, they can just use a different product and nothing <em>downstream</em> of that has to change. The impact starts and ends with that single experience.</p>

<p>If I want to watch a fun movie and I sit down at my TV and the movie I picked turns out to be really bad‚Ä¶ well, that‚Äôs a <em>bad product</em>. I can always turn it off, or watch a fun show instead, or play a video game.</p>

<p>If, however, I sit down and my <strong>TV</strong> doesn‚Äôt work, now I‚Äôm stuck. The product I wanted - the movie - isn‚Äôt even an option, <em>and neither are alternatives</em>. I can‚Äôt watch a different movie, nor watch a show, nor play a video game. My goal was <strong>dependent</strong> on the tool and when the tool fails it is much more disruptive than a simple ‚Äúbad product‚Äù like a bad movie.</p>

<h2 id="physical-products">‚ÄúPhysical‚Äù Products</h2>

<p>Arguably, everything except leisure pursuits are tool uses. This is an important insight! Tool use is what humans do; we build tools to abstract processes so that we can get outcomes quicker, more reliably, and more efficiently.</p>

<p>Refrigerators and microwaves make it really easy to get a warm meal. They‚Äôre tools. I <em>never</em> go to use my refrigerator solely for the joy of witnessing refrigeration. I go because I want something that it‚Äôs done for me - kept food fresh and cold. Maybe I pop it in the microwave to heat it up - not because I like watching electromagnetic radiation of that particular wavelength, but because I want a warm meal.</p>

<p>This distinction is relatively easy to draw in the physical world. It can be less-obvious in the digital world.</p>

<p>Your computer‚Ä¶ is it a tool? Almost certainly, though the outcomes it offers are usually <em>themselves</em> tools too: you can run other pieces of software on it. Some will be games, which, if played for entertainment, would qualify as products. Inside the tool of a video player you might find the <em>product</em> of a movie. A web browser, though? That‚Äôs <em>another</em> layer of tooling, and if you load up an e-mail service, that‚Äôs <em>another</em> layer‚Ä¶ Tools all the way down, until you <em>finally</em> get to the raison d‚Äô√™tre of the tool: somewhere, a deep, buried <em>product</em> you want to <em>experience</em>.</p>

<h2 id="the-siren-call-of-product-management">The Siren Call of Product Management</h2>

<p>The techniques, processes, and dare I say, <em>tools</em> that are useful in building and delivering good products <em>also</em> work very well for getting digital artifacts out the door. Sometimes the artifact is indeed a product, like a movie, a video game, a book, song, etc.</p>

<p>But sometimes it‚Äôs <em>not</em> - even though on the surface the following look and <em>can</em> be produced similarly:</p>

<ul>
  <li>An executable file that, when launched, presents a video game</li>
  <li>An executable file that, when launched, presents a code editor</li>
</ul>

<p>The first is a product, and the second is a tool. After each one is initially released, the <em>ongoing</em> treatment that results in <em>continued success</em> is different.</p>

<h3 id="products-forgive-change">Products Forgive Change</h3>

<p>People like it when there are new products that are better. People especially like it if a product they already own gets better. In the world of leaf-node products, ‚Äúbetter‚Äù is usually fairly clear-cut. The product has a purpose, and turning the knob up on how well it delivers, is better.</p>

<p>You liked a funny movie? We released a new one that‚Äôs more of that, and also funnier. You like that song? We have a whole album of songs like it. You liked that video game? We fixed a bunch of bugs and improved the framerate. Clearly better.</p>

<h3 id="tools-dont-forgive-change">Tools Don‚Äôt Forgive Change</h3>

<p>Tools sit in the middle of people‚Äôs workflows. While many people may have many aspects of their workflows in common, there are many opportunities for personal preference and individual circumstances to enter the equation. So, a tool that‚Äôs ‚Äúfor X‚Äù may be used in significantly different ways by different people.</p>

<p>Web Browsers are a great example: They‚Äôre a tool ‚Äúfor browsing the internet.‚Äù But one person might be big into browser-based games, and want a Flash plugin, WebGL support, Websockets, and GPU acceleration. A researcher at a university, though, would probably be much more concerned about compatibility with e-mail &amp; legacy academic systems, bookmarks (preserving and/or sharing across devices) and perhaps VPN support.</p>

<p>Specific examples notwithstanding, web browsers‚Äô single purpose (‚Äúbrowse the web‚Äù) belies a myriad of features and capabilities that result in no single pattern of use. It can be difficult to make a change to a browser that‚Äôs better <em>for everyone</em>. The more things a tool does, the greater the chance that any change at all will be a detriment to <em>someone</em>.</p>

<blockquote class="link-card">
  
  <h1>Hyrum&#39;s Law, or The Law of Implicit Interfaces</h1>
  
  <a href="https://kb.feval.ca/engineering/design/law-of-implicit-interface.html" target="_blank" rel="noopener">kb.feval.ca/engineering/design/law-of-implicit-interface.html</a>
  
  <small class="link-card-archive">
    (<a href="https://web.archive.org/web/20251214040814/https://kb.feval.ca/engineering/design/law-of-implicit-interface.html" target="_blank" rel="noopener">archive</a>)
  </small>
  
</blockquote>

<h2 id="what-to-do">What to Do?</h2>

<p>So, when it comes time to spruce up the web browser (or other software tool) <em>product</em> and make a change‚Ä¶ how do you do that nondisruptively?</p>

<h3 id="option-0-let-them-eat-cake">Option 0: Let Them Eat Cake!</h3>

<blockquote>
  <p>‚ÄúNo, it‚Äôs the users who are wrong!</p>
</blockquote>

<p>When someone is upset because of changes to a software tool, the peanut gallery often has responses ready:</p>

<ul>
  <li>‚Äúthe new website layout is different‚Äù
    <ul>
      <li>But it‚Äôs actually a better-thought-out design, see‚Ä¶</li>
    </ul>
  </li>
  <li>‚Äúthey moved all the menu items around in the application I was using‚Äù
    <ul>
      <li>But they‚Äôre all still there, what‚Äôs the problem?</li>
      <li>Why not switch to (other similar software) if you hate it so much?</li>
    </ul>
  </li>
  <li>‚Äúthey removed one (of the several hundred) of capabilities of this piece of software I was using‚Äù
    <ul>
      <li>But everything else is still there, that‚Äôs like 1% of what it does!</li>
      <li>Hardly anyone used that anyway, it‚Äôs better for the authors to stop spending time on it!</li>
    </ul>
  </li>
</ul>

<p>All of which miss the point - which may be understandable, as the complainers themselves may have been gaslit into engaging with the software as if it were a product offered to them, too!</p>

<p>The <em>point</em> of all the complaints is</p>

<blockquote>
  <p>I had been using this <strong>tool</strong> to achieve X, and now I have to go invest additional time and effort to figure out how to keep achieving the same old X.</p>
</blockquote>

<p>The changes impose a cost on the complainers that they have not budgeted for. Even if there is a real benefit to be had, <em>it wasn‚Äôt in the budget</em> of time and attention. That‚Äôs what they‚Äôre upset about.</p>

<p>So, how <strong>do</strong> you make changes to a software tool without inconveniencing the people who use it?</p>

<h3 id="option-1-just-dont">Option 1: Just Don‚Äôt</h3>

<p><code class="language-plaintext highlighter-rouge">Hyrum's Law</code>: Once you‚Äôve got a tool out there and it‚Äôs got behaviors and features and buttons in places, people start building on top of those. If you shake up your ‚Äúsoftware product,‚Äù you risk toppling the things they‚Äôve built. It‚Äôs just as absurd as breaking into a blacksmith‚Äôs workshop overnight and changing the shape and size of the anvil, tongs, and hammer. It doesn‚Äôt matter what you change them to, even if it‚Äôs an objectively better design - you‚Äôve <em>interrupted</em> their workflow.</p>

<p>That‚Äôs what software updates are to software tools. That sort of breaking-and-entering is usually infeasible and criminal in the real world but in the world of software, it‚Äôs easy and commonplace.</p>

<h4 id="why-not-though">Why Not Though?</h4>

<p>I said previously,</p>

<blockquote>
  <p>The more things a tool does, the greater the chance that any change at all will be a detriment to <em>someone</em>.</p>
</blockquote>

<p>It‚Äôs also true that</p>

<blockquote>
  <p>The more you change what a tool does, the greater the chance that you‚Äôll entice a new user to use it.</p>
</blockquote>

<p>After all, if they weren‚Äôt already using it, maybe this new thing or change will make it appealing-enough for them to start! At least, that‚Äôs the Product and Marketing angle.</p>

<p>When you combine these things, you burn a different set of users with each change, and pull in a new set of users with each change. If your tool was actually good-enough to attract users, a small burn from one nuisance change may not drive them away. Two such burns might not. But eventually, you‚Äôll reach a tipping point.</p>

<p>Because tools have such varying use-patterns, though, your first 10 changes aren‚Äôt all going to burn the same user 10 times - it will be distributed across your userbase. You‚Äôll see new users show up for each of the 10 changes and very little churn, and then it‚Äôll look like your <em>product</em> is improving! But then you‚Äôll hit enough changes that you‚Äôve burnt people enough that they start to leave. You make that 11th change and finally some people are fed up and leave. They‚Äôre replaced by new users who were enticed by that 11th change though, so it doesn‚Äôt look disastrous yet. But then you make the 12th change, and another set of users leave, hopefully replaced again.</p>

<p>Now you‚Äôre in a stagnant holding pattern: to attract new users, you are used to making changes. Each change you make will push away a long-time user who has finally had enough, and pull in a new one. Now you‚Äôre not growing anymore. You‚Äôve saturated the upheaveal threshold of your userbase and the techniques you had been using - <em>product development</em> - no longer work. You can do more of it, and it will just burn your resources without growth.</p>

<h4 id="it-can-get-worse">It Can Get Worse</h4>

<p>When products reach that stagnant holding pattern, they get desperate for revenue. It‚Äôs not available from growth anymore, so the inevitable happens‚Ä¶</p>

<blockquote class="link-card">
  
  <h1>Social Quitting - where &#39;enshittification&#39; was coined</h1>
  
  <a href="https://doctorow.medium.com/social-quitting-1ce85b67b456" target="_blank" rel="noopener">doctorow.medium.com/social-quitting-1ce85b67b456</a>
  
  <small class="link-card-archive">
    (<a href="https://archive.ph/uBvd9" target="_blank" rel="noopener">archive</a>)
  </small>
  
</blockquote>

<blockquote>
  <p>When switching costs are high, services can be changed in ways that you dislike without losing your business. The higher the switching costs, the more a company can abuse you, because it knows that as bad as they‚Äôve made things for you, you‚Äôd have to endure worse if you left.</p>
</blockquote>

<p>This results in actual capability degradations, and</p>

<blockquote class="link-card">
  
  <h1>Needy Programs @ tonsky.me</h1>
  
  <a href="https://tonsky.me/blog/needy-programs/" target="_blank" rel="noopener">tonsky.me/blog/needy-programs</a>
  
  <small class="link-card-archive">
    (<a href="https://web.archive.org/web/20251212040814/https://tonsky.me/blog/needy-programs/" target="_blank" rel="noopener">archive</a>)
  </small>
  
</blockquote>

<p>programs that want you to make an account, give an e-mail, download an update, etc. These ‚Äúneedy programs‚Äù are what happens when tool-makers who think their tool <strong>is</strong> the goal come face-to-face with reality. They see people dissatisfied with their tool, abandoning it, and they apply the thumbscrews of enshittification. <strong>No</strong>. Nobody wants to use your tool. Nobody ever <em>wanted</em> to use your tool. They want to use your tool <em>for</em> something, and you‚Äôve demonstrated that your tool is not reliable enough for that. That means your tool is useless. Offering a subscription to a ‚Äúproduct tips‚Äù newsletter isn‚Äôt going to fix that.</p>

<h3 id="option-2-be-like-ls">Option 2: Be like <code class="language-plaintext highlighter-rouge">ls</code></h3>

<p>Tonsky says:</p>

<blockquote>
  <blockquote>
    <p><em><code class="language-plaintext highlighter-rouge">ls</code> never asks you to create an account or to update.</em></p>
  </blockquote>

  <p>I agree. <code class="language-plaintext highlighter-rouge">ls</code> is a good program. <code class="language-plaintext highlighter-rouge">ls</code> is a tool. It does what I need it to do and stays quiet otherwise. I use it; it doesn‚Äôt use me.</p>
</blockquote>

<p>That‚Äôs one of the outcomes you‚Äôll get if you follow the <code class="language-plaintext highlighter-rouge">Unix Philosophy</code> when building things.</p>

<blockquote class="link-card">
  
  <h1>Unix Philosophy @ Harvard CS201</h1>
  
  <a href="https://cscie2x.dce.harvard.edu/hw/ch01s06.html" target="_blank" rel="noopener">cscie2x.dce.harvard.edu/hw/ch01s06.html</a>
  
  <small class="link-card-archive">
    (<a href="https://web.archive.org/web/20251212040814/https://cscie2x.dce.harvard.edu/hw/ch01s06.html" target="_blank" rel="noopener">archive</a>)
  </small>
  
</blockquote>

<p><a href="/garden/a-history-of-the-ls-command.html">Who wrote <code class="language-plaintext highlighter-rouge">ls</code>, anyway?</a> Turns out a bunch of people were involved, but <em>none</em> of those people are known for sitting around shipping updates to <code class="language-plaintext highlighter-rouge">ls</code>, with the goal of trying to get new users, get them to make an account for it, get them to subscribe, etc. <code class="language-plaintext highlighter-rouge">ls</code> is probably available on almost every full computer system on earth. Anything derivative of UNIX? Yes. Any siblings of UNIX? Likely. Windows? In the WSL somewhere. Smartphones? Android is linux-based, so it‚Äôll be in there. iOS is *nix-ish, so it‚Äôll be there, too. It‚Äôs probably literally ubiquitous.</p>

<p>And it just works. It ‚Äúdoes its job and stays quiet otherwise.‚Äù It would never have made it to that level of ubiquity if it was run like a product.</p>

<p>Counter-argument, of course, is those people probably didn‚Äôt make money off of <code class="language-plaintext highlighter-rouge">ls</code> directly, and a lot of people <em>do</em> want to make money off of their software. That‚Äôs fine! Just, make actual <em>products</em> and make money off those. But if you want to build a <em>tool</em>‚Ä¶</p>

<h3 id="option-3-build-tools-that-respect-builders">Option 3: Build Tools that Respect Builders</h3>

<p>The sweet spot is the spirit of <a href="https://semver.org/">Semantic Versioning</a>.</p>

<h4 id="1-be-a-lot-better">1. Be A Lot Better</h4>

<p>Make sure that the change to the tool is actually objectively better for users, and by a good deal.</p>

<ul>
  <li>‚ùå Just-as-good, but different</li>
  <li>‚ùå Only slightly better</li>
</ul>

<p>Remember, you‚Äôre going to <em>interrupt</em> people‚Äôs workflows with your demand for their attention. You will burn goodwill if you don‚Äôt make it worth the effort!</p>

<blockquote>
  <p>It‚Äôs not enough for a new product simply to be better. Unless the gains far outweigh the losses, consumers will not adopt it.</p>
</blockquote>

<blockquote class="link-card">
  
  <h1>Eager Sellers and Stony Buyers - Understanding the Psychology of New-Product Adoption</h1>
  
  <a href="https://web.mit.edu/mamd/www/tech_strat/courseMaterial/topics/topic4/readings/Eager_Sellers_and_Stony_Buyers/Eager_Sellers_and_Stony_Buyers.pdf/Eager_Sellers_and_Stony_Buyers.pdf" target="_blank" rel="noopener">web.mit.edu/mamd/www/tech_strat/courseMaterial/topics/topic4/readings/Eager_Sellers_and_Stony_Buyers/Eager_Sellers_and_Stony_Buyers.pdf/Eager_Sellers_and_Stony_Buyers.pdf</a>
  
  <small class="link-card-archive">
    (<a href="https://web.archive.org/web/20251212040814/https://web.mit.edu/mamd/www/tech_strat/courseMaterial/topics/topic4/readings/Eager_Sellers_and_Stony_Buyers/Eager_Sellers_and_Stony_Buyers.pdf/Eager_Sellers_and_Stony_Buyers.pdf" target="_blank" rel="noopener">archive</a>)
  </small>
  
</blockquote>

<h4 id="2-hold-their-hands">2. Hold Their Hands</h4>

<p>Plan to walk users through the changes - don‚Äôt just leave them to figure it out on their own. Have an interactive walkthrough that highlights major changes. It‚Äôs the <strong>changes</strong>, not the new features, that you need to call out.</p>

<p>You can pitch your <em>new</em> features in marketing copy targeted at new users. You also need to offer to hand-hold your existing users through the <em>disruption</em> you‚Äôre introducing. These are two different audiences - do not forget that!</p>

<h4 id="3-phased-rollout">3. Phased Rollout</h4>

<p>Gradually introduce the change to users, giving them every opportunity to engage with the changes on <em>their</em> terms.</p>

<ol>
  <li><strong>Deployed, opt-in.</strong> Users see a small <em>non-modal</em> notification about the change that invites them to try it. A banner across the top or something - something they can fully ignore with no consequences. Something that does <em>not</em> demand their attention when they opened the software tool to get something done. Clicking takes them to the walkthrough.</li>
  <li><strong>Deployed, opt-in.</strong> Users start receiving modal/pop-up notifications that the change will automatically apply after a given date. Clicking takes them to the walkthrough.</li>
  <li><strong>Deployed, opt-out.</strong> Users are greeted with the walkthrough when they first open the tool after the change from opt-in to opt-out. <strong>This</strong> walkthrough ends with an option to opt-out, with a reminder of the date after which the change will be permanent.</li>
  <li><strong>Fully deployed</strong>, old configuration is gone.</li>
</ol>

<p>You might be thinking that that sounds like a lot of work for what could be a small improvement or change to your tool. You would be right; see the first step above: You <strong>don‚Äôt ship minor changes to a tool</strong>. You let them stack up until the onslaught of change is worthy of your users actually context-switching into learning about it. That‚Äôs how you avoid creating a userbase that‚Äôs constantly annoyed by minor changes every time they use your tool.</p>

<p>But also, once you build that <em>once</em> and integrate it with whatever feature-flag tech you‚Äôre using, the only real work is making that tutorial. And if you can‚Äôt be bothered to write documentation for your long-time users, your tool‚Äôs probably doomed regardless.</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="essay" /><category term="product-management" /><category term="software-development" /><category term="tools" /><summary type="html"><![CDATA[Tools are not products and building them *like* products results in bad tools. Nobody actually wants to use tools. They want the *result* of using the tool.]]></summary></entry><entry><title type="html">Hook-Based Local Mode for ai-rizz</title><link href="https://blog.cani.ne.jp/2025/12/12/hook-based-local-mode-for-ai-rizz.html" rel="alternate" type="text/html" title="Hook-Based Local Mode for ai-rizz" /><published>2025-12-12T00:00:00+00:00</published><updated>2025-12-12T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/12/12/hook-based-local-mode-for-ai-rizz</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/12/12/hook-based-local-mode-for-ai-rizz.html"><![CDATA[<p>Recent Cursor builds on Mac <a href="https://forum.cursor.com/t/cursor-2-1-50-ignores-rules-in-git-info-exclude-on-mac-not-on-windows-wsl/145695/4">ignore all git-ignored files</a>, including those in <code class="language-plaintext highlighter-rouge">.git/info/exclude</code>. This broke <a href="https://github.com/texarkanine/ai-rizz">ai-rizz</a>‚Äôs local mode, which relies on <code class="language-plaintext highlighter-rouge">.git/info/exclude</code> to keep personal rules out of commits while making them visible to Cursor.</p>

<p>The fix: a <code class="language-plaintext highlighter-rouge">--hook-based-ignore</code> flag that uses a pre-commit hook instead of git excludes.</p>

<h2 id="the-problem">The Problem</h2>

<p>ai-rizz has two modes:</p>
<ul>
  <li><strong>Local mode</strong>: Personal rules in <code class="language-plaintext highlighter-rouge">.cursor/rules/local/</code>, git-ignored via <code class="language-plaintext highlighter-rouge">.git/info/exclude</code></li>
  <li><strong>Commit mode</strong>: Team rules in <code class="language-plaintext highlighter-rouge">.cursor/rules/shared/</code>, committed to the repository</li>
</ul>

<p>The <code class="language-plaintext highlighter-rouge">.git/info/exclude</code> approach worked because it kept files out of commits without hiding them from Cursor. Mac Cursor‚Äôs new behavior broke this assumption.</p>

<h2 id="the-solution">The Solution</h2>

<p>When initialized with <code class="language-plaintext highlighter-rouge">--hook-based-ignore</code>, local mode:</p>
<ol>
  <li>Skips adding files to <code class="language-plaintext highlighter-rouge">.git/info/exclude</code></li>
  <li>Installs a pre-commit hook that unstages local files before each commit</li>
  <li>Leaves files visible to Cursor (and <code class="language-plaintext highlighter-rouge">git status</code>)</li>
</ol>

<p>The hook reads the manifest dynamically to find which files to unstage:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Find local manifest in repo root</span>
<span class="nv">REPO_ROOT</span><span class="o">=</span><span class="si">$(</span>git rev-parse <span class="nt">--show-toplevel</span> 2&gt;/dev/null <span class="o">||</span> <span class="nb">pwd</span><span class="si">)</span>
<span class="nv">LOCAL_MANIFEST</span><span class="o">=</span><span class="s2">""</span>
<span class="k">for </span>file <span class="k">in</span> <span class="s2">"</span><span class="nv">$REPO_ROOT</span><span class="s2">"</span>/<span class="k">*</span>.local.skbd<span class="p">;</span> <span class="k">do</span>
    <span class="o">[</span> <span class="nt">-f</span> <span class="s2">"</span><span class="nv">$file</span><span class="s2">"</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="nv">LOCAL_MANIFEST</span><span class="o">=</span><span class="si">$(</span><span class="nb">basename</span> <span class="s2">"</span><span class="nv">$file</span><span class="s2">"</span><span class="si">)</span> <span class="o">&amp;&amp;</span> <span class="nb">break
</span><span class="k">done</span>
<span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$LOCAL_MANIFEST</span><span class="s2">"</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="nb">exit </span>0

<span class="c"># Parse target directory from manifest</span>
<span class="nv">MANIFEST_PATH</span><span class="o">=</span><span class="s2">"</span><span class="nv">$REPO_ROOT</span><span class="s2">/</span><span class="nv">$LOCAL_MANIFEST</span><span class="s2">"</span>
<span class="nv">TARGET_DIR</span><span class="o">=</span><span class="si">$(</span><span class="nb">head</span> <span class="nt">-n1</span> <span class="s2">"</span><span class="nv">$MANIFEST_PATH</span><span class="s2">"</span> 2&gt;/dev/null | <span class="nb">cut</span> <span class="nt">-f2</span><span class="si">)</span>
<span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="nv">$TARGET_DIR</span><span class="s2">"</span> <span class="o">]</span> <span class="o">&amp;&amp;</span> <span class="nb">exit </span>0

<span class="c"># Unstage local files</span>
git reset HEAD <span class="nt">--</span> <span class="s2">"</span><span class="nv">$TARGET_DIR</span><span class="s2">/local/"</span> <span class="s2">"</span><span class="nv">$LOCAL_MANIFEST</span><span class="s2">"</span> 2&gt;/dev/null <span class="o">||</span> <span class="nb">true</span>
</code></pre></div></div>

<p>This works with custom manifest names and target directories since it reads from the manifest itself.</p>

<h2 id="mode-switching">Mode Switching</h2>

<p>Users can switch between modes idempotently:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Switch to hook-based</span>
ai-rizz init <span class="nt">--local</span> <span class="nt">--hook-based-ignore</span>

<span class="c"># Switch back to regular</span>
ai-rizz init <span class="nt">--local</span>
</code></pre></div></div>

<p>Switching from regular to hook-based removes <code class="language-plaintext highlighter-rouge">.git/info/exclude</code> entries and installs the hook. Switching back does the reverse.</p>

<h2 id="the-trade-off">The Trade-off</h2>

<p>Hook-based mode leaves a ‚Äúdirty‚Äù <code class="language-plaintext highlighter-rouge">git status</code>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ git status
On branch main
Untracked files:
  (use "git add &lt;file&gt;..." to include in what will be committed)
        .cursor/rules/local/
        ai-rizz.local.skbd
</code></pre></div></div>

<p>This may conflict with tooling that expects a clean status. But for Mac users whose Cursor can‚Äôt see local rules otherwise, it‚Äôs the only option until Cursor changes behavior.</p>

<h2 id="implementation-details">Implementation Details</h2>

<p>The hook preserves existing user hooks by wrapping the ai-rizz section in <code class="language-plaintext highlighter-rouge">BEGIN</code>/<code class="language-plaintext highlighter-rouge">END</code> markers. On deinit, only the ai-rizz section is removed.</p>

<p>The <code class="language-plaintext highlighter-rouge">validate_git_exclude_state()</code> function now checks for hook presence before warning about missing git excludes - if the hook exists, files don‚Äôt need to be in <code class="language-plaintext highlighter-rouge">.git/info/exclude</code>.</p>

<h2 id="usage">Usage</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Initialize with hook-based mode</span>
ai-rizz init https://github.com/user/rules.git <span class="nt">--local</span> <span class="nt">--hook-based-ignore</span>

<span class="c"># Add rules as normal</span>
ai-rizz add rule my-rule.mdc <span class="nt">--local</span>

<span class="c"># Files remain visible to Cursor but won't be committed</span>
</code></pre></div></div>

<p>The hook is harmless if files are already git-ignored (it becomes a no-op), so it could theoretically run in all local mode setups. For now, it‚Äôs opt-in via the flag.</p>]]></content><author><name>Niko</name></author><category term="blog" /><category term="record" /><category term="ai-rizz" /><category term="cursor" /><category term="git" /><category term="pre-commit" /><category term="tdd" /><summary type="html"><![CDATA[Recent Cursor builds on Mac ignore all git-ignored files, including those in .git/info/exclude. This broke ai-rizz‚Äôs local mode, which relies on .git/info/exclude to keep personal rules out of commits while making them visible to Cursor.]]></summary></entry></feed>