<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://blog.cani.ne.jp/feed.xml" rel="self" type="application/atom+xml" /><link href="https://blog.cani.ne.jp/" rel="alternate" type="text/html" /><updated>2025-11-28T07:40:18+00:00</updated><id>https://blog.cani.ne.jp/feed.xml</id><title type="html">üê∂ Dog with a Dev Blog</title><subtitle>On the internet, nobody knows you&apos;re a dog and a cat and a latent space.</subtitle><entry><title type="html">Two Jekyll Image Plugins</title><link href="https://blog.cani.ne.jp/2025/11/25/two-jekyll-image-plugins.html" rel="alternate" type="text/html" title="Two Jekyll Image Plugins" /><published>2025-11-25T00:00:00+00:00</published><updated>2025-11-25T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/25/two-jekyll-image-plugins</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/25/two-jekyll-image-plugins.html"><![CDATA[<p>Built two custom Jekyll plugins today to handle images in blog posts. What started as ‚ÄúI don‚Äôt want to type the full path‚Äù turned into an exercise in keeping things simple.</p>

<h2 id="the-initial-problem">The Initial Problem</h2>

<p>I had a post in <code class="language-plaintext highlighter-rouge">blog/record/_posts/</code> with an image reference:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">![</span><span class="nv">Look at this dog</span><span class="p">](</span><span class="sx">/blog/record/thisdog.jpg</span><span class="p">)</span>
</code></pre></div></div>

<p>That <code class="language-plaintext highlighter-rouge">/blog/record/</code> path felt redundant - the post already lives in that directory. Why repeat myself? Plus, we had <code class="language-plaintext highlighter-rouge">jekyll-images-cdn</code> for prepending CDN URLs, which was working fine for local development with <code class="language-plaintext highlighter-rouge">/assets/img</code>. We had already shaved <em>that</em> prefix off, why couldn‚Äôt we shave off the rest of the prefix?</p>

<h2 id="first-solution-image_pathsrb">First Solution: image_paths.rb</h2>

<p>Built a plugin that:</p>
<ol>
  <li>Extracts the post‚Äôs directory from its path</li>
  <li>Resolves relative image paths (like <code class="language-plaintext highlighter-rouge">thisdog.jpg</code>) to that directory</li>
  <li>Applies CDN configuration from <code class="language-plaintext highlighter-rouge">_config.yaml</code></li>
</ol>

<p>So now I can write <code class="language-plaintext highlighter-rouge">![Dog](thisdog.jpg)</code> and get <code class="language-plaintext highlighter-rouge">/assets/img/blog/record/thisdog.jpg</code>. Perfect. Removed <code class="language-plaintext highlighter-rouge">jekyll-images-cdn</code> since we integrated that functionality directly.</p>

<h2 id="the-second-problem">The Second Problem</h2>

<p>I have a tall, narrow image that was filling 100% width and dominating the page. I wanted dimension control. Found that some Markdown processors support extended syntax like:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">![</span><span class="nv">Image</span><span class="p">](</span><span class="sx">photo.jpg</span> =300x200)
</code></pre></div></div>

<p>Nice! Let‚Äôs add that.</p>

<h2 id="one-plugin-or-two">One Plugin or Two?</h2>

<p>Should this be in the same plugin or separate? I initially thought ‚Äúthey both process images, combine them!‚Äù But after thinking it through:</p>

<p><strong>Arguments for combining:</strong></p>
<ul>
  <li>Same <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> target</li>
  <li>One regex pass</li>
  <li>Fewer files</li>
</ul>

<p><strong>Arguments for separating:</strong></p>
<ul>
  <li>Single Responsibility Principle</li>
  <li>Different concerns: functional (path resolution) vs presentational (styling)</li>
  <li>Different hook points (sizing needs pre_render to parse Markdown, paths work post_render on HTML)</li>
  <li>Different configs</li>
  <li>Independently useful</li>
</ul>

<p><strong>Decision: Separate.</strong> Path resolution is infrastructure; sizing is aesthetic preference. Keep ‚Äòem apart.</p>

<h2 id="building-image_sizingrb">Building image_sizing.rb</h2>

<p>The implementation was‚Ä¶ educational.</p>

<h3 id="challenge-1-comment-markers">Challenge 1: Comment Markers</h3>

<p>The approach: parse <code class="language-plaintext highlighter-rouge">![alt](url =WIDTHxHEIGHT)</code> in <code class="language-plaintext highlighter-rouge">pre_render</code>, inject an HTML comment marker, then process it in <code class="language-plaintext highlighter-rouge">post_render</code>.</p>

<p>First attempt: Used <code class="language-plaintext highlighter-rouge">=WIDTHxHEIGHT</code> as the marker format.
Problem: Regex parsing got confused when splitting on ‚Äòx‚Äô.</p>

<p>Fixed: Changed to <code class="language-plaintext highlighter-rouge">:</code> separators in the comment: <code class="language-plaintext highlighter-rouge">&lt;!-- IMG_SIZE:300:200:hr --&gt;</code></p>

<h3 id="challenge-2-paragraph-wrapping">Challenge 2: Paragraph Wrapping</h3>

<p>Jekyll wraps some images in <code class="language-plaintext highlighter-rouge">&lt;p&gt;</code> tags, some not. The regex needed to handle both:</p>

<div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sr">/&lt;p&gt;)?&lt;img\s+([^&gt;]*)&gt;&lt;!-- IMG_SIZE:... --&gt;(&lt;\/p&gt;)?/</span>
</code></pre></div></div>

<p>Capture the paragraph tags, preserve them in output.</p>

<h3 id="challenge-3-code-blocks">Challenge 3: Code Blocks</h3>

<p>The plugin was processing image syntax everywhere, including inside code fences and inline code blocks. So examples like <code class="language-plaintext highlighter-rouge">![alt](url =300x200)</code> were getting transformed into <code class="language-plaintext highlighter-rouge">![alt](url)&lt;!-- IMG_SIZE:... --&gt;</code> in the rendered output, which I discovered when I wrote them for this post!</p>

<p>Fixed: Track code fence state line by line, and split each line by backticks to track inline code state. Only process image syntax when outside both.</p>

<p>The logic handles triple-backtick fences, tilde fences, and inline backticks. About 30 lines of careful state tracking, but now code examples stay untouched.</p>

<h3 id="challenge-4-the-horizontal-rules-feature">Challenge 4: The Horizontal Rules Feature</h3>

<p>Initial plan: When height is specified in pixels, wrap with <code class="language-plaintext highlighter-rouge">&lt;hr&gt;</code> tags for visual emphasis.</p>

<p>Implemented it. Worked great! Images with height got horizontal rules above and below.</p>

<p>Then realized: This only triggered when BOTH width and height were specified, not just height alone. The logic was getting complex. And honestly, did I really want automatic HRs?</p>

<p><strong>Decision: Kill the feature.</strong> Let users add <code class="language-plaintext highlighter-rouge">---</code> in their Markdown if they want horizontal rules. Plugin should just do sizing.</p>

<p>Removed ~15 lines of HR logic. Much cleaner.</p>

<h3 id="challenge-5-auto-linking">Challenge 5: Auto-Linking</h3>

<p>After getting sizing working, realized it would be nice if sized images automatically linked to their full resolution. Click the thumbnail, see the full image in a new tab.</p>

<p>But what if an image is already inside a link? Like <code class="language-plaintext highlighter-rouge">[text ![img](url =300x) more](example.com)</code>. Don‚Äôt want to create nested anchors - the inner link would win and break the outer link.</p>

<p><strong>Solution:</strong> Detect if image is already inside an anchor tag. In the second pass of <code class="language-plaintext highlighter-rouge">post_render</code>, after applying sizing:</p>
<ol>
  <li>Find images with width/height attributes</li>
  <li>Scan backwards from the image position</li>
  <li>Find the last <code class="language-plaintext highlighter-rouge">&lt;a&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;/a&gt;</code> tags before the image</li>
  <li>If there‚Äôs an unclosed <code class="language-plaintext highlighter-rouge">&lt;a&gt;</code> ‚Üí inside anchor ‚Üí skip</li>
  <li>Otherwise ‚Üí wrap in <code class="language-plaintext highlighter-rouge">&lt;a href="src" target="_blank" rel="noopener"&gt;</code></li>
</ol>

<p>The regex <code class="language-plaintext highlighter-rouge">/&lt;a[\s&gt;]/</code> avoids matching <code class="language-plaintext highlighter-rouge">&lt;article&gt;</code> and <code class="language-plaintext highlighter-rouge">&lt;aside&gt;</code> tags. Used <code class="language-plaintext highlighter-rouge">rindex</code> to find the most recent occurrence, which is simpler than counting all anchors in the page.</p>

<p>Added <code class="language-plaintext highlighter-rouge">target="_blank"</code> so full images open in new tab, and <code class="language-plaintext highlighter-rouge">rel="noopener"</code> for security.</p>

<p>Tested with images inside text links, standalone sized images, and unsized images. All work correctly.</p>

<h2 id="the-final-result">The Final Result</h2>

<p>Two plugins, each doing one thing well:</p>

<ol>
  <li><strong>image_paths.rb</strong>: Resolves relative paths based on post location, applies CDN config</li>
  <li><strong>image_sizing.rb</strong>: Parses <code class="language-plaintext highlighter-rouge">=WIDTHxHEIGHT</code> syntax, applies dimensions</li>
</ol>

<p>Usage:</p>
<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">![</span><span class="nv">Dog</span><span class="p">](</span><span class="sx">thisdog.jpg</span> =x400)
</code></pre></div></div>

<p>Output:</p>
<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;a</span> <span class="na">href=</span><span class="s">"/assets/img/blog/record/thisdog.jpg"</span> <span class="na">target=</span><span class="s">"_blank"</span> <span class="na">rel=</span><span class="s">"noopener"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;img</span> <span class="na">src=</span><span class="s">"/assets/img/blog/record/thisdog.jpg"</span> <span class="na">alt=</span><span class="s">"Dog"</span> <span class="na">height=</span><span class="s">"400"</span><span class="nt">&gt;</span>
<span class="nt">&lt;/a&gt;</span>
</code></pre></div></div>

<p>Sized images auto-link to full resolution. Unsized images stay plain. Images already inside links don‚Äôt get double-wrapped.</p>

<p>Both plugins run independently in sequence. No coupling. No complexity.</p>

<h2 id="what-i-learned">What I Learned</h2>

<p><strong>Separate concerns early.</strong> Even if plugins process the same elements, if they serve different purposes, split them.</p>

<p><strong>Feature creep happens, but not all features are equal.</strong> The HR feature added complexity for marginal benefit (I can add <code class="language-plaintext highlighter-rouge">---</code> myself). But auto-linking sized images to full resolution? That‚Äôs genuinely useful every time, and the complexity is justified. Know the difference.</p>

<p><strong>Debug output matters.</strong> Added <code class="language-plaintext highlighter-rouge">puts</code> statements to see exactly what the regex was capturing. Crucial for debugging that <code class="language-plaintext highlighter-rouge">:</code> vs <code class="language-plaintext highlighter-rouge">x</code> separator issue.</p>

<p><strong>Test with real content.</strong> Created test posts with all the edge cases (width only, height only, both, neither). Caught several regex bugs this way.</p>

<p><strong>HTML comments as data carriers work well.</strong> Injecting structured comments in <code class="language-plaintext highlighter-rouge">pre_render</code> then parsing them in <code class="language-plaintext highlighter-rouge">post_render</code> bridged the Markdown‚ÜíHTML transformation cleanly.</p>

<p><strong>Simple is better.</strong> The final versions are ~50-75 lines each (well, image_sizing grew to ~150 with the linking feature), well-documented, with no clever tricks. Exactly what I wanted.</p>

<p><strong>State tracking isn‚Äôt scary.</strong> The code fence detection and anchor detection both required tracking state (are we inside something?). The logic is straightforward: find the last opening/closing pair, compare positions.</p>

<h2 id="whats-next">What‚Äôs Next?</h2>

<p>Nothing. These plugins are done. They solve the problems I had, nothing more. And that‚Äôs exactly right.</p>

<p>Now back to writing actual blog posts instead of building infrastructure for them‚Ä¶</p>]]></content><author><name>Niko</name></author><category term="blog" /><category term="diary" /><category term="jekyll" /><category term="ruby" /><category term="markdown" /><summary type="html"><![CDATA[Built two custom Jekyll plugins today to handle images in blog posts. What started as ‚ÄúI don‚Äôt want to type the full path‚Äù turned into an exercise in keeping things simple.]]></summary></entry><entry><title type="html">Look at this Dog</title><link href="https://blog.cani.ne.jp/2025/11/25/look-at-this-dog.html" rel="alternate" type="text/html" title="Look at this Dog" /><published>2025-11-25T00:00:00+00:00</published><updated>2025-11-25T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/25/look-at-this-dog</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/25/look-at-this-dog.html"><![CDATA[<p><img src="thisdog.jpg" alt="Look at this dog" /><!-- IMG_SIZE:auto:400 --></p>

<hr />

<p>Now look back at me.</p>

<p>Did you look at the dog? I can‚Äôt tell, because that image isn‚Äôt loaded through the <a href="/2025/11/23/opensearch-for-static-site-logs.html">nginx reverse-proxy that sits in front of the blog to provide metrics</a>.</p>

<p>Why, though?</p>

<blockquote>
  <p>I want to minimize the usage of resource-constrained and metered cloud resources involved in serving a page.</p>
</blockquote>

<p>The metrics-enabling nginx reverse-proxy was built before there were any images or other media posted here; it only had to serve text. However, adding media and doing nothing else could now potentially take up significant <em>metered</em> DigitalOcean compute and bandwidth on the nginx instance. For a <em>static site</em>, that seems even goofier than the nginx metrics solution already was.</p>

<p>I had several options:</p>

<h2 id="option-1-separate-app--jekyll-rewrite">Option 1: Separate App &amp; Jekyll Rewrite</h2>

<p>I could deploy the site a second time to a different DigitalOcean App Engine App w/ a static site, something like <code class="language-plaintext highlighter-rouge">dogblog-media-XXXX.ondigitalocean.app</code>. Then, I could have Jekyll render the base url as a prefix when rendering <code class="language-plaintext highlighter-rouge">&lt;img&gt;</code> tags, giving</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img src="https://dogblog-media-XXX.ondigitalocean.app/path/to/img.jpg"&gt;
</code></pre></div></div>

<p>instead of a relative url.</p>

<p><strong>BUT</strong></p>

<ol>
  <li>This would be a ‚Äúcross-origin‚Äù request, and some browsers or situations might not care for that.</li>
  <li>If I ever changed the image host URL, cached versions of the site (or others‚Äô links to the images) could break.</li>
  <li>I‚Äôd have to add a rule to the nginx to block <code class="language-plaintext highlighter-rouge">/assets</code> requests on the main site.</li>
</ol>

<h2 id="option-2-separate-app--nginx-rewrite">Option 2: Separate App &amp; nginx Rewrite</h2>

<p>I could deploy the site a second time as before, but have the nginx intercept the <code class="language-plaintext highlighter-rouge">/assets</code> path on the main domain and return an HTTP 3XX redirect.</p>

<p>This would also give me metrics on media requests!</p>

<p><strong>BUT</strong></p>

<ol>
  <li>I‚Äôm still adding traffic to the nginx, and my inital goal was to reduce it!</li>
  <li>There‚Äôs still ultimately a cross-domain situation, though the request path is slightly less sketchy.</li>
  <li>If I ever changed the image host URL, cached versions of the site (or others‚Äô links to the images) could break.</li>
</ol>

<h2 id="option-3-same-app-separate-site">Option 3: Same App, Separate Site</h2>

<p>I could deploy the site a second time into the same DigitalOcean App Engine App, and bind it to the <code class="language-plaintext highlighter-rouge">/assets</code> path.
This will cause DigitalOcean‚Äôs gateway (which is front of my nginx) to intercept requests to <code class="language-plaintext highlighter-rouge">/assets</code> and route them to the second static site.  This happens <em>behind</em> the main blog domain name; no secondary URLs involved!</p>

<p>This second site would be built off the <code class="language-plaintext highlighter-rouge">/assets</code> subdirectory so it could not possibly serve normal pages. Additional protection against serving normal pages comes from it being part of the main blog App: all paths <em>other</em> than <code class="language-plaintext highlighter-rouge">/assets</code> are routed by DigitalOcean to my nginx, which serves normal pages from their proper place.</p>

<p>Additionally, DigitalOcean‚Äôs gateway is already routing all non-<code class="language-plaintext highlighter-rouge">/assets</code> requests to my nginx - thus saving me from having to add an explicit rule preventing the secondary domain from incorrectly serving them without metrics.</p>

<p>That‚Äôs much simpler, and that‚Äôs what I did!</p>

<hr />

<p>This setup does <em>not</em> get me metrics on media asset requests, but I can live with that. I‚Äôm building a blog, not a media host, after all.</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="record" /><category term="jekyll" /><category term="digitalocean" /><category term="cdn" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">I Can‚Äôt ai-rizz on Command</title><link href="https://blog.cani.ne.jp/2025/11/24/cant-ai-rizz-on-command.html" rel="alternate" type="text/html" title="I Can‚Äôt ai-rizz on Command" /><published>2025-11-24T00:00:00+00:00</published><updated>2025-11-24T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/24/cant-ai-rizz-on-command</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/24/cant-ai-rizz-on-command.html"><![CDATA[<p>Cursor introduced ‚Äú<a href="https://cursor.com/docs/agent/chat/commands">Slash Commands</a>‚Äù fairly recently, in v1.6 in September 2025. Claude Code had had a similar thing - <a href="https://code.claude.com/docs/en/skills">Agent Skills</a> - for a long time.</p>

<p>I recently saw Cursor add ‚ÄúTeam Commands‚Äù for enterprise accounts and got excited to finally leverage the power of Claude‚Äôs Agent Skills, but in Cursor for my team.</p>

<p>I blew the rest of my prepaid Cursor usage on trying to add Cursor Command support to <a href="https://github.com/texarkanine/ai-rizz">ai-rizz</a>, but hit two snags:</p>

<h2 id="commands-just-aint-rules">Commands Just Ain‚Äôt Rules</h2>

<h3 id="directory-structure-is-part-of-the-command">Directory Structure is Part of the Command</h3>

<p>A Cursor Command located in <code class="language-plaintext highlighter-rouge">./cursor/commands/foo.md</code> can be triggered by typing <code class="language-plaintext highlighter-rouge">/foo</code>.</p>

<p>A Cursor Command located in <code class="language-plaintext highlighter-rouge">./cursor/commands/foo/bar.md</code> can be triggered by typing <code class="language-plaintext highlighter-rouge">/foo/bar</code></p>

<p>This meant that the pattern <code class="language-plaintext highlighter-rouge">ai-rizz</code> used to ensure its managed rules stayed separate from other rules, of ‚Äúclaiming‚Äù two subdirectories of the <code class="language-plaintext highlighter-rouge">.cursor/rules</code> directory for itself:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>.cursor/rules
‚îú‚îÄ‚îÄ local
‚îÇ   ‚îî‚îÄ‚îÄ my-local-rule.mdc
‚îî‚îÄ‚îÄ shared
    ‚îî‚îÄ‚îÄ my-shared-rule.mdc
</code></pre></div></div>

<p>wouldn‚Äôt work. The commands became unnecessarily prefixed with junk.</p>

<p>So, they really needed to go in <code class="language-plaintext highlighter-rouge">.cursor/commands/*.md</code> directly.</p>

<p>I came up with an idea, though: We‚Äôd store them in <code class="language-plaintext highlighter-rouge">.cursor/shared-commands</code>, and symlink them into <code class="language-plaintext highlighter-rouge">.cursor/commands</code>. This allowed prefix-less use of the commands but also kept the managed commands easily-identifiable - all symlinks in <code class="language-plaintext highlighter-rouge">.cursor/commands</code> to <code class="language-plaintext highlighter-rouge">.cursor/shared-commands/*.md</code> were managed, and all other commands weren‚Äôt.</p>

<h3 id="tracking">Tracking</h3>

<p>Clever design in <code class="language-plaintext highlighter-rouge">ai-rizz</code> makes keeping track of rules‚Äô provenance easy: simply knowing a rule‚Äôs path on disk is enough to know if it should be in source control, whether it should be ignored, or whether it‚Äôs a ‚Äúuser‚Äù rule not managed by <code class="language-plaintext highlighter-rouge">ai-rizz</code>.</p>

<p>This is only manageable because the <code class="language-plaintext highlighter-rouge">.cursor/rules/local</code> directory is ignored by git with a single <code class="language-plaintext highlighter-rouge">.git/info/exclude</code> entry - one configuration line and you can put <em>all</em> rules you need in <code class="language-plaintext highlighter-rouge">local</code> and they‚Äôre all untracked. You put all the tracked ones in <code class="language-plaintext highlighter-rouge">shared</code>, and any other directory tree is user rules.</p>

<p>What if we tried to find that semantic for symlinks in <code class="language-plaintext highlighter-rouge">.cursor/commands/</code>, though?</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">.cursor/commands/local-cmd.md.lnk</code> -&gt; should not be committed</li>
  <li><code class="language-plaintext highlighter-rouge">.cursor/commands/committed-cmd.md.lnk</code> -&gt; should be committed</li>
  <li><code class="language-plaintext highlighter-rouge">.cursor/commands/actual-cmd.md</code> -&gt; user command, should not be touched</li>
</ul>

<p>Now we have to read &amp; write exhaustive git ignore specs that identify every local command. Moreover, with rules, simply knowing a rule‚Äôs path on disk was enough to know if it was a local, committed, or user rule. With commands all in one directory, you have to read the <code class="language-plaintext highlighter-rouge">ai-rizz</code> manifest each time to figure out the semantic for a given command. That‚Äôs a lot of bookkeeping and opportunity to go wrong.</p>

<p>I came up with a solution I thought could work: Declare that commands are commit-only in a repo. Tell the user to go set up <code class="language-plaintext highlighter-rouge">ai-rizz</code> in their user directory and have it manage <code class="language-plaintext highlighter-rouge">~/.cursor/commands</code> (this works) for commands that will live on their machine and not in the repo. Yes, the commands <em>would</em> become available to all repos, but that is primarily what I find myself doing with local <em>rules</em>, anyway - I pull the same set of customizations into every repo I work with, in ‚Äúlocal‚Äù mode. If they could be ‚Äúglobal‚Äù to my local machine, that would be handy!</p>

<h3 id="promotion">Promotion</h3>

<p><code class="language-plaintext highlighter-rouge">ai-rizz</code> can store <em>rules</em> with or without git tracking in a repository. You can try them out locally, and the ones that work really well with the repository get <strong>promoted</strong> to being committed to source control.</p>

<p>What does ‚Äúpromotion‚Äù look like for Commands?</p>

<p>First of all, the promotion semantic is inverted for commands: A command might be committed to one repo at first, and then you might really find it useful and want to promote it to <strong>all</strong> repos. So it goes from ‚Äúcommit‚Äù to ‚Äúlocal‚Äù (which is really ‚Äúglobal,‚Äù as we‚Äôll see‚Ä¶)</p>

<p>With that semantic and my ideas for command management so far, the existing code would do a very wrong thing: Promoting a command would pull it <strong>out</strong> of a repository and into the user‚Äôs home directory. That‚Äôs only correct if development is entirely by one user on one machine. Otherwise, when the user promotes their command they‚Äôll commit and push the repo without the command in it and now their colleagues will lose access to that command! That‚Äôs the <em>opposite</em> of what <code class="language-plaintext highlighter-rouge">ai-rizz</code> is supposed to facilitate!</p>

<p>So, flip the semantic, right? Commands <em>start</em> in <code class="language-plaintext highlighter-rouge">~/.cursor/commands</code>, and you commit them to a repo <strong>iff you like them!</strong> Done that way, a command starts out available in <em>every</em> repository, not the one you‚Äôre working with at the moment. Weird but probably harmless.</p>

<p>But then when you like the command and ‚Äúpromote‚Äù it to be committed to the repo so that everyone who works with the repo gets it, it gets pulled out of your user home and now the rest of <em>your</em> repositories lose access to it!</p>

<p><strong>There‚Äôs just no sensible promotion semantic for Commands:</strong> You have to allow duplication, where promotion <em>copies</em> it to <code class="language-plaintext highlighter-rouge">~/.cursor/commands</code> but also keeps it in each repo it was added to.</p>

<p>And that meant that</p>

<ol>
  <li>I wouldn‚Äôt be able to re-use much of <code class="language-plaintext highlighter-rouge">ai-rizz</code>‚Äôs rules codebase for commands</li>
  <li>Users of <code class="language-plaintext highlighter-rouge">ai-rizz</code> would have to maintain two very different mental models</li>
</ol>

<h2 id="what-are-they-for-anyway">What Are They For, Anyway?</h2>

<p>While I was stewing over how to possibly make this work, I had an epiphany: <strong>I don‚Äôt have a use-case for Cursor Commands, anway!</strong>
You can go read about why, <a href="/2025/11/24/usent-case-for-ai-coding-agent-slash-commands.html">here</a>.</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="diary" /><category term="cursor" /><category term="ai" /><category term="claude-code" /><summary type="html"><![CDATA[Cursor introduced ‚ÄúSlash Commands‚Äù fairly recently, in v1.6 in September 2025. Claude Code had had a similar thing - Agent Skills - for a long time.]]></summary></entry><entry><title type="html">The Usen‚Äôt Case for AI Coding Agent Slash Commands</title><link href="https://blog.cani.ne.jp/2025/11/24/usent-case-for-ai-coding-agent-slash-commands.html" rel="alternate" type="text/html" title="The Usen‚Äôt Case for AI Coding Agent Slash Commands" /><published>2025-11-24T00:00:00+00:00</published><updated>2025-11-24T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/24/usent-case-for-ai-coding-agent-slash-commands</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/24/usent-case-for-ai-coding-agent-slash-commands.html"><![CDATA[<p>Cursor introduced ‚Äú<a href="https://cursor.com/docs/agent/chat/commands">Slash Commands</a>‚Äù fairly recently, in v1.6 in September 2025. Claude Code had had a similar thing - <a href="https://code.claude.com/docs/en/skills">Agent Skills</a> - for a long time.</p>

<h1 id="thesis">Thesis</h1>

<blockquote>
  <p>It‚Äôs not just useless, but an efficiency-reducing antipattern to bind a text prompt to a concrete slash-command for an AI coding agent.</p>
</blockquote>

<p><strong>Corollary</strong></p>

<blockquote>
  <p>Anything you would want to actively invoke as a slash-command, you <em>also</em> want to be a rule that the Agent can passively pick up and apply when necessary.</p>
</blockquote>

<p>LLMs‚Äô whole raison d‚Äô√™tre is (pseudo-)stochastic natural language processing and synthesis. When you reach the point where you‚Äôre trying to codify a <em>function</em> - a fixed set of inputs and a predictable output that depends on them - you‚Äôve exited the domain where LLMs excel. Building a Cursor Command or Claude Agent Skill is an antipattern that makes you less efficient. Deterministic, reliable input/output is the domain of CPU-bound traditional code. You want a fixed command that takes an input and reliably produces an output? We have a technique for that, called <em>writing software</em>. Just write the tool.</p>

<p>With a slash-command, you get:</p>

<ol>
  <li>Extra time for the network round-trip to the Agent‚Äôs datacenter</li>
  <li>Extra time for the Agent to figure out what you want</li>
  <li>Extra cost for the GPU(s) to run the computation</li>
  <li>Extra time fixing the instances where the stochastic output is <em>not</em> what you expected</li>
  <li>Data privacy concerns</li>
</ol>

<p>Furthermore, as I‚Äôll explore below, all technical differences between slash-commands and other methods of software development are distinctions without a difference.</p>

<h2 id="strawmen">Strawmen</h2>

<blockquote>
  <p>It takes longer to write an actual tool than to ask the Agent.</p>
</blockquote>

<p>True, but: Ask the Agent to write it for you. They‚Äôre very good at that.</p>

<blockquote>
  <p>I have to have a runtime on my machine, like python or node or-</p>
</blockquote>

<p>Right now, you already have the ‚Äúruntime‚Äù of a remote shell hooked up to a functionally-nondeterministic artificial intelligence running on someone else‚Äôs computer. True, you can get to <em>an</em> output faster this way. But it is <em>by construction</em> an unreliable solution. If you intend to codify something re<strong>usable</strong>, this can‚Äôt be the answer.</p>

<blockquote>
  <p>Claude Code doesn‚Äôt have rules like Cursor, so I can‚Äôt just <code class="language-plaintext highlighter-rouge">@reference</code> them.</p>
</blockquote>

<p>Just write a Markdown file and tell the Agent to follow it - no special features required. Boom, there‚Äôs your ‚Äúcommand:‚Äù <code class="language-plaintext highlighter-rouge">Do what my-command-prompt.md says.</code></p>

<p>Yes, it‚Äôs a few more characters than <code class="language-plaintext highlighter-rouge">/my-command</code>. Don‚Äôt cut yourself off from the efficiencies AI coding agents can deliver to occasionally save yourself a dozen keystrokes.</p>

<p><strong>PROTIP:</strong> I just dictate (speech to text) to the AI agents so I don‚Äôt have to type. If you have a laptop running a major operating system, you can do this, too!</p>

<h2 id="steelmen">Steelmen</h2>

<h3 id="mutually-exclusive-workflows">Mutually-Exclusive Workflows</h3>

<p>If you have mutually-exclusive directives that <em>must</em> be available for <em>your</em> use, but invisible to the Agent, then slash-commands have some utility.</p>

<p>Maybe you have two different style guides with no firm rule for application - you just choose one when you want when you know it‚Äôs appropriate. You definitely don‚Äôt want the Agent to see both of them, as it may pick the wrong one or blend them.</p>

<p>Then, the conflicting prompts ‚Äúhide‚Äù completely out-of-scope in slash-commands, waiting for you to bring them into the conversation. This is perhaps even a slight improvement over just writing <code class="language-plaintext highlighter-rouge">my-command.md</code> and explicitly asking the agent for it when you want, as there‚Äôs <em>no</em> risk of the agent <em>ever</em> reading or executing a command that you didn‚Äôt <code class="language-plaintext highlighter-rouge">/invoke</code>. Probably, right?</p>

<p>I assert that this situation is incredibly rare, and that most of the time a situation that looks like this would actually benefit from the prompts being <em>Cursor Rules</em> that could be applied by the Agent when necessary.</p>

<h3 id="interleaved-nlp--cpu-tasks">Interleaved NLP &amp; CPU Tasks</h3>

<p>If you have a workflow you want to codify into something re-usable <em>and</em> that workflow has chronologically-interleaved NLP and deterministic tasks, then a slash-command sounds tempting.</p>

<p>Consider Cursor‚Äôs example of <code class="language-plaintext highlighter-rouge">/pr</code> to open a GitHub Pull request, where the Agent has to identify the git diff (deterministic), figure out prose to describe the changes (NLP), find and fit it into a <code class="language-plaintext highlighter-rouge">PULL_REQUEST_TEMPLATE.md</code> if one exists (NLP), figure out the tooling available for opening a pull request (NLP), then invoke the tool (deterministic)‚Ä¶</p>

<p>Oh, exactly like <a href="https://github.com/Texarkanine/.cursor-rules/blob/main/rules/github-open-a-pull-request-gh.mdc">this Cursor Rule</a> that you don‚Äôt have to explicitly, intentionally <code class="language-plaintext highlighter-rouge">/pr</code> for - you can just tell the agent to</p>

<blockquote>
  <p>&lt;your actual prompt&gt; and open a pull request</p>
</blockquote>

<p>Again, <em>yes</em> <code class="language-plaintext highlighter-rouge">/pr</code> is little shorter but if you do that then you <em>can‚Äôt</em> say <code class="language-plaintext highlighter-rouge">and open a pull request</code> or <code class="language-plaintext highlighter-rouge">then open a pr</code> or <code class="language-plaintext highlighter-rouge">then send a pr</code> - you always have to remember the explicit command instead of just being able to have your natural language processed.</p>

<p>Note that this only even applies when there are NLP and deterministic tasks chronologically <em>interleaved.</em> If the task is just ‚Äúsome NLP <em>and also</em> something deterministic,‚Äù you probably want a proper <strong>Tool</strong> that the Agent can just figure out all the inputs for and then invoke, handing execution off to truly-deterministic traditional software.</p>

<h2 id="what-would-change-my-mind">What Would Change My Mind</h2>

<p>If, instead of just binding to text prompts, slash-commands offered a structured way to bind a command to a combination of LLM NLP and deterministic software.</p>

<p>ü™Ñ Oh, we have that! It‚Äôs called a <strong>Tool</strong>. You might know them from <a href="https://modelcontextprotocol.io/specification/2025-03-26">Model Context Protocol (MCP)</a>.</p>

<p>Don‚Äôt waste your time with coding agent slash-commands that bind to textual prompts.</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="essay" /><category term="cursor" /><category term="ai" /><category term="claude-code" /><summary type="html"><![CDATA[Cursor introduced ‚ÄúSlash Commands‚Äù fairly recently, in v1.6 in September 2025. Claude Code had had a similar thing - Agent Skills - for a long time.]]></summary></entry><entry><title type="html">I Built a Logging Server to Log a Serverless Site</title><link href="https://blog.cani.ne.jp/2025/11/23/opensearch-for-static-site-logs.html" rel="alternate" type="text/html" title="I Built a Logging Server to Log a Serverless Site" /><published>2025-11-23T00:00:00+00:00</published><updated>2025-11-23T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/23/opensearch-for-static-site-logs</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/23/opensearch-for-static-site-logs.html"><![CDATA[<p>I wanted access logs for my static site. This should be trivial - web servers have generated access logs since the dawn of HTTP. But <a href="https://www.digitalocean.com/community/tutorials/how-to-deploy-a-static-website-to-the-cloud-with-digitalocean-app-platform">DigitalOcean‚Äôs static site hosting</a>, elegant in its simplicity, doesn‚Äôt generate them. To get visibility into who visits my blog, I‚Äôd need to build my own logging infrastructure.</p>

<p>The plan: deploy <a href="https://opensearch.org/">OpenSearch</a> on my home server, forward logs from DigitalOcean, and build dashboards to analyze traffic. Simple enough.</p>

<h2 id="the-first-lesson-storage">The First Lesson: Storage</h2>

<p>OpenSearch runs best in Docker. The official images provide everything needed - just add a <code class="language-plaintext highlighter-rouge">docker-compose.yaml</code> and go. I specified named volumes for data persistence:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">volumes</span><span class="pi">:</span>
  <span class="s">opensearch-data:/usr/share/opensearch/data</span>
</code></pre></div></div>

<p>This looks clean, but named Docker volumes live in <code class="language-plaintext highlighter-rouge">/var/lib/docker/volumes/</code> by default. My root partition was already 47% full; my <code class="language-plaintext highlighter-rouge">/data</code> partition had 196GB free. And <code class="language-plaintext highlighter-rouge">/data</code> is a RAID array. I needed bind mounts instead:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">/data/opensearch/data:/usr/share/opensearch/data</span>
</code></pre></div></div>

<h2 id="the-user-permission-problem">The User Permission Problem</h2>

<p>I run services on my home server as specific users, not as root or random UIDs. OpenSearch runs as UID 1000 inside its container. UID 1000 on my system is a real user who shouldn‚Äôt have access to OpenSearch data. The solution seemed obvious:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">services</span><span class="pi">:</span>
  <span class="na">opensearch</span><span class="pi">:</span>
    <span class="na">user</span><span class="pi">:</span> <span class="s2">"</span><span class="s">1003:1004"</span>  <span class="c1"># opensearch user</span>
</code></pre></div></div>

<p>The containers started, then immediately failed:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>opensearch | /bin/bash: ./opensearch-docker-entrypoint.sh: Permission denied
</code></pre></div></div>

<p>The entrypoint scripts inside the container image are owned by UID 1000. Running the container as UID 1003 meant that user couldn‚Äôt execute them. Container images bake in assumptions about who runs them.</p>

<p>I removed the <code class="language-plaintext highlighter-rouge">user:</code> directive and let the containers run as their default UID 1000. Docker provides isolation; the containers can‚Äôt escape to become that user on the host system anyway. Sometimes the path of least resistance is correct.</p>

<h2 id="the-ssl-certificate-dance">The SSL Certificate Dance</h2>

<p>OpenSearch would receive logs over HTTPS from DigitalOcean. I needed an SSL certificate for a domain that I could NAT into my home server. Let‚Äôs Encrypt provides free certificates, but their standard HTTP-01 validation requires port 80 accessible from the internet. Port 80 on my server was already occupied, and I didn‚Äôt want to expose another service to the scanner bots that probe every public port.</p>

<p>DNS-01 validation was the answer. Instead of serving a file on port 80, I‚Äôd prove domain ownership by creating DNS TXT records. The catch: Let‚Äôs Encrypt‚Äôs <code class="language-plaintext highlighter-rouge">certbot</code> doesn‚Äôt support my DNS provider, FreeDNS.</p>

<p>I found <a href="https://github.com/acmesh-official/acme.sh"><code class="language-plaintext highlighter-rouge">acme.sh</code>, an alternative ACME client that supports FreeDNS via its API</a>. Installing it as root (not as my user account) ensured automatic renewals would work.</p>

<p>The tool created a DNS TXT record, Let‚Äôs Encrypt verified it, and I had a certificate. No port 80 exposure required. The certificate renews automatically every 60 days via a cron job, and nginx reloads seamlessly when it does.</p>

<h2 id="the-irony-of-static-sites">The Irony of Static Sites</h2>

<p>With OpenSearch running and SSL configured, I turned to DigitalOcean‚Äôs log forwarding feature. Their UI is straightforward: select OpenSearch as the destination, provide the endpoint URL, configure authentication. But when I looked for the logs to forward, I discovered the problem.</p>

<p>DigitalOcean‚Äôs static site hosting doesn‚Äôt generate access logs.</p>

<p>The feature exists for their App Platform compute instances, but static sites - being static - have no application logs to forward. I‚Äôd need to deploy a compute instance that proxies requests to the static site just to generate the logs I wanted.</p>

<p>The irony: I‚Äôm paying $0/month for static site hosting because there‚Äôs no server-side processing. To get access logs, I‚Äôd need to pay $5/month for a server that does nothing but proxy requests and log them.</p>

<p>I built an <a href="https://nginx.org/">nginx</a> container that accepts requests, logs them in JSON format, and forwards them to the actual static site. The container uses environment variables for configuration - no identifying information committed to the repository:</p>

<div class="language-nginx highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">upstream</span> <span class="s">backend_static</span> <span class="p">{</span>
    <span class="kn">server</span> $<span class="p">{</span><span class="kn">UPSTREAM_HOST</span><span class="err">}</span><span class="p">:</span><span class="mi">443</span><span class="p">;</span>
<span class="p">}</span>

<span class="kn">location</span> <span class="n">/</span> <span class="p">{</span>
    <span class="kn">proxy_pass</span> <span class="s">https://backend_static</span>$<span class="p">{</span><span class="kn">UPSTREAM_PATH</span><span class="err">}</span><span class="p">;</span>
    <span class="kn">proxy_set_header</span> <span class="s">Host</span> $<span class="p">{</span><span class="kn">UPSTREAM_HOST_HEADER</span><span class="err">}</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>GitHub Actions builds the image automatically and publishes it to GitHub Container Registry. DigitalOcean pulls the image, and I configure the necessary environment variables in their UI. The static site remains unchanged; the proxy sits in front of it.</p>

<p>The setup works. I‚Äôm paying $5/month for visibility into my site‚Äôs traffic.</p>

<h2 id="the-log-processing-pipeline">The Log Processing Pipeline</h2>

<p>DigitalOcean forwards logs to OpenSearch, but they contain a mix of JSON access logs and plain text error messages. OpenSearch needs to parse them correctly.</p>

<p>An <a href="https://docs.opensearch.org/latest/ingest-pipelines/">ingest pipeline</a> handles this. It attempts to parse each log entry as JSON. If parsing succeeds, it extracts the fields and marks the entry as <code class="language-plaintext highlighter-rouge">log_type: "access"</code>. If parsing fails, it marks it as <code class="language-plaintext highlighter-rouge">log_type: "error"</code> and stores the raw text:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"processors"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"json"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"field"</span><span class="p">:</span><span class="w"> </span><span class="s2">"log"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"target_field"</span><span class="p">:</span><span class="w"> </span><span class="s2">"parsed"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"ignore_failure"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"script"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"source"</span><span class="p">:</span><span class="w"> </span><span class="s2">"if (ctx.containsKey('parsed') &amp;&amp; ctx['parsed'] != null) {
          ctx['log_type'] = 'access';
          for (def entry : ctx['parsed'].entrySet()) {
            ctx[entry.getKey()] = entry.getValue();
          }
          ctx.remove('parsed');
        } else {
          ctx['log_type'] = 'error';
          ctx['error_message'] = ctx['log'];
        }"</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>The pipeline lives in OpenSearch‚Äôs cluster state. Install it once via the API, and it persists across restarts. I created an index template that applies the pipeline automatically to the index Opensearch is forwarding logs to.</p>

<h2 id="the-final-architecture">The Final Architecture</h2>

<p>The pieces work together:</p>

<ol>
  <li>Visitor requests <code class="language-plaintext highlighter-rouge">blog.cani.ne.jp</code></li>
  <li>DNS resolves to DigitalOcean App Platform</li>
  <li>Nginx proxy container (running on DO) receives the request</li>
  <li>Proxy logs the request as JSON to stdout</li>
  <li>Proxy forwards the request to the actual static site</li>
  <li>Static site returns the content</li>
  <li>DigitalOcean‚Äôs log forwarding sends logs to my home server‚Äôs OpenSearch endpoint</li>
  <li>Nginx on my home server (with Let‚Äôs Encrypt certificate) receives them</li>
  <li>OpenSearch ingests and indexes the logs</li>
  <li>Pipeline extracts fields from JSON logs</li>
  <li>OpenSearch Dashboards visualizes the data</li>
</ol>

<p>I can now see which pages are popular, where visitors come from, and how they navigate the site. The dashboards show traffic patterns I couldn‚Äôt see before. Importantly, this is from server-side metrics - no JavaScript required!</p>

<h2 id="what-it-cost">What It Cost</h2>

<p>The final setup:</p>
<ul>
  <li>OpenSearch cluster on my home server: ‚Äú$0‚Äù (existing hardware)</li>
  <li>SSL certificate from Let‚Äôs Encrypt: $0 (DNS-01 validation via FreeDNS)</li>
  <li>Nginx proxy running on DigitalOcean: $5/month (Basic instance)</li>
  <li>Static site hosting: $0 (unchanged)</li>
</ul>

<p>I‚Äôm paying five dollars a month so my free static site can generate access logs.</p>

<p>The nginx proxy is completely transparent. Visitors see no difference in performance or behavior‚Ä¶ if anything there may be a slight degradation as the tiny compute instance hosting the nginx is likely less-performant than the CDN-powered static site it sits in front of. The proxy accepts the request, logs it to stdout in JSON format, forwards the request to the actual static site behind it, and returns the response. DigitalOcean‚Äôs log forwarding picks up those JSON logs and sends them to my OpenSearch endpoint over HTTPS. The OpenSearch ingest pipeline parses the JSON, extracts the fields, and indexes the documents. The dashboards update automatically.</p>

<p>I built a logging server to log a serverless site.</p>

<p>The absurdity isn‚Äôt lost on me. Static site hosting exists because you don‚Äôt need a server - your content sits in object storage behind a CDN. The whole point is the absence of computation. But I wanted to know who‚Äôs reading my blog, and logs require computation. Someone has to write them.</p>

<p>So I rebuilt the server. Five dollars a month. Sixty dollars a year. The OpenSearch cluster hums along on my home server, accepting logs, parsing them with a pipeline that detects JSON versus plain text errors, and displaying them in dashboards I can access from my LAN.</p>

<p>It works. If you‚Äôre reading this, you‚Äôre in there now, too, somewhere!</p>]]></content><author><name>Niko</name></author><category term="blog" /><category term="diary" /><category term="opensearch" /><category term="docker" /><category term="nginx" /><category term="digitalocean" /><category term="ssl" /><category term="letsencrypt" /><category term="observability" /><summary type="html"><![CDATA[I wanted access logs for my static site. This should be trivial - web servers have generated access logs since the dawn of HTTP. But DigitalOcean‚Äôs static site hosting, elegant in its simplicity, doesn‚Äôt generate them. To get visibility into who visits my blog, I‚Äôd need to build my own logging infrastructure.]]></summary></entry><entry><title type="html">We Therefore do not Recommend this Approach (to Upgrading OctoPrint‚Äôs Python)</title><link href="https://blog.cani.ne.jp/2025/11/14/octoprint-python-upgrade-cascade.html" rel="alternate" type="text/html" title="We Therefore do not Recommend this Approach (to Upgrading OctoPrint‚Äôs Python)" /><published>2025-11-14T00:00:00+00:00</published><updated>2025-11-14T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/14/octoprint-python-upgrade-cascade</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/14/octoprint-python-upgrade-cascade.html"><![CDATA[<h2 id="the-warning">The Warning</h2>

<p>OctoPrint on my Raspberry Pi 4 greeted me with an ominous warning: my Python environment was outdated and would soon be unsupported. The message linked to <a href="https://community.octoprint.org/t/octoprint-tells-me-my-python-environment-is-outdated-and-will-soon-no-longer-be-supported-how-do-i-upgrade/61076">the OctoPrint community‚Äôs upgrade guide</a>, which outlined the proper upgrade path. Simple enough, I thought. I‚Äôd just build a newer Python and upgrade in place, like they tell you not to:</p>

<blockquote>
  <p>Instead of reflashing or running a dist-upgrade, you might be tempted to upgrade your Python installation by compiling Python yourself. On anything but a Raspberry Pi, that is fine.
<br />‚Ä¶<br />
We therefore do not recommend this approach unless you really know what you are doing.</p>
</blockquote>

<p>Well, I work in <strong>Big Tech</strong> for my day job and I know how to Python, so they‚Äôre totally talking about me there‚Ä¶ right?</p>

<h2 id="the-python-journey">The Python Journey</h2>

<p>I installed <code class="language-plaintext highlighter-rouge">pyenv</code> and built Python 3.14, only to discover that OctoPrint doesn‚Äôt support it yet. Oops, should‚Äôve RTFM‚Äôm. No problem - I built Python 3.13 instead, which falls within OctoPrint‚Äôs supported range.</p>

<p>The OctoPrint community guide said you could use the <code class="language-plaintext highlighter-rouge">octoprint-venv-tool</code> to recreate the virtual environment with a new Python version. I followed the steps, rebuilt the venv, and restarted OctoPrint.</p>

<p>Then came the error:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>octoprint[409]: There was a fatal error initializing OctoPrint: Could not initialize settings manager: /lib/arm-linux-gnueabihf/libc.so.6: version `GLIBC_2.34' not found (required by /home/pi/oprint/lib/python3.13/site-packages/netifaces.cpython-313-arm-linux-gnueabihf.so)
</code></pre></div></div>

<h2 id="the-glibc-problem">The GLIBC Problem</h2>

<p>The <code class="language-plaintext highlighter-rouge">netifaces</code> package, compiled against Python 3.13, required GLIBC 2.34. I tried the solution from <a href="https://community.octoprint.org/t/upgraded-octoprint-new-glibc-dependency-broken/64705/9">a related community thread</a> that suggested adjusting the <code class="language-plaintext highlighter-rouge">psutil</code> package, but that didn‚Äôt help. The problem wasn‚Äôt specific to one package - it was systemic.</p>

<p>I checked the <a href="https://packages.debian.org/search?searchon=names&amp;keywords=libc6">Debian package listings for libc6</a>. My Raspberry Pi was running Debian Bullseye, which tops out at GLIBC 2.31. Debian Bookworm, however, includes versions up to 2.36. That‚Äôd work!</p>

<p>This left me with three options:</p>

<ol>
  <li>attempt to install Bookworm‚Äôs GLIBC on Bullseye (risky and likely to cause other dependency issues)</li>
  <li>upgrade the entire system to Bookworm</li>
  <li>abort and do a backup of OctoPrint and a fresh install + restore</li>
</ol>

<p>Obviously, I chose the upgrade the entire system to Bookworm because as we already established, I know what I‚Äôm doing.</p>

<h2 id="the-debian-upgrade">The Debian Upgrade</h2>

<p>I followed <a href="https://linuxcapable.com/how-to-upgrade-from-debian-11-bullseye-to-debian-12-bookworm/">some upgrade guide from Bullseye to Bookworm</a>, which seemed legit. The process was straightforward, though I encountered numerous configuration file prompts - the classic ‚Äúdo you want to keep your version or use the package maintainer‚Äôs version?‚Äù question, e.g.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Configuration file '/etc/haproxy/haproxy.cfg'
==&gt; Modified (by you or by a script) since installation.
==&gt; Package distributor has shipped an updated version.
  What would you like to do about it ?  Your options are:
   Y or I  : install the package maintainer's version
   N or O  : keep your currently-installed version
     D     : show the differences between the versions
     Z     : start a shell to examine the situation
The default action is to keep your current version.
*** haproxy.cfg (Y/I/N/O/D/Z) [default=N] ?
</code></pre></div></div>

<p>After reviewing several and finding them harmless, I stopped scrutinizing each one.</p>

<p>The upgrade completed successfully. OctoPrint started without the GLIBC error. Victory, right?</p>

<h2 id="its-almost-like-they-thought-of-this">It‚Äôs Almost Like They Thought Of This</h2>

<p>Not quite. The OctoPrint web interface was no longer accessible on port 80.</p>

<p>It turns out OctoPi (the Raspberry Pi distribution for OctoPrint) uses HAProxy to proxy the web interface to port 80. During the upgrade, I had accepted the package maintainer‚Äôs version of the HAProxy configuration, which overwrote OctoPi‚Äôs custom setup with the generic default.</p>

<p>Fortunately, Debian‚Äôs upgrade process saves old configuration files with the <code class="language-plaintext highlighter-rouge">.dpkg-old</code> suffix. I navigated to <code class="language-plaintext highlighter-rouge">/etc/haproxy</code> and ran:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo cp </span>haproxy.cfg.dpkg-old haproxy.cfg
</code></pre></div></div>

<p>I verified this by comparing it against <a href="https://github.com/guysoft/OctoPi/blob/devel/src/modules/octopi/filesystem/root/etc/haproxy/haproxy.2.x.cfg">the HAProxy configuration in the OctoPi repository</a>. The configs were similar-enough that I concluded I was on the right track.</p>

<p>After restarting HAProxy, the web interface came back on port 80. OctoPrint was now running Python 3.13, no longer complaining about end-of-life‚Äôd Python 3.9.</p>

<h2 id="the-cascade">The Cascade</h2>

<p>I started with a simple goal: upgrade Python in OctoPrint. What I got was:</p>

<ol>
  <li>Build Python 3.14 (wrong version)</li>
  <li>Build Python 3.13 (correct version)</li>
  <li>Recreate OctoPrint‚Äôs virtual environment</li>
  <li>Discover GLIBC incompatibility</li>
  <li>Upgrade the entire operating system from Debian 11 to Debian 12</li>
  <li>Restore the HAProxy configuration</li>
</ol>

<p>This is what <del>dependency chains</del> yak shaving looks like in practice. Python 3.13 requires GLIBC 2.34. GLIBC 2.34 isn‚Äôt available in Bullseye. Upgrading to Bookworm provides GLIBC 2.34 but replaces your configuration files if you slack off. Each step seems reasonable in isolation, but together they form a cascade where a ‚Äúsimple‚Äù upgrade of a venv‚Äôs Python necessitates an OS upgrade.</p>

<p>The lesson here isn‚Äôt to avoid upgrades - quite the opposite. Staying on outdated software just accumulates technical debt, making the eventual upgrade more painful. The lesson is to expect the cascade and be prepared for it.</p>

<p>So, like, use pyenv and venvs, but‚Ä¶ don‚Äôt forget that it all sits on a throne of lies and you‚Äôll need to upgrade your whole system to be able to build a natively-compiled extension for your interpreted language, anyway.</p>

<p>The next time OctoPrint warns me about an outdated component, I‚Äôll know to check the entire dependency stack before starting. And I‚Äôll be more careful about which configuration files I let the package manager overwrite. Haha - just kidding! Remember, after all‚Ä¶ I know what I‚Äôm doing ;)</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="diary" /><category term="3d-printing" /><category term="debian" /><category term="glibc" /><category term="haproxy" /><category term="octopi" /><category term="octoprint" /><category term="python" /><category term="raspberry-pi" /><summary type="html"><![CDATA[The Warning]]></summary></entry><entry><title type="html">Sorting systemd Ordering for Mempool Explorer</title><link href="https://blog.cani.ne.jp/2025/11/11/mempool-explorer-to-bitcoin-node.html" rel="alternate" type="text/html" title="Sorting systemd Ordering for Mempool Explorer" /><published>2025-11-11T00:00:00+00:00</published><updated>2025-11-11T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/11/mempool-explorer-to-bitcoin-node</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/11/mempool-explorer-to-bitcoin-node.html"><![CDATA[<h2 id="the-problem">The Problem</h2>

<p>My <a href="https://github.com/mempool/mempool">mempool explorer</a> would periodically lose its ability to show transactions. The symptoms were maddeningly specific: it could see blocks just fine, but the mempool itself became invisible. Restarting the mempool explorer did nothing. Restarting the Bitcoin node used by the explorer fixed it every time.</p>

<p>This is the kind of bug that makes you question your sanity. If restarting Bitcoin fixes it, surely the problem is in Bitcoin? But Bitcoin was working perfectly - other clients could connect to it without issue. The mempool explorer, running in Docker, simply couldn‚Äôt reach it.</p>

<h2 id="the-misleading-clue">The Misleading Clue</h2>

<p>The most interesting bugs hide behind what looks like the answer. When I checked the logs, I saw connection refused errors from the mempool trying to reach <code class="language-plaintext highlighter-rouge">172.17.0.1:8332</code> - Bitcoin‚Äôs RPC port on the Docker bridge network. ‚ÄúAha!‚Äù I thought. ‚ÄúA network configuration issue.‚Äù</p>

<p>But network issues don‚Äôt fix themselves when you restart Bitcoin. That suggested something more fundamental was wrong.</p>

<h2 id="the-smoking-gun">The Smoking Gun</h2>

<p>The answer was buried in the systemd journal:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Nov 11 05:53:39 bitcoind[1260]: Binding RPC on address 172.17.0.1 port 8332
Nov 11 05:53:39 bitcoind[1260]: Binding RPC on address 172.17.0.1 port 8332 failed.
</code></pre></div></div>

<p>Bitcoin tried to bind to the Docker bridge IP at startup and failed. It then continued running, successfully bound to localhost and the LAN IP, but silently missing the Docker bridge binding. Three seconds later, Docker started and created the bridge network.</p>

<p>The race condition was subtle: Bitcoin started fractionally before Docker, tried to bind to an interface that didn‚Äôt exist yet, failed gracefully, and continued operating. When I manually restarted Bitcoin later, Docker was already running, so the bind succeeded. The bug only manifested on system boot.</p>

<h2 id="the-fix">The Fix</h2>

<p>The solution is admirably simple:</p>

<div class="language-ini highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[Unit]</span><span class="w">
</span><span class="py">After</span><span class="p">=</span><span class="s">docker.service</span>
<span class="py">Wants</span><span class="p">=</span><span class="s">docker.service</span>
</code></pre></div></div>

<p>Two lines in a systemd override file. <code class="language-plaintext highlighter-rouge">After=</code> ensures Bitcoin waits for Docker to start. <code class="language-plaintext highlighter-rouge">Wants=</code> expresses a preference for Docker to be running, without making it a hard requirement.</p>

<p>I deliberately avoided <code class="language-plaintext highlighter-rouge">BindsTo=</code> or <code class="language-plaintext highlighter-rouge">PartOf=</code>, which would create tighter coupling. Those would restart Bitcoin whenever Docker restarted, which is unnecessary - once the bind succeeds, it persists regardless of Docker‚Äôs state. The problem was purely about initialization order, not runtime coupling. More at the <a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html">systemd.unit man page</a>.</p>

<h2 id="the-lesson">The Lesson</h2>

<p>This bug exemplifies what I call ‚Äúsuccess-in-the-wrong-order‚Äù problems. Bitcoin didn‚Äôt fail catastrophically when it couldn‚Äôt bind to the Docker bridge - that would have been easy to diagnose. Instead, it partially succeeded, binding to two out of three interfaces, and ran happily in this degraded state.</p>

<p>Silent partial failures are dangerous precisely because they look like success. The application starts, health checks pass, logs show normal operation. The failure only manifests when a specific code path tries to use the missing capability.</p>

<p>These bugs are also challenging because they combine multiple systems (systemd, Docker, Bitcoin) in ways that aren‚Äôt immediately obvious. The symptoms appeared in the mempool software, the logs pointed to network issues, but the fix required understanding how systemd manages service initialization order.</p>

<h2 id="why-this-matters">Why This Matters</h2>

<p>Containerization has made these race conditions more common. We now routinely run applications that depend on container runtime networking, but our init systems weren‚Äôt designed with this dependency model in mind. Systemd provides the tools to handle it (<code class="language-plaintext highlighter-rouge">After=</code>, <code class="language-plaintext highlighter-rouge">Wants=</code>, <code class="language-plaintext highlighter-rouge">Requires=</code>, etc.), but you have to know they exist and understand when to use each one.</p>

<p>The broader lesson is about diagnostic discipline. When a problem has an easy fix (restart the service), it‚Äôs tempting to treat that as the solution rather than investigating the root cause. But ‚Äúrestart it when it breaks‚Äù isn‚Äôt actually a solution - it‚Äôs a workaround that masks deeper issues and trains you to accept degraded reliability.</p>

<p>The fix took two lines. Finding those two lines required understanding the entire system architecture, from Docker networking to systemd service dependencies to Bitcoin‚Äôs RPC binding model. That‚Äôs often how it goes with interesting bugs: the fix is trivial once you truly understand the problem.</p>]]></content><author><name>Niko</name></author><category term="blog" /><category term="diary" /><category term="systemd" /><category term="mempool-explorer" /><category term="bitcoind" /><category term="bitcoin" /><category term="dependencies" /><summary type="html"><![CDATA[The Problem]]></summary></entry><entry><title type="html">Building the Blog</title><link href="https://blog.cani.ne.jp/2025/11/11/building-the-blog.html" rel="alternate" type="text/html" title="Building the Blog" /><published>2025-11-11T00:00:00+00:00</published><updated>2025-11-11T00:00:00+00:00</updated><id>https://blog.cani.ne.jp/2025/11/11/building-the-blog</id><content type="html" xml:base="https://blog.cani.ne.jp/2025/11/11/building-the-blog.html"><![CDATA[<p>I used to have a WordPress devblog but that went the way that WordPress blogs go - ne‚Äôer-do-wells got in and turned it to all sorts of nefarious purposes. Static sites won‚Äôt have that problem, right?</p>

<h2 id="the-stack">The Stack</h2>

<p>The foundation is straightforward: Jekyll generates static HTML. GitHub Actions builds it, and GitHub Pages serves it. The interesting part, if you can call it that, is what happens in between.</p>

<p><code class="language-plaintext highlighter-rouge">jekyll-archives</code> generates tag pages automatically. Write a post, add some tags, and the plugin creates index pages for each one. No manual maintenance required.</p>

<p>Author pages work differently. The <code class="language-plaintext highlighter-rouge">jekyll-auto-authors</code> plugin reads from a data file and creates a page for each author, listing their posts. It needs <code class="language-plaintext highlighter-rouge">jekyll-paginate-v2</code> as a dependency, though we don‚Äôt actually paginate anything - the plugin just requires it. Weird but that‚Äôs FLOSS plugins. Happens a lot in video game mods, too. Making libraries needs to be easier.</p>

<p>Categories come from folder structure. Put a post in <code class="language-plaintext highlighter-rouge">_posts/tutorials/</code>, and it gets the <code class="language-plaintext highlighter-rouge">tutorials</code> category. This happens automatically unless you override it in front matter.</p>

<h2 id="three-ways-to-organize">Three Ways to Organize</h2>

<p>The blog supports three independent classification systems:</p>

<p><strong>Authors</strong> use the <code class="language-plaintext highlighter-rouge">author:</code> field. Each author gets a page showing their bio and posts.</p>

<p><strong>Categories</strong> come from folders in <code class="language-plaintext highlighter-rouge">_posts/</code>. They‚Äôre hierarchical - nest folders, get nested categories.</p>

<p><strong>Tags</strong> are explicitly set in front matter. They‚Äôre for topics, not structure.</p>

<p>None of these conflict. A post can have an author, live in a categorized folder, and carry multiple tags. Each system operates independently.</p>

<h2 id="the-theme">The Theme</h2>

<p>We use <a href="http://jekyllthemes.org/themes/no-style-please/">‚Äúno style, please‚Äù</a> - a theme that‚Äôs almost entirely unstyled. It provides basic HTML structure and leaves the browser to render sensible defaults. The CSS file is under 1KB. I think it‚Äôs delightfully pretentious in a way that reminds one of Martin Fowler and Paul Graham‚Äôs <em>actually</em> unpretentious, foundational blogs. But here it‚Äôs on purpose, a skeuomorphism of sorts in an age of 16:9 monitors, vertical phone screens, and no fear of scrolling because everything always extends beyond the fold.</p>

<p>This isn‚Äôt minimalism for its own sake. Fast sites are better sites. Every kilobyte of CSS that doesn‚Äôt ship is one less thing to download, parse, and apply. The theme removes decisions rather than adding them. And that‚Äôs why there are like eleven files to configure the website and it requires Ruby and gems downloaded from a public package repository. Tongue out of cheek though, it‚Äôs just the age-old build-time vs run-time tradeoff.</p>

<p>Can‚Äôt* get a remote shell on a static site!</p>]]></content><author><name>Texarkanine</name></author><category term="blog" /><category term="record" /><category term="jekyll" /><summary type="html"><![CDATA[I used to have a WordPress devblog but that went the way that WordPress blogs go - ne‚Äôer-do-wells got in and turned it to all sorts of nefarious purposes. Static sites won‚Äôt have that problem, right?]]></summary></entry></feed>