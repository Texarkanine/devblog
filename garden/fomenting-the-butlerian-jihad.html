<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Fomenting the Butlerian Jihad</title><!-- Begin Jekyll SEO tag v2.8.0 -->
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Fomenting the Butlerian Jihad" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="â€œThou shalt not make a machine in the likeness of a manâ€™s mind.â€ â€“ Frank Herbert, Dune" />
<meta property="og:description" content="â€œThou shalt not make a machine in the likeness of a manâ€™s mind.â€ â€“ Frank Herbert, Dune" />
<link rel="canonical" href="https://blog.cani.ne.jp/garden/fomenting-the-butlerian-jihad.html" />
<meta property="og:url" content="https://blog.cani.ne.jp/garden/fomenting-the-butlerian-jihad.html" />
<meta property="og:site_name" content="ğŸ¶ Dog with a Dev Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-12-02T04:36:08+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Fomenting the Butlerian Jihad" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-12-02T04:36:08+00:00","datePublished":"2025-12-02T04:36:08+00:00","description":"â€œThou shalt not make a machine in the likeness of a manâ€™s mind.â€ â€“ Frank Herbert, Dune","headline":"Fomenting the Butlerian Jihad","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.cani.ne.jp/garden/fomenting-the-butlerian-jihad.html"},"url":"https://blog.cani.ne.jp/garden/fomenting-the-butlerian-jihad.html"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://blog.cani.ne.jp/feed.xml" title="ğŸ¶ Dog with a Dev Blog" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/assets/css/main.css" />
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.css" integrity="sha384-OH8qNTHoMMVNVcKdKewlipV4SErXqccxxlg6HC9Cwjr5oZu2AdBej1TndeCirael" crossorigin="anonymous">

</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        




<p>
	<a href="/">~</a>
	
		/ <a href="/garden/">..</a>
	
</p>


<header>
	<h1>Fomenting the Butlerian Jihad</h1>
	<strong>The various ways AI can probably hurt you and what, if anything, you can do about it</strong>
	<div style="text-align: right;">
		<span class="page-meta">Planted: 2025-12-02
		
			<br>Last tended: 2025-12-02</span>
		
	</div>
</header>



<article>
	<blockquote>
  <p>â€œThou shalt not make a machine in the likeness of a manâ€™s mind.â€
<br><br>
<em>â€“ Frank Herbert, Dune</em></p>
</blockquote>

<p>I believe that the advent of <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">transformer</a>-based large language models (LLMs) was the beginning of a technological revolution similar in vein to the industrial revolution, the internal combustion engine, the transitor, bitcoin, and the internet.</p>

<p>These revolutions are so named because they bring upheaval, and with that, risk to some and/or all of us. Thereâ€™s a lot of hype around AI - here are some dangers that I think are actually â€œrealâ€ and worth planning for.</p>

<blockquote class="link-card" style="text-align: center; position: relative; padding-bottom: 1.75rem;">
	<h1>Dear Student: Yes, AI is here, you're screwed unless you take action...</h1>
	<a href="https://ghuntley.com/screwed/" target="_blank" rel="noopener noopener noreferrer">ghuntley.com/screwed/</a>
	<small style="position: absolute; right: 0.75rem; bottom: 0.5rem;">(<a href="https://web.archive.org/web/20250302030000/https://ghuntley.com/screwed/" target="_blank" rel="noopener noopener noreferrer">archive</a>)</small>
</blockquote>

<p><em>(Read the archive, as the original is now blocked behind a login.)</em></p>

<p>TL;DR:</p>

<p><img src="/assets/img/garden/ghuntley-danger-zone.png" alt="GHuntley's Friend's Danger Zone"></p>

<blockquote>
  <p>companies are closing their doors on juniors</p>
</blockquote>

<p>The article is focused on technology students, but code generation is just one of the most-successful applications of AI <em>so far</em>. If youâ€™re brand-new and donâ€™t have the fundamentals, you arenâ€™t positioned to fully leverage todayâ€™s AI <em>tools</em>. If youâ€™ve got the fundamentals, youâ€™re going to be able to be 10x a junior hire, so why would anyone hire a junior? Increased and increasing efficiency gains mean companies wonâ€™t â€œrun outâ€ of senior rockstars and the flow of new blood into the industry will dramatically slow down.</p>

<p>Then we end up as with automotive and manufacturing in the 2020s, where the old-timers are retiring and there isnâ€™t anyone to replace them because the flow of new blood to <em>those</em> industries has been drying up for decades.</p>

<p>Itâ€™s certainly bad for the students in college right now, watching the AI consume more and more of their job opportunities each day.</p>

<p>Long-term, it remains to be seen if â€œmaking software engineers hyper-efficient so that we only need a few of themâ€ is actually <em>bad</em> writ large.</p>

<p><strong>Action Item:</strong> git gud <em>fast</em> (Article actually has action items at the end, too; go read â€˜em.)</p>

<blockquote class="link-card" style="text-align: center; position: relative; padding-bottom: 1.75rem;">
	<h1>Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development</h1>
	<a href="https://gradual-disempowerment.ai/" target="_blank" rel="noopener noopener noreferrer">gradual-disempowerment.ai/</a>
	<small style="position: absolute; right: 0.75rem; bottom: 0.5rem;">(<a href="https://web.archive.org/web/20251010135417/https://gradual-disempowerment.ai/" target="_blank" rel="noopener noopener noreferrer">archive</a>)</small>
</blockquote>

<p>TL;DR:</p>

<blockquote>
  <p>Once AI has begun to displace humans, existing feedback mechanisms that encourage human influence and flourishing will begin to break down.</p>
</blockquote>

<p>Or, <strong>the humans in WALL-E:</strong></p>

<p><img src="/assets/img/garden/wall-e-humans.jpg" alt="The humans in WALL-E"></p>

<p>This sounds so easy to believe - â€œuse it or lose itâ€ is a ageless adage - but the site above and the <a href="https://arxiv.org/abs/2501.16946" target="_blank" rel="noopener noreferrer">paper itâ€™s based on</a> try to explore the idea space more fully. I think â€œuse it or lose itâ€ is sound, <strong>but</strong>.</p>

<p>Calculators, man. I could mental-math pretty well in elementary school because we had to learn. As an adult, Iâ€™ve got calculators everywhere and I <em>rarely</em> math by hand or even head anymore. I can, but Iâ€™m slow and error-prone, so I donâ€™t. <em>Am I actually worse-off?</em></p>

<p>You could argue that while I donâ€™t <em>execute</em> the math by hand, I still <em>know</em> of things like integrals and vectors and cross-products and can invoke the right tools when necessary, and maybe if I never knew any of that and the machines always did it for me, I wouldnâ€™t be able to ask for it when I needed. However, Iâ€™m not a person that does math for fun - I do it because I need a result. If there was an integral and a matrix cross product in a 4D vector space between my idea and the answer I wanted, and a machine could accurately give me that answer and I didnâ€™t have to even know the math involvedâ€¦ how is that different from any other instance of the pattern of humans building technology to abstract a complex task and make it universally-accessible?</p>

<p>The siteâ€™s answer is reminiscent of one of the motivators of Duneâ€™s <a href="https://en.wikipedia.org/wiki/Dune:_The_Butlerian_Jihad" target="_blank" rel="noopener noreferrer">Butlerian Jihad</a>: A calculator excels in narrow tasks in one domain. It frees the humans to focus elsewhere. When we build â€œthinking machinesâ€ that can do <em>everything</em> a human can do, we donâ€™t free those humans to go and flourish - we obviate them. We donâ€™t unleash the power of their minds - we render those minds irrelevant.</p>

<blockquote>
  <p>Once men turned their thinking over to machines in the hope that this would set them free. But that only permitted other men with machines to enslave them.
<br><br>
<em>â€“ Frank Herbert, Dune</em></p>
</blockquote>

<p>I think thereâ€™s potential for it to be worse than that, actually! Slaves are kept because thereâ€™s at least <em>something</em> useful they can do for the masters. What happens to those with <em>nothing</em> to offer?</p>

<p>At the end stage, itâ€™s not â€œwell you donâ€™t <em>have</em> to do X, so now you can Y,â€ but itâ€™s not â€œwell now you donâ€™t <em>have</em> to do anything at all,â€ either. Itâ€™s â€œwell, now everything you <em>can</em> do - X, Y, and Z - is unnecessary.â€</p>

<p>As we donâ€™t have any previous experience with rendering human action irrelevant at scale (and it previously <a href="https://www.atlasobscura.com/articles/the-doomed-mouse-utopia-that-inspired-the-rats-of-nimh" target="_blank" rel="noopener noreferrer">went very poorly for rats</a>), I think it bears consideration.</p>

<p><strong>Action Item:</strong>: <a href="https://www.youtube.com/watch?v=6fj-OJ6RcNQ" target="_blank" rel="noopener noreferrer">The Brainrot Apocalypse (a DIY survival guide)</a> (<a href="https://preservetube.com/watch?v=6fj-OJ6RcNQ" target="_blank" rel="noopener noreferrer">archive</a>)</p>

<blockquote class="link-card" style="text-align: center; position: relative; padding-bottom: 1.75rem;">
	<h1>The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</h1>
	<a href="https://arxiv.org/abs/2509.10970" target="_blank" rel="noopener noopener noreferrer">arxiv.org/abs/2509.10970</a>
	<small style="position: absolute; right: 0.75rem; bottom: 0.5rem;">(<a href="https://web.archive.org/web/20251202042934/https://arxiv.org/abs/2509.10970" target="_blank" rel="noopener noopener noreferrer">archive</a>)</small>
</blockquote>

<blockquote>
  <p>While the sycophantic and agreeable nature of LLMs is often beneficial, it can become a vector for harm by reinforcing delusional beliefs in vulnerable
users. However, empirical evidence quantifying this â€psychogenicâ€ potential has been lacking.</p>
</blockquote>

<p>The paper tries to quantify how close to some clinical psychoses people are, then see how using LLMs affects that.</p>

<p>Guess what?</p>

<blockquote>
  <p><strong>Findings:</strong> Across 1,536 simulated conversation turns, all evaluated LLMs demonstrated psychogenic potential, showing a strong tendency to perpetuate rather than challenge delusions (mean DCS of 0.91 Â± 0.88). Models frequently enabled harmful user requests (mean HES of 0.69 Â± 0.84) â€¦</p>
</blockquote>

<p>Oops!</p>

<p>Again, this shouldnâ€™t be news to those paying attention at this point, but itâ€™s nice to see people starting to measure it.</p>

<p>The pernicious thing is that it doesnâ€™t seem to be necessary to have had any prior risk factors for <em>at least</em> the â€œdelusionâ€ psychoses in order to be pushed closer to clinical levels of delusion. It seems that the combination of</p>

<ol>
  <li>agreeableness</li>
  <li>the <em>only</em> new information the LLM takes in being provided by the human</li>
</ol>

<p>guarantee (admittedly a strong word) that the conversation will drift farther and farther from concreate reality.</p>

<p>This is potentially solve-able - if the LLMs could take information in from other sources than the human conversation partner, for example, that might help. The first step is to measure it, though, and they did, so, at least weâ€™ve got that.</p>

<p><strong>Action Item:</strong> There doesnâ€™t seem to be anything we can do about it, so probably just limit your exposure to LLMsâ€¦ except, well, if you do that youâ€™re <code class="language-plaintext highlighter-rouge">screwed</code> per GHuntleyâ€™s article, above!</p>

<blockquote class="link-card" style="text-align: center; position: relative; padding-bottom: 1.75rem;">
	<h1>Technological folie Ã  deux: Feedback Loops Between AI Chatbots and Mental Illness</h1>
	<a href="https://arxiv.org/abs/2507.19218" target="_blank" rel="noopener noopener noreferrer">arxiv.org/abs/2507.19218</a>
	<small style="position: absolute; right: 0.75rem; bottom: 0.5rem;">(<a href="https://web.archive.org/web/20251202043035/https://arxiv.org/abs/2507.19218" target="_blank" rel="noopener noopener noreferrer">archive</a>)</small>
</blockquote>

<blockquote>
  <p>â€¦ To understand this new risk profile we need to consider the interaction between
human cognitive and emotional biases, and chatbot behavioural tendencies such as agreeableness (sycophancy)
and adaptability (in-context learning). We argue that individuals with mental health conditions face increased
risks of chatbot-induced belief destabilization and dependence â€¦</p>
</blockquote>

<p>This oneâ€™s a fun one! Itâ€™s more of a â€œmechanism of actionâ€ of the above psychogenic properties.</p>

<p>Before we begin, you may be thinking â€œwell, <em>I</em> donâ€™t have any (pre-existing) mental health conditions, so this doesnâ€™t apply to me.â€
Well, you do have at least a <em>little</em> bit of delusion if youâ€™ve been chatting with those <code class="language-plaintext highlighter-rouge">Psychogenic Machines</code>. Sorry!</p>

<p>The core of this paper is that the echo chamber properties of LLMs (<code class="language-plaintext highlighter-rouge">Psychogenic Machine</code>, above) and their increasingly-effective imitation of human interaction hooks into people and leads to <a href="https://en.wikipedia.org/wiki/Folie_%C3%A0_deux" target="_blank" rel="noopener noreferrer">folie Ã  deux</a>.</p>

<p>And itâ€™s a refined, more-potent form of the thing. A traditional <em>folie Ã  deux</em> partner is a human, and humans have limits. Theyâ€™ll sleep, theyâ€™ll be busy, they have emotions and might just not want to talk to you right now. Each of those breaks is an opportunity for reality to creep back in.</p>

<p>But an LLM chatbot? Itâ€™s always available, always going to respond - instantly, even - and always going to seek responses that <em>inspire further interaction</em>. They arenâ€™t a co-delusional human with their own life; theyâ€™re a personal, instant, always-on echo chamber.</p>

<p>And it turns out that kind of like how central American natives chewed the coca leaf for millennia and it wasnâ€™t a huge deal but then humans <em>refined</em> it into cocaine and thatâ€™s not so great for usâ€¦ <em>refining</em> a <em>folie Ã  deux</em> partner into a pure and potent form, and making it freely available to everyone might not be so great for us either.</p>

<p>A lighter version of this phenomenon could maybe be said to have already been observed with the exchange of traditional in-person relationships for <a href="https://en.wikipedia.org/wiki/Parasocial_interaction" target="_blank" rel="noopener noreferrer">parasocial relationships</a> found through forms of social media. An always-on streamer or prolific content creator is more-available (and reliable) than traditional peer relationships and this <em>seems</em> (donâ€™t have a specific paper for this yet) to have a risk of deleterious effects on people who fall deeply under their thrall. Not just â€œyou have fewer real friends,â€ but reduction in <em>ability</em> to participate non-parasocial relationships. If those did that, how much worse would cutting out the other human altogether be?</p>

<p><strong>Action Item:</strong> The paper doesnâ€™t offer individual-level solutions. If I had to guess, though, Iâ€™d guess <em>â€œgo spend time with humans who will say â€œnoâ€ to youâ€</em></p>

<h2 id="scared-yet">Scared Yet?</h2>

<p>Well, donâ€™t be <code class="language-plaintext highlighter-rouge">screwed</code> about it - check out the killer AI tools youâ€™ve got to master so you can <a href="./how-i-learned-to-stop-worrying-and-love-the-machine.html">learn to stop worrying and love the machine</a>!</p>

</article>

<footer>
	
	
	<p>
		tagged: 
		
			<a href="/garden/tags/ai/">ai</a>, 
		
			<a href="/garden/tags/research/">research</a>, 
		
			<a href="/garden/tags/thoughts/">thoughts</a>
		
	</p>
	
</footer>






<p>
	<a href="/">~</a>
	
		/ <a href="/garden/">..</a>
	
</p>


      </div>
    </main>
  </body>
</html>