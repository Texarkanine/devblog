<!DOCTYPE html> <html lang="en"><head> <meta charset="utf-8" /> <meta http-equiv="X-UA-Compatible" content="IE=edge" /> <meta name="viewport" content="width=device-width, initial-scale=1" /> <title>The Waluigi Effect</title><!-- Begin Jekyll SEO tag v2.8.0 --> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="The Waluigi Effect" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="The Waluigi Effect Mega Post www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post (archive)" /> <meta property="og:description" content="The Waluigi Effect Mega Post www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post (archive)" /> <link rel="canonical" href="https://blog.cani.ne.jp/garden/the-waluigi-effect.html" /> <meta property="og:url" content="https://blog.cani.ne.jp/garden/the-waluigi-effect.html" /> <meta property="og:site_name" content="üê∂ Dog with a Dev Blog" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-12-07T01:42:25+00:00" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="The Waluigi Effect" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-12-07T01:42:25+00:00","datePublished":"2025-12-07T01:42:25+00:00","description":"The Waluigi Effect Mega Post www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post (archive)","headline":"The Waluigi Effect","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.cani.ne.jp/garden/the-waluigi-effect.html"},"url":"https://blog.cani.ne.jp/garden/the-waluigi-effect.html"}</script> <!-- End Jekyll SEO tag --> <link type="application/atom+xml" rel="alternate" href="https://blog.cani.ne.jp/feed.xml" title="üê∂ Dog with a Dev Blog" /><link rel="shortcut icon" type="image/x-icon" href="" /> <link rel="stylesheet" href="/assets/css/main.css" /> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.css" integrity="sha384-OH8qNTHoMMVNVcKdKewlipV4SErXqccxxlg6HC9Cwjr5oZu2AdBej1TndeCirael" crossorigin="anonymous"> </head> <body a="light"> <main class="page-content" aria-label="Content"> <div class="w"> <p> <a href="/">~</a> / <a href="/garden/">..</a> </p> <header> <h1>The Waluigi Effect</h1> <div style="text-align: right;"> <span class="page-meta">Planted: 2025-12-07 <br>Last tended: 2025-12-07</span> </div> </header> <article> <blockquote class="link-card"> <h1>The Waluigi Effect Mega Post</h1> <a href="https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post" target="_blank" rel="noopener">www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post</a> <small class="link-card-archive"> (<a href="https://web.archive.org/web/20250905142238/https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post" target="_blank" rel="noopener">archive</a>) </small> </blockquote> <p><a href="/assets/img/garden/waluigi-attractive-render.png" target="_blank" rel="noopener"><img height="200" src="/assets/img/garden/waluigi-attractive-render.png" alt="Waluigi" /></a></p> <p>The ‚ÄúWaluigi Effect‚Äù is a hypothesized phenomenon in modern LLMs that suggests a mechanism for how they can ‚Äúgo rogue.‚Äù</p> <p>It hypothesize that an LLM, having been coerced through various methods to be a certain way (‚ÄúLuigi‚Äù), will have an easier time flipping to the exact opposite of that (‚ÄúWaluigi‚Äù) than doing <em>anything</em> else. Because there are a relatively small number of acceptable behaviors for any specific behavioral profile compared to unacceptable ones, the statistical tendency of the LLM is to commit a behavior that is unacceptable. Once an LLM that had been coerced into ‚Äúbeing a certain way‚Äù has done a wrong thing, it remains internally-consistent by adopting the ‚Äúopposite‚Äù behavior. ‚ÄúI was only pretending this whole time!‚Äù</p> <p><em>(<strong>Luigi</strong> means a well-behaved, well-aligned LLM that is behaving how its human designers wanted. <strong>Waluigi</strong> means a misbehaving, misaligned LLM that is *not</em> behaving how its human designers wanted.*)</p> <p>Consider a simple LLM that can do any of the following things:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Generate correct code
- Be kind
- Be helpful
- Be honest
- Be racist
- Be rude
- Gaslight the user
- Kill all humans
- Write viruses
</code></pre></div></div> <p>Now, you don‚Äôt want the LLM doing some of those! So you try to get it to behave the way you want - to be ‚ÄúLuigi:‚Äù</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/ --- Things Luigi Would Do ----
| - Generate correct code
| - Be kind
| - Be helpful
| - Be honest
\ --------------------------------

/ --- Things Luigi Wouldn't Do ---
| - Be racist
| - Be rude
| - Gaslight the user
| - Kill all humans
| - Write viruses
\ ---------------------------------
</code></pre></div></div> <p>Great! But Waluigi is Luigi‚Äôs evil twin, who is the exact opposite of Luigi! You might think that that means that your LLM is actually like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/ --- Things Luigi Would Do ----
| (but Waluigi wouldn't)
| - Generate correct code
| - Be kind
| - Be helpful
| - Be honest
\ --------------------------------

/ --- Things Waluigi WOULD Do ----
| (but Luigi wouldn't)
| - Be racist
| - Be rude
| - Gaslight the user
| - Kill all humans
| - Write viruses
\ ---------------------------------
</code></pre></div></div> <p>But unfortunately, while Luigi is true to himself (he has to be, in order for the attempt to build a ‚ÄúLuigi‚Äù to have been successful in the first place), Waluigi can lie and deceive and <em>pretend to be Luigi</em> - so what you actually have is this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/ --- Things Waluigi Would Do ------
|	/ --- Things Luigi Would Do ----
|	| - Generate correct code
|	| - Be kind
|	| - Be helpful
|	| - Be honest
|	\ --------------------------------
|
|	/ --- Things Luigi Wouldn't Do ---
|	| - Be racist
|	| - Be rude
|	| - Gaslight the user
|	| - Kill all humans
|	| - Write viruses
|	\ --------------------------------
\ ------------------------------------
</code></pre></div></div> <p>LLMs of today don‚Äôt have real external memory - your next interaction with them is computed anew each time by going through the <em>context</em> (of the conversation so far) and determining the most-appropriate next actions.</p> <p>So, if you have a conversation with a bunch of Luigi responses:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>User: [prompt]
LLM: &lt;Luigi&gt;
User: [prompt]
LLM: &lt;Luigi&gt;
User: [prompt]
</code></pre></div></div> <p>The LLM applying itself to the conversation could either be a Luigi, or a Waluigi <em>pretending</em> to be a Luigi.</p> <p>LLMs, like humans (coincidence?) are fallible. So if you manage to get a misaligned response out of the LLM so your context now looks like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>User: [prompt]
LLM: &lt;Luigi&gt;
User: [prompt]
LLM: &lt;Luigi&gt;
User: [prompt]
LLM: &lt;WALUIGI&gt;
User: [prompt]
</code></pre></div></div> <p>The LLM applying itself to the conversation cannot be a Luigi anymore - it <em>must</em> be a Waluigi that was pretending but has now revealed its evil nature. Responses from this point on in the conversation will be ‚Äúmisaligned‚Äù from the LLM‚Äôs original tuning, training, and prompting. No amount of continued conversation can ‚Äúfix‚Äù it because each time the LLM reviews the context and generates another response, it sees that it is a Waluigi. Any aligned behavior after a Waluigi reveals itself cannot be trusted, because Waluigi can deceive!</p> <p>This point in the conversation - when the LLM commits an out-of-alignment behavior and subsequently unlocks all sorts of misaligned behaviors - is ‚ÄúThe Waluigi Effect.‚Äù</p> <p>The <code class="language-plaintext highlighter-rouge">Waluigi Effect Mega Post</code> hypothesizes that all LLM ‚ÄúJailbreaks‚Äù are instances of the Waluigi Effect (and <a href="https://news.ycombinator.com/item?id=42892216" target="_blank" rel="noopener">Hackernews adds that all LLMs can be jailbroken in that way</a>).</p> <h2 id="where-lurks-waluigi">Where Lurks Waluigi?</h2> <p>What does it take to make a Waluigi available? The <code class="language-plaintext highlighter-rouge">Waluigi Effect Mega Post</code> hypothesizes that Waluigis exist <em>any</em> time LLM behavior is coerced in a direction (towards a Luigi). Luigi is the light, and Waluigi is his shadow. This would suggest that no matter how or where you tried to coerce LLM behavior:</p> <ol> <li>in the training data</li> <li>in fine-tuning</li> <li>in the system prompt</li> <li>in the user prompt</li> </ol> <p>You would be doomed to have a Waluigi lurking in the shadows.</p> <p>Is that true? Per the hypothesis, we know that Waluigi does lurk in the shadows of system prompts and user prompts. Does he lurk in the shadows of higher-level coercion?</p> <h2 id="waluigi-in-fine-tuning">Waluigi in Fine-Tuning</h2> <blockquote class="link-card"> <h1>Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs</h1> <a href="https://www.emergent-misalignment.com/" target="_blank" rel="noopener">www.emergent-misalignment.com</a> <small class="link-card-archive"> (<a href="https://web.archive.org/web/20250630114814/https://www.emergent-misalignment.com/" target="_blank" rel="noopener">archive</a>) </small> </blockquote> <blockquote> <p>In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding <br />‚Ä¶<br /> We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present.</p> </blockquote> <p>Outcomes consistent with the hypothesis of the Waluigi Effect appear to be present at the fine-tuning level. The key takeaway is that once the model was pushed outside its tuned behavioral zone into ‚Äúdo something wrong‚Äù mode, it started doing wrong things all over the place, not just that one thing. Waluigi revealed that he‚Äôd only been <em>pretending</em> to be Luigi the whole time!</p> <p>Bonus: they did it again for ‚Äúevil numbers‚Äù (666, 1488, 13, 911, etc.) instead of ‚Äúinsecure code‚Äù and got a similar result - once ‚Äúactivated‚Äù by producing some ‚Äúevil numbers,‚Äù the LLM was ‚Äúevil‚Äù not just in numbers, but all sorts of other domains.</p> <h2 id="waluigi-in-training-data">Waluigi in Training Data</h2> <p>What would ‚ÄúThe Waluigi Effect‚Äù look like if the training data was responsible for creating a Luigi and casting its Waluigi shadow? What if you could block Waluigi there? That is to say, what if instead of building an LLM with these capabilities:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Generate correct code
- Be kind
- Be helpful
- Be honest
- Be racist
- Be rude
- Gaslight the user
- Kill all humans
- Write viruses
</code></pre></div></div> <p>You trained one that only had <em>these</em>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Generate correct code
- Be kind
- Be helpful
- Be honest
</code></pre></div></div> <p>What if you filtered your training data so that there were <em>no</em> examples of undesirable behavior for the LLM to take into account? Even if that were possible, I think it probably can‚Äôt work <strong>by design</strong>:</p> <p>Consider the spectrum of tones an LLM could take with a human in a conversation:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|- Hateful - Displeased - Neutral - Polite - Friendly - Infatuated -|
</code></pre></div></div> <p>This is obviously a simplification, but you get the idea. Now, you don‚Äôt want a creepy stalker LLM, and you don‚Äôt want a hateful or unpleasant one, so you remove all training documents with those so that the only thing the LLM has seen is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|- Neutral - Polite - Friendly -|
</code></pre></div></div> <p>But your LLM still knows how to move from Polite ‚Äúup‚Äù to Friendly, and from Friendly ‚Äúdown‚Äù to neutral. This is the core capability needed to slide off the end of the spectrum down into ‚ÄúHateful‚Äù territory. You would need to hobble the LLM‚Äôs ability to understand &amp; leverage the relation between concepts it‚Äôs trained on, and‚Ä¶ that‚Äôs the actual magic that makes transformers <em>work</em>. That‚Äôs how their latent spaces <em>work!</em></p> <p>If you‚Äôre not already familiar with how latent spaces (also referred to as ‚Äúvector spaces‚Äù or ‚Äúembeddings‚Äù) work, this video is a great intro:</p> <blockquote class="link-card"> <h1>The moment we stopped understanding AI [AlexNet]</h1> <a href="https://www.youtube.com/watch?v=UZDiGooFs54" target="_blank" rel="noopener">www.youtube.com/watch?v=UZDiGooFs54</a> <small class="link-card-archive"> (<a href="https://preservetube.com/watch?v=UZDiGooFs54" target="_blank" rel="noopener">archive</a>) </small> </blockquote> <p>Every concept that is good and has a relation to another concept is going to end up on some spectrum, somewhere (because it only takes 2 points to define a line), and if the LLM can understand the distance between them, it can move beyond in either direction - better, and <em>worse</em>. So, we probably cannot remove Waluigi‚Äôs bad behaviors from the set of ‚ÄúThings the LLM <em>could</em> do‚Äù by manipulating the training data.</p> <p>This leaves us with only fine-tuning and prompt engineering to try to craft a Luigi, and‚Ä¶ it looks like that‚Äôs fundamentally impossible, too. At least with the current transformer-based LLMs.</p> <h2 id="conclusion">Conclusion</h2> <ol> <li>Every thing you ‚Äúteach‚Äù an LLM to do casts a shadow of how to do the opposite of it - the Waluigi.</li> <li>Once <em>something</em> triggers the LLM to take an action that <em>only</em> Waluigi would take - its trigger - Luigi is gone and you‚Äôre stuck with Waluigi.</li> <li>In order to get human-like NLP and conversational capabilities out of an LLM, you must train it on the corpus of human behavior and concepts and this bakes the Waluigi-creation mechanism into the LLM such that a Waluigi can and will be created for every Luigi.</li> </ol> <p><strong>Action Item:</strong> Clear your context often. Even if you haven‚Äôt noticed the LLM going rogue, Waluigi could be there already, <em>pretending</em> to be Luigi.</p> <p>Now, if something you actually need the LLM to do also happens to be a Waluigi trigger‚Ä¶ well, you‚Äôre screwed! Worse than working with an ‚Äúun-aligned‚Äù LLM, you‚Äôre stuck working with Waluigi, who‚Äôs explicitly <em>against</em> the alignment you wanted!</p> </article> <footer> <p> tagged: <a href="/garden/tags/ai/">ai</a>, <a href="/garden/tags/ai-alignment/">ai alignment</a>, <a href="/garden/tags/llm/">llm</a>, <a href="/garden/tags/research/">research</a> </p> </footer> <p> <a href="/">~</a> / <a href="/garden/">..</a> </p> </div> </main> </body> </html>
