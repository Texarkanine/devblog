---
layout: post
title: "The Load-Bearing Rate Limiter Was Human"
author: texarkanine
tags:
  - ai
  - economics
  - productivity
---

## Thesis

> The money-to-productivity pipeline no longer requires time as a fixed-rate intermediary, and neither companies nor individuals are ready for what that means.

## The Transaction That Changed My Mind

I had a task. With Claude Sonnet, I could babysit the model through understanding, designing, planning, checking, and committing over the course of about four hours. Cost: maybe $5 in Cursor usage and four hours of my undivided attention.

With Opus, I wrote a spec, walked away, and came back to a finished product. Thirty minutes. About $30.

The dollar cost was 6x higher. The *me* cost was roughly 8x lower. I wasn't writing code in either case - the difference was whether I had to stand there being the bottleneck manager or whether I could go be a person for a while.

Dollars are painful when they leave your wallet... But I spent my day on the cheap version. And I only get so many of those (days, that is).

That $30/30min vs $5/4hr comparison is the transaction that reframed everything for me because it surfaces a question that sounds simple but has enormous consequences: 

> who really came out ahead?

## The 1X Bottleneck

Historically, the path from money to productivity in knowledge work has always been:

**Money -> Time (1X) -> Productivity**

A company pays a salary. That salary buys a commitment to a year of work hours. The company tries to hire the human who will convert those hours into the most output. Maybe they pay $50K and get a baseline, or $300K for someone who produces genuinely 10x the value - but even the mythical 10X engineer lives through time at 1X speed. You could buy a *better* converter, but you could never buy a *faster clock*. No matter the conversion rate, it was always going to run at 1X speed: you can't overclock a human.

In my experience, as of late 2025, frontier AI models have broken that constraint. Nevermind the early-2026 drops of Opus 4.6 and GPT-5.3-Codex. The middle of the pipeline is no longer a human metabolizing time. It's tokens, and tokens scale in a way  that humans don't.

When crypto miners discovered GPUs could convert electricity into money faster than entertained gamers could, miners repriced the entire GPU market in the blink of an eye. AI is doing the same thing to knowledge labor: a more efficient converter has arrived, and it's about to reprice everything (the RAM market is already feeling this, because gamers can't catch a break).

## Productivity Hyperscaled. Revenue Hasn't.

The immediate, observable consequence: productivity is going vertical while revenue takes the stairs.

After my Opus revelation, I burned through the $200 of my Cursor Ultra budget in barely five days. The productivity was real - tasks that would have taken weeks were landing in hours. But I couldn't sustain it. I didn't have $1,000/month for this. So I fell back to the slow path, rationing tokens, babysitting models again. Fifteen days into austerity and counting and it feels like Slow Wifi.

And here's where it got interesting: when my budget constraints had been relaxed on hobby projects, I started finishing them at an unprecedented rate. Projects that would have taken a year of spare evenings now cost $50 and a Saturday morning. But! These projects were never going to generate revenue. When they cost a year of spare time, that was fine - the time was "free" in the sense that I wasn't selling it to anyone. Now that they are costing me actual dollars, the lack of a revenue model becomes a real problem.

I am the company in miniature. I have a fixed budget. I discovered I can convert money into productivity at unprecedented rates. And now I'm staring at the constraint that actually matters: I can produce faster than the world can pay me for it.

This is not a problem unique to individuals. Companies budget annually: a year of salary against a year of expected output against a year of expected revenue. When a skilled AI-wielding engineer burns through a year of planned work in two months, they also burn through a year of token budget. The work is done, the money is spent, and there are ten months of operational costs still to cover - rent, hosting, salaries, utilities - while the company waits for customers to adopt, evaluate, and respond to what just shipped.

The budget was for a marathon, but the engineers ran a sprint and now everyone's standing around the finish line with nothing to do and no new revenue.

The constraint isn't distribution - you *can* ship ten or even a hundred improvements to a platform in a month. The constraint is *absorption*. Human customers adopt, integrate, and respond to changes at human speed. Drop ten improvements on a product in a week and your conversion rate doesn't budge ten-fold that week. The demand side has its own metabolism, and it's still running at 1X.

You don't have a productivity problem anymore. You have a *digestion* problem.

## The Discernment Imperative

The intuitive response to unprecedented productivity is "do everything faster." The correct response is almost the opposite: be more discerning about what you unleash productivity on.

If you can finish a project in a week, you could theoretically finish at least four, maybe a dozen, in a month. But you don't want to, because you'll burn all your cash before the market has a chance to respond to any of them. The capability is there. The absorption capacity isn't.

This is a genuinely novel constraint for most knowledge workers and the companies that employ them. We've spent decades optimizing for "how do we get more done?" and now the answer is "easily, but that's no longer the right question." The right question is 

> Which things, done now, will generate returns before the budget runs out?

When the human was the rate limiter, spend and revenue were temporally coupled - a year of salary forced a year of output forced a year of collection. Remove the rate limiter and both output and spend can compress to weeks while revenue still takes quarters. The calendar was load-bearing too.

## The Multi-Tenancy Engineer

Follow the math to its conclusion.

If you can convert a company's entire annual productivity budget into output in one-tenth the time, you're meaningfully working about 80 to 90 minutes of an eight-hour day (accounting for overhead & that I'm doing napkin math here). The company got every dollar's worth of productivity it paid for. You just delivered it fast.

The logical outcome is one engineer, multiple employers. Nobody is worse off. Company A paid for X productivity and got X productivity. The fact that it took you 90 minutes instead of 8 hours is a property of the converter, not the conversion. You haven't cheated anyone; you've simply become a more efficient machine.

This used to be frowned upon. It might still be, but the reasoning behind the taboo - that working for multiple employers means each one gets less than they paid for - breaks down when the bottleneck was never your effort but the clock you were living through. The multi-tenancy engineer isn't moonlighting. They're just... done.

## The Dark Corollary

The multi-tenancy engineer is perhaps an optimistic outcome. The pessimistic one is that companies see six hours of surplus capacity per day and reprice labor downward. "We'll give you a token budget instead of a raise." They can't ask for *more* productivity, because of the revenue scaling constraint - there's nothing beneficial to spend that productivity on. But someone's going to look at a 10x engineer with six free hours and want to capture that spread.

*Arbitrage* is the word for this entire landscape.

Engineers arbitrage their own time across employers. Companies try to arbitrage compensation against token costs. Token providers arbitrage compute against everyone. VCs arbitrage the information asymmetry of who understands the new exchange rates and who doesn't.

Arbitrage, all the way down. Whoever recognizes the new rates first wins. Whoever clings to the old rates longest gets arbitraged themselves.

## What to Do

I don't have a clean answer. Some fragments:

Watch whether revenue scaling follows productivity scaling. If it doesn't, the gap between what you *can* build and what the market will pay for is your actual constraint. Not your engineering capacity. Not your headcount. The market's ability to digest what you ship.

Be discerning. Strategic restraint in the face of unprecedented capability is counterintuitive, but burning cash faster than revenue arrives is how companies die. The ability to ship isn't the bottleneck anymore: market absorption is. Many people haven't noticed yet.

If you're an individual knowledge worker, start thinking about what it means when you can fulfill everything a company can afford to pay you for in a fraction of the time it used to take. That surplus capacity is yours, even if the cultural norms haven't caught up yet.

And keep an eye on the arbitrage. It's coming from every direction and your position in the spread determines whether this transition is a windfall or a haircut. Right now, the spread is wide - which means the stakes are high and the window won't stay open forever.
